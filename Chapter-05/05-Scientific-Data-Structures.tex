% Chapter 5 -- Scientific Data Structures

\chapter{Scientific Data Structures}
\label{chap:05}
\TallC{Data} structures are the soul of any computer program in any language. Some languages, most notably FORTRAN and BASIC, predefine some data structures but require extensive contortions to define others. This straitjacket approach has virtues as well as defects:

\begin{itemize}
    \item The re-defined structures are what most users need to solve standard problems, so meet 80-90\% of the cases in practice. That is, they are not terribly restrictive.
    \item Because the most-needed structures are predefined and have a standard format, they do not have to be invented each time a program is written. Standardization facilitates the exchange an portability of programs.
    \item Standardized data structures aid program development in discrete modules, permitting sections written by different persons or teams to interface properly with minimal tuning.
\end{itemize}

FORTH pre-defines a minimal set of data structures but allows unlimited definition of new structures. How is this different from Pascal, Ada, or even C? FORTH not only permits extension of the set of data structures, it permits definition of new operators on them. Thus, \eg, FORTH permits simple implementation of complex arithmetic whereas the aforementioned do not.

This chapter\sepfootnote{05_01} suggests protocols for arrays and typed data\sepfootnote{05_02} that will increase the portability of code and encourage the exchange of scientific programs. The keys to this are generic operations that recognize the data type of a scalar or array variable at run-time and act appropriately.

\section{Typed data structures}

One of the virtues of FORTRAN or BASlC is that the programmer does not have to keep track of what type of data he is fetching and storing from memory. In fact the user does not even program such operations explicitly -- the compiler takes care of everything induding the bookkeeping. Mixed-arithmetic expressions like

\begin{lstlisting}
Z = -37.2E-17*CEXP(CMPLX(R**2,W)/32)/DSIN(W)
\end{lstlisting}

place great demands on a compiler. The compiler first tabulates the types of the variables and literals in the expression, and then decide which run-time routines to insert. With two types of integers and four types of floating-point numbers (REAL*4, REAL*8, COMPLEX*8, and COMPLEX*16) a typical binary operator such as exponentiation (**) offers 36 possibilities. No wonder FORTRAN compilers are slow. 

\TallC{FORTH} sacrifices automation opting for a small, fast, flexible compiler. The traditional FORTH style gives each type of data its own operators. However, if a program demands all the standard REAL*4, REAL*S, COMPLEX*8 and COMPLEX*16 data types (not to mention INTEGER*2 and *4), having to remember them all and use them appropriately is a chore. This problem has led me to experiment with generic access operators, \bc{G@} and \bc{G!}. These let FORTH keep track of which words to use in fetching and storing the "scientific" data types to the fstack (which may partly reside on a co-processor like the 80x87 or MC68881 chips). Corresponding generic unary and binary floating point operators \bc{GDUP}, \bc{G*}, \textit{etc}. allow programs themselves to be generic.

\TallC{I }have lately further modified the scheme to permit more complete automation. The kernel of the method is an "intelligent" fstack, or ifstack, that records the type of each number on it. The generic arithmetic operators and library functions decide from the information on the ifstack how to treat their operands.

An ifstack-base protocol for floating point and complex arithmetic has drawbacks and advantages. A major drawback is the run-time overhead in maintaining the ifstack and in choosing the appropriate operator for a given situation. In other words we trade convenience for a non-negligible execution speed penalty. To some extent this can be mitigated by \textit{computing} decisions and by vectoring rather than branching (\textit{i.e.} no Eaker \bc{CASE} statements or \bc{IF ... ELSE ... THEN}s). Moreover, although the definitions are coded in high-level FORTH for portability, the key words should be hand-assembled for the target machine. Finally, my high-level ifstack manager has plenty of error checking that could be dispensed with when speed is an issue.

The chief advantages of the ifstack are:
\begin{itemize}
    \item Unlike FORTRAN, this scheme permits generic routines that will accept several types of input. Hence, \eg, a matrix inversion routine will happily invert REAL*4, REAL*8, COMPLEX, and DCOMPLEX matrices.
    \item A FORTRAN $\rightarrow$ FORTH translator\sepfootnote{05_03} becomes simple with generic operators.
    \item The ifstack permits recursive programming \textit{a la} LISP.
\end{itemize}

\subsection{Type descriptors}
\TallC{To} decide at run-time which \regc{@} or \regc{!} to use for a particular datum, FORTH needs to know what type of datum it is. The scheme described here wastes a little memory by attaching to each variable a label that tells \bc{G@} and \bc{G!} how to get hold of it.

Here is how we label types:
\begin{lstlisting}
    \ Data type identifiers
    0 CONSTANT REAL*4   \  4 bytes long
    1 CONSTANT REAL*8   \  8 bytes long
    2 CONSTANT COMPLEX  \  8 bytes long
    3 CONSTANT DCOMPLEX \ 16 bytes long

    \ a simple version of #BYTES
    CREATE #bytes 4 C, 8 C, 8 C, 16 C,
    : #BYTES (type -- #bytes) #bytes + C@ ;
\end{lstlisting}

\subsection{Typed scalars}
\label{sec:Typed_scalars}

\TallC{We} want the machine to remember for us the data-specific fetches and stores to the co-processor. To accomplish this, the typed variable has to place its address and type on the stack. Thus we need a data structure that we might visualize diagramatically in Fig. \ref{fig:05_01} below (a cell $\mathbf{\big |}$ {\colorbox{gray}{\color{gray}X}} \space {\colorbox{gray}{\color{gray}X}} $\mathbf{\big |}$ represents 2 bytes): 
% Fig. 5-1 Memory structure a! a typed scalar
\begin{figure}
    \center
    \begin{tikzpicture}
        \foreach \n in {0,...,3}
        {
            \draw [xshift=1.2*\n cm,thick] (0,.4cm)--(0,-.0cm);
            \draw [xshift=1.2*\n cm,fill, opacity=0.2] (+.1,0.4) rectangle (+.4,-.0) {};
            \draw [xshift=1.2*\n cm,fill, opacity=0.2] (+.5,0.4) rectangle (+.8,-.0) {};
            \draw [xshift=1.2*\n cm,thick] (.9,.4cm)--(0.9,-.0cm);
            \coordinate (C\n) at (1.2*\n cm, .2);
        }
        \node[right=1.6cm of C3, anchor=north] (N0) {$\dots$};
        \node[right=2cm of C3] (N1) {$\rightarrow$};
        \node[right=0 of N1] (N2) {to higher memory};
        \node[draw, below= of C0.west,anchor=west] (N3) {type};
        \node[draw, above=1.2cm of C1.west,anchor=west] at (0.6,0) (N4) {start of data};
        \draw[-latex] ($(N4)+(-0.25,-0.25)$) -- ++(0,-.5);
        \draw[-latex] ($(N3.west)+(0.2,0.25)$)  -- ++(0,.5) ;
    \end{tikzpicture}  
    \caption{\textit{Memory structure of a typed scalar}}
    \label{fig:05_01}
\end{figure}
 
We implement a scalar through the defining word

\begin{lstlisting}
    : SCALAR ( type -- )
       CREATE DUP , #BYTES ALLOT
       DOES> DUP@ SWAP 2+ SWAP ; ( -- adr t )
\end{lstlisting}

The word \bc{SCALAR} is used as
\begin{lstlisting}
REAL*4   SCALAR X
REAL*8   SCALAR XX
COMPLEX  SCALAR Z
DCOMPLEX SCALAR ZZ
  etc.
\end{lstlisting}

\subsection{Defining several scalars at once}
\TallC{One} aspect of the FORTH method of handling variables, that seems strange to programmers familiar with Pascal, BASIC, or FORTRAN, is that \bc{VARIABLE}, \bc{CONSTANT} or a new defining word like \bc{SCALAR} need to be repeated for each one defined, as above. That is, such defining words generally do not accept name-lists.

This idiosyncracy can be traced to FORTH's abhorrence of variables:
\begin{itemize}
    \item Easily read (and maintained) FORTH code consists of short definitions with few (generally $\leq$ 4) numbers on the stack. Such programs have small use for variables, especially since the top of the return stack can serve as a local variable.
    \item In FORTH as in BASIC, variables tend to be global and hence corruptible. The variables in a large program can have unmnemonic names or names that do not express their meaning simply because we run out of names.
    \item Experienced FORTH programmers tend to reserve named variables for such special purposes as vectoring execution.
    \item The standard FORTH kernel therefore discourages named variables by making them as tedious as possible.
\end{itemize}

Most objections to variables can be resolved by making them local. Local variables are relatively easy to define in FORTH: a straightforward but cumbersome method for making "headerless" words is given in Kelly's and Spies's book\sepfootnote{05_04}.

HS/FORTH\sepfootnote{05_05} provides beheading in a particularly simple form: \bc{BEHEAD' NAME}, or \bc{BEHEAD" NAME1 NAME2}.

Used after \bc{NAME} has been invoked in the words that need to reference it, \bc{BEHEAD'} removes \bc{NAME}'s dictionary entry leaving pointers and code fields intact and recovering the unused dictionary space. The more powerful word \bc{BEHEAD"} does the same for the range of dictionary entries \bc{NAME1 ... NAME2}, inclusive.

Beheading variable or constant names makes them local to the definitions that use them; they cannot be further accessed --or corrupted-- by later definitions. (Pountain\sepfootnote{05_06} has given yet another method for making variables local, using a syntax derived from "object-oriented" languages such as SMALLTALK.)

Variables are essential for scientific programming. Since we must often have more than two variables, it is silly to repeat \bc{SCALAR}. A simple way to allow \bc{SCALAR} to use a list is

\begin{lstlisting}
    :SCALARS ( n -- )
      SWAP 0 DO DUP SCALAR LOOP DROP ;
    \ Examples:
    \ 2 REAL*4  SCALARS  A  B
    \ 5 COMPLEX SCALARS XA XB XC XD XE
\end{lstlisting}

I find the use of \bc{SCALARS} with modifiers and lists more convenient and readable than many repetitions of \bc{SCALAR}. Its resemblance to FORTRAN (thereby helping me live with my FORTRAN-inspired habits) is pure coincidence. Although possible to use a terminator (",\eg) rather than a count (to define the variable list) I feel it is desirable for the programmer to know how many variable names he has supplied, hence the counted version.

\subsection{Generic access}
\TallC{A} major theme of FORTH is to replace decisions by calculation henever possible\sepfootnote{05_07}. This philosophy usually pays dividends in execution speed and brevity of code.

But there is an even more important reason to avoid \bc{IF ... THEN} decisions, especially when working with modern microprocessors. CPUs like 80x86 and MC680x0 achieve their speed in part by pre-fetching instructions and storing them in a queue in high speed on-chip cache memory. A conditional-branch machine instruction (the crux of \bc{IF ... THEN}) empties the queue whenever the branch is taken. Branches should be avoided because they slow execution far more than one might expect based on their clock-counts alone.

To replace decisions, we use the standard FORTH technique of the execution array (analogous to the familiar assembly language jump table). This lets us compute from the type descriptor which fetch or store to use.

\TallC{We} now define \bc{G@} and \bc{G!} as execution arrays using\sepfootnote{05_08} anexecution-array-defining word \bc{G:}

\begin{lstlisting}
    : G: CREATE ]
       DOES> OVER + + @ EXECUTE ; ( t -- )
    G: G@ R32@ R64@ X@ DX@ ;
    G: G! R32! R64! X! DX! ;
\end{lstlisting}

assembled from components of the FORTH compiler. That is, the ordinary colon \bc{:} might have the high-level definition (shorn of error detection)

\begin{lstlisting}
    : : CREATE ] DOES> @ EXECUTE ;
\end{lstlisting}

\bc{CREATE} makes the new dictionary entry, and \bc{]} switches to
compile mode. \bc{DOES>} specifies the run-time action (recall any word created by \bc{CREATE} leaves its parameter field address
\textbf{-pfa-} on the stack at run-time, \textit{prior} to the actions following \bc{DOES>}). In the case of \bc{:} the run-time action is to fetch the pfa of the new word and execute it. At run-time, words defined using \bc{G:} add twice the type descriptor to the pfa (to get the offset into the array) then fetch the desired address and \bc{EXECUTE} it.

Microprocessors like the MC680x0 and 80386 that can address large, level memories require no further elaboration for \bc{G@} and \bc{G!}. However, if large arrays are to be addressed within the segmented memory addressing protocol of the 8086/80286 chips, we would have to define \bc{G@} and \bc{G!} to use the "far" forms of addressing words\sepfootnote{05_09}. For example, in HS/FORTH such words as \bc{R32@L} expect a segment paragraph number and offset (32 bits total) as the complete address of the variable being fetched to the 87stack. In that case we modify the definition of \bc{SCALAR} to include the segment paragraph number (seg) in the definition (\bc{LISTS} is nonstandard - it is HS/FORTH's name for the portion of the dictionary containing the word headers)

\begin{lstlisting}
    : SCALAR ( type -- )
    CREATE DUP ,        \ make header ,type
    #BYTES ALLOT        \ reserve space
    DOES>  >R
    [ LISTS @ ] LITERAL ( -- seg)
    R@ 2+               ( -- seg off)
    R> @ ;              ( -- seg off type)
    \ Ex: REAL*4 SCALAR X
\end{lstlisting}

\subsection{The intelligent floating point stack (ifstack)}
\TallC{The} \textbf{ifstack} is a more complex data structure than either a simple fstack or the parameter/retum stacks. When a typed datum is placed on the ifstack its type must be placed there also.

But the typed data have varying lengths, from 4 to 16 bytes. We can deal with this two different ways: either \bc{ALLOT} enough memory to hold a stack of the longest type, making each position on the ifstack 18 bytes wide (to hold datum plus type); or manage the ifstack as a modified heap, with the address of a given datum being computable from the ifstack-pointer and the data type.

The 18-byte wide ifstack wastes memory, but is easy to program. (In retrospect, this is exactly the method I used to program adaptive numerical quadrature\sepfootnote{05_10}.) After several false attempts I settled on the fixed-width ifstack. High level FORTH code for this variant is given below.
\begin{multicols}{2}
\begin{lstlisting}[basicstyle=\small,]
\ TYPED DATA STACK MANAGER
TASK FSTACKS
FIND CP@ 0=
?( FLOAD COMPLEX.FTH )

\ define data-type tokens
0 CONSTANT REAL*4
1 CONSTANT REAL*8
2 CONSTANT COMPLEX
3 CONSTANT DCOMPLEX

CREATE #bytes 4 C, 8 C, 16 C,
: #BYTES #bytes + C@ ; 
( type -- length in bytes )

\define scalar and scalars
: SCALAR        ( type -- ) 
    CREATE DUP , #BYTES ALLOT 
    DOES> DUP@ SWAP 2+ SWAP ;

( -- seg off type )
\ say: REAL*4 SCALAR X
: SCALARS ( n type -- )
    SWAP 0 DO DUP SCALAR LOOP
    DROP ;
\ say: 4 DCOMPLEX SCALARS XA XB XC XD

\ definitions for the parallel stack
\ of types and data 
\ Brodle, TF (Brady, NY, 1984) p. 207.

CREATE FSTACK 20 18 * 2+ ALLOT
\ 2 tos-pointer, 20 18-byte cells
: FS.INT FSTACK 0! ;
: EMPTY         ( -- seg off )
    [ LISTS @ ] LITERAL
    FSTACK DUP@ 18 * 2+ + ;
: >FS ( seg off type -- ) \ say: X >FS
    >R >EMPTY ( -- seg off seg' off )
    R@ OVER ! \ store type on ifstack

FS> ( seg off type -- ) \ say: X FS>
FSTACK 1-!              \ dec ifstack ptr
>R >EMPTY               ( -- seg off seg' off' )
DUP@ R@ =               \ srce.type = dest.type ? 
IF 2+ DSWAP             ( -- seg' off' sef off )
R> #BYTES               ( -- seg' off' sef off n )
CMOVEL                  \ move data from ifstack
ELSE RDROP CR 
    ." ATTEMPT TO STORE TO
    WRONG DATA TYPE" ABORT
THEN ;

\ execution-array defining word
\ HS/FORTH has the faster
\ CASE:...;CASE pair for the same job

: G: CREATE ] DOES> OVER + +
    @EXECUTE ; ( t -- )

G: G@ R32@L R64@L X@L DX@L ;
G: G! R32!L R64!L X!L DX!L ;
\ move data from ifstack to/from FPU
: FS>F ( -- t 87: -- x )
    FSTACK 1-!  \ dec ifstack ptr
    >EMPTY      ( -- seg off )
    DUP@ >R 2+
    R@ ( sef off' type )
    G@ R> ;
\ move data from ifstack to 87stack, leave type

:F>FS (t-- 87:x--)
    >R >EMPTY   ( -- sef off )
    R@ OVER !
    2+ R> ;     ( -- sef off' type)
\end{lstlisting}
\end{multicols}
 
The stack comments and comments should make the preceding code self-explanatory.

\subsection{Unary and binary generic operators}
\label{chap:05_02_06}
\TallC{We} want to define generic unary and binary operators whose run-time action selects the desired operation using information contained in the ifstack. A unary operator such as \bc{FNEGATE} or \bc{FEXP} expects one argument and leaves one result. With a floating-point coprocessor (FPU) the only distinction is between real or complex. This distinction is contained in the second bit of the type descriptor, which we exhibit in Table \ref{tbl:05_01} on page \pageref{tbl:05_01}, in binary notation.

Real and complex can then be distinguished \textit{via} the code fragment

\quad 2 AND ( type - - 0 = real$\mid$ 2 = complex )

% Table 5-1
\begin{table}{H!}
    \centering
    \caption{\textit{Bit-patterns of data type descriptors}}
        \bigskip
    \label{tbl:05_01}
    \setlength{\tabcolsep}{30pt}
        \begin{tabular}{|lll|}
            \hline
            & &\\
            \textbf{Type} & \textbf{BINARY} & \textbf{representation} \\
            & &\\
            REAL*4     &  00000000 & 00000000 \\
            & &\\
            REAL*8     &  00000000 & 00000001 \\
            & &\\
            COMPLEX    &  00000000 & 00000010 \\
            & &\\
            DCOMPLEX   &  00000000 & 00000011 \\
            & &\\
            \hline 
        \end{tabular}
\end{table}

Since most unary operators produce results of the same type as their argument, we write a defining word for \textit{generic} unary operators:

\begin{lstlisting}
: GU: CREATE ] DOES>    ( -- pfa )
    FS>F ( -- pfa t )   \ get data
    UNDER 2 AND +       ( -- t adr )
    @ EXECUTE           \ do it
    F>FS ;              \ return ans.
\end{lstlisting}

When we use \bc{GU:} in the form
\begin{lstlisting}
    GU: GNEGATE FNEGATE XNEGATE ;
\end{lstlisting}

\bc{CREATE} produces a dictionary entry for \bc{GNEGATE; ]} turns on the compiler so the previously defined words \bc{FNEGATE} and \bc{XNEGATE} have their addresses compiled into \bc{GNEGATE}'s parameter field; and \bc{DOES>} attaches the run-time code. The run-time code converts the real/complex bit into an offset, 0 or 2 which is added to the address of the daughter word to get the address where the pointer to the actual code is stored. This pointer is fetched and \bc{EXECUTE}d.

A few unary operators like \bc{XABS} (complex absolute value) return real values from complex arguments. If we want to use \bc{GU:} to define, say, \bc{GABS}, we must remember to redefine \bc{XABS} so it zeros the second bit of the type descriptor left on the stack, before returning its result to the ifstack. This is just a \bc{1 AND} so is fast.

% fig. 5-2 
\begin{figure}
    \center
    \newcolumntype{x}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
    \setlength{\extrarowheight}{0.1cm}
    \begin{tabular}{|llm{7cm}|}
        \hline
        TYPE\textsubscript{ab} & & \\
        & &
        \begin{tabular}{ x{0.8cm}x{0.8cm} x{0.8cm} x{0.8cm} x{0.8cm} }
            \textsubscript{\space \space b}&&&&\\
            %\textsuperscript{b}&&&&\\
            \diaghead(-3,2){\hskip \hsize} a & R & D & X & DX  \\
            \cline{2-5}
            \multicolumn{1}{ x{0.8cm}|}{R  }&  R & R  & X & X  \\
            \multicolumn{1}{ x{0.8cm}|}{D  }&  R & D  & X & DX \\
            \multicolumn{1}{ x{0.8cm}|}{X  }&  X & X  & X & X  \\
            \multicolumn{1}{ x{0.8cm}|}{DX }&  X & DX & X & DX 
        \end{tabular}
        \\
        & & \\
        \hline
    \end{tabular}
    \caption{\textit{Types resulting from 2-argument operators}}
    \label{fig:F05_02}
\end{figure}

A binary operator (one that takes two arguments) expects its arguments \textit{and} their types on the ifstack. There is no distinction between single- and double-precision arithmetic on most numeric coprocessors. However, the result must leave the proper type-label on the stack. Here is what we want to happen, illustrated in Fig. \ref{fig:F05_02} as a matrix \regc{TYPE(arga, argb))}

\leftbar[1\linewidth]
Note: this protocol avoids misleading precision for the results of computations. It seems more scientific than FORTRAN's "convert intermediate results to the precision of the highest-precision operand" protocol.
\endleftbar

If we think of the indices and entries in Fig. 5-2 as numbers 0, 1, 2, 3 (so we can use them as indices into a table) rather than as letters, a simple algorithm emerges: the first bit of the result is the logical-AND of the first bits of the two operands, and the second bit of the result is the logical-OR of their second bits. Although we would program this in assembler for speed, the high-level definition is

\begin{lstlisting}
: NEW.TYPE  ( a b -- a2 + b2 + a1b1 )
    DDUP        ( -- a b a b        )
    AND         ( -- a b ab         )
    1 AND       ( -- a b [ab]1      )
    -ROT OR     ( -- [ab]1 a+b      )
    2AND        ( -- [ab]1 [a+b]2   )
    + ;         ( -- a2 + b2 + a1b1 )
\end{lstlisting}

Since only logical operations are used, \bc{NEW.TYPE} is faster than table lookup or branching. Note that in programming this key word we have obeyed the central FORTH precept: "Keep it simple!" by choosing a data structure (the numeric type tokens 0-3) that is easily manipulated.

We will also need a way to select the appropriate operator from a jump table of addresses. Given that the precision (internal) is irrelevant, again all that matters is whether the number is real or complex, \ie \,the second bits of the numbers. The first operation must then be to divide by 2 (right-shift by one bit). We then have the matrix of Fig. \ref{fig:05_03} below
% Fig. 5-3 
\begin{figure}[H]
    \center
    \begin{tabular}{|lll|}
        \hline
        && \\
        && \\
        \begin{tabular}{ccc}
            & 0 & 1 \\
            \cline{2-3}
            \multicolumn{1}{c|}{0  }&  RR & RX \\
            \multicolumn{1}{c|}{1  }&  XR & XX \\
        \end{tabular}
        %& $\rightarrow $ &
        & $ \xrightarrow{\hspace*{1cm}} $ &
        \begin{tabular}{ccc}
            & 0 & 1 \\
            \cline{2-3}
            \multicolumn{1}{c|}{0  }&  0 & 1 \\
            \multicolumn{1}{c|}{1  }&  2 & 3 \\
        \end{tabular}
        \\
        &&\\
        &&\\
        \hline
    \end{tabular}
    \caption{\textit{Operator selection matrix}}
    \label{fig:05_03}
\end{figure}
where RR stands for real-real, \textit{etc.} The numerical elements are generated as \regc{2*J+I}. This leads to the word

\begin{lstlisting}
    : WHICH.0P ( a b -- c )
        2/ SWAP 2 AND + ;
\end{lstlisting}

Thus we come to the binary generic-operator defining word
\begin{lstlisting}
    : GB: CREATE ] DOES>        ( -- pfa )
        FS>F FS>F               ( -- pdf t0 t1 )
        NEW.TYPE UNDER          ( -- t' pfa t' )
        WHICH.OP 2* +           \ make result-type
            @ EXECUTE           \ select binop
        F>FS ;                  \ save result
    \say: GB: G*   F*  F*X  X*F  X* ;
\end{lstlisting}

The generic multiply \bc{G*}, \eg, picks out, at run-time, which of four routines to use. By using only logical or shift operations we have made even the high-level definitions fairly quick in comparison with the times of floating point operations.

The only instance where one might forego the overhead penalty paid for the convenience of generic coding would be in nested inner loops, such as occur in matrix operations. Here it might pay to code four inner loops, one for each type, and then access them generically, \eg

\begin{lstlisting}
    : RLOOP      ... real words ...     ;
    : DRLOOP     ... dreal words ...    ;
    : XLOOP      ... complex words ...  ;
    : DXLOOP     ... dcomplex words ... ;
    G: GLOOP  RLOOP DRLOOP XLOOP DXLOOP ;
\end{lstlisting}

\section{Arrays of typed data}

\TallC{Numerical} arrays represent a frequently encountered characteristic feature of scientific programming. Arrays \textit{per se} are hardly foreign to FORTH. Arrays of typed data are novel, however, and therefore worth elaborating. Following Brodie's advice(\TF, p. 48ff) we first specify the "user interface" (matrix notation) and then proceed to implementation.

\subsection{Improved (FORTRAN-like) array notation}
\TallC{Something} like V(15) --the 15'th element of V-- is the commonest notation for array elements in high-level languages because lineprinters and terminals do not easily recognize subscripts. In FORTH, the most natural notation would be postfix (RPN), \bc{15 V} --but this is both hard to read and unintuitive\sepfootnote{05_11}. That is, \bc{15 V} does not say, immediately and unambiguously, "I am the 15th element of the array V !"

FORTH's idiosyncrasies forbid saying V(15) because the parser recognizes V(15) as a single word\sepfootnote{05_12}. Since we want the 15 to be parsed, we would have to modify the FORTRAN-ish notation to $\mathbf V\;\bullet(\;\bullet15)$ or $\mathbf V(\;\bullet15\;\bullet)$, where $\mathbf\bullet$ stands for a blank space (ASCII 32). Unfortunately, "(" is a reserved word. While we might place the matrix definitions in a separate vocabulary --which would let us redefine anything we want-- "(" is too useful as a comment delineator to dispense with.

\TallC{This} leaves the second possibility, where "(" becomes part of the array name, \bc{V(}. To make $\mathbf V(\;\bullet15\;\bullet)$ work,")" must become an operator --unless we want to leave postfix notation entirely, with all the complication \textit{that} would entail\sepfootnote{05_13}. Since ")" is not a reserved word, nothing in principle prevents defining it as an operator. However, such usage would conflict with comments.

The square braces, [ ], are commonly used in matrix notation; however both are reserved FORTH words, \ie\; forbidden. This leaves the curly braces \{ \}, which are unused by FORTH.

Of the two possible forms, $\mathbf V\;\bullet\{\;\bullet15\}$ or $\mathbf V\{\;\bullet15\;\bullet\}$, the latter has the advantage that the opening brace, \bc{\{}, is only part of the name, but reminds us that the name \bc{V\{} \textit{is} an array, exactly as names ending with \bc{\$} are strings, \etc The notation suggests a further mnemonic refinement, namely to place \bc{\{\{} and \bc{\}\}} at the ends of 2-dimensional arrays, as in $\mathbf{M\{\{\bullet \;3 \bullet 5 \;\bullet \}\}}$.

How will this notation operate? Clearly, to place the (generalized) address of the n'th element (of a 1-dimensional array) on the stack we would say

\begin{align*}
    V\{\bullet \;n \;\bullet\}
\end{align*}

whereas

\begin{align*}
    M\{\{ \bullet \;m \bullet n \;\bullet\}\}
\end{align*}

should analogously place the address of the m,n'th element of a 2-dimensional array on the stack.

\subsection{Large matrices}
\TallC{The} defining word \bc{SCALAR} given in \S\ref{sec:Typed_scalars} above allots space in the dictionary --for most FORTHs, code + data must fit here-- or in the LISTS segment of HS/FORTH (part of the dictionary). This is OK for variables, but not for arrays, since even a modest matrix would exhaust the ( $\leq$64 Kbyte) LISTS segment.

A \bc{REAL*4} matrix uses 4 bytes per ellement. The largest such array that can be stored in a 65,536 (\ie, $2^{16}$) -byte segment is 128x128. This is the largest array that can be addressed with unsigned 16-bit numbers. On the other hand, a filled IBM PC/XT clone has 64 Kbytes of memory under MS-DOS. Even a generous F0RTH kernel (plus DOS) takes up less than 150 K; hence 450 K is available to hold large arrays. Up to 8 Mbytes can be added as EMS storage, assuming a suitable memory management scheme\sepfootnote{05_14}. That is, in principle one could tackle matrix problem of order 350x350. What about speed? The dominant term is solving linear equations by --say-- Gaussian elimination with partial pivoting is
\[
T = \frac{1}{3} mn^{3}
\]
where m is the time for 1 multiply and 1 add, and n is the order of the matrix. We should also include the fetch + store time, since the bus bandwidth is as much a limiting factor as the FPU arithmetic speed. For the 8086/8087 the time $m$ is of order 400 clock cycles. Thus the asymptotic execution time on a 10 MHz machine should be of order 10 minutes for n = 350.

On the 80386/80387 combination running at 25 MHz, the execution time for the same problem should be only 2.3 minutes or so. Thus it would be practical (\ie, execution time $\cong$ 1 hour) on such machines, even without special equipment such as the IIT 80c387, or an array co-processor, or a faster procedure such as Strassen's algorithm (see Ch. 4 \S8), to tackle $10^{3} \times 10^{3}$ dense matrix problems.

The crucial question therefore, is memory. The Intel machines were designed around a segmented memory architecture. That is, to avoid having to use (expensive) 32-bit address registers, the 8086/80286 chips were designed to use 16-bit registers. However, these chips have more than 16 external address lines -- 20 for the 8086, 24 for the 80286. Thus the absolute address is compounded of two numbers: a \textbf{segment descriptor} and an \textbf{offset}, which must be present in appropriate registers. The segment descriptor is the \textbf{absolute address} of a l6-byte \textbf{paragraph}, divided by 16. The offset is any (unsigned) integer from 0 to $2^{16}-1 = 65,535$ that can fit in a 16-bit register.

The chips contain 4 segment registers: SS (stack segment), CS (code segment), DS (data segment) and ES (extra segment). Five registers, BP, SP, SI, DI, and BX, can be used for offsets, although they are not entirely interchangeable (some have specific functions in some of the more complex machine instructions, such as string operations). Manifestly, since the 8086 can address

\quad$2^{20} = 1,048,576 \textrm{ bytes ("1 megabyte")}$,

the largest segment number is $2^{14}-1 = 16,383$.

A typical (segmented) address is expressed in Intel assembly code
as

\begin{lstlisting}
    CS: [BX + SI + 0008]
\end{lstlisting}

which translates in words to "add the offset in BX to that in SI and then add 8 to get the total offset; take the segment descriptor in CS, multiply by 16 and add to produce the absolute address\erratumAdd{forgotten word}{"}.

\subsection{Using high memory}
\TallC{HS/FORTH} permits accessing all the memory in a PC/AT (up
to 1 megabyte) in the following manner:
\begin{itemize}
    \item Define a named segment of length 1 byte: this marks the beginning of available memory.
    \item Then tell both FORTH and DOS how much memory you want.
\end{itemize} 

As might be expected, HS/FORTH defines non-standard words (coded as DOS function calls) to use the various DOS service routines that allocate memory, \textit{etc}.\sepfootnote{05_15}

\begin{lstlisting}
    MEMORY 4+ @ S->D
    DCONSTANT MEM.START \ beg. of free memory
    40.960 DCONSTANT MAX.PARS
        \ 40960 = 655360 /16
    : TOTAL.PARS MAX.PARS MEM.START D- ;
        \ # pars of memory available
    1 SEGMENT SUPERSEG
        \ define named segment 1 byte long
    TOTAL.PARS DROP FREE-SIZE
        \ tell DOS and HS/FORTH about it
\end{lstlisting}

Having allocated the memory, how can we address it efficiently? We would like the simplicity of double-length integer arithmetic for computing an (absolute) array address, as in

\begin{lstlisting}
    abs.adr(A_{ij}) = abs.adr(A_{00}) + (row.length*I+J) *#BYTES
\end{lstlisting}

However, although the absolute address referenced by a segment and offset is unique, \ie the absolute address in bytes is

\begin{lstlisting}
    aba.adr = 16*segment + offset ,
\end{lstlisting}

the reverse translation, of an absolute address (in bytes) to the segment + offset notation expected by 80x86 processors is \textit{not} unique. This naturally poses a problem when the processor tries to prevent segments from overlapping (protected mode). In such cases, the only answer is a memory management scheme that computes segments and offsets (by brute force) in a non-overlapping fashion. For example, we might define large arrays such that each row has its own segment paragraph.

The 80386 CPU has a third mode that permits direct 32-bit addressing of 4 gigabytes (albeit few computer users have quite this much fast memory available). A scheme for addressing large amounts of RAM in 80386 machines (without leaving MS-DOS) has been discussed in \textit{Dr Dobb's Journal}\sepfootnote{05_16}.

For 8086 PC's and/or real-mode programming on 80286 + machines, we can merely ignore whether segments overlap. Oddly, standard assembly programming books\sepfootnote{05_17} omit this way of addressing segmented memory.

The 8086 permits 32-bit addressing as long as we translate 32-bit addresses to the segment + offset notation expected by the 80x86 processors in real mode. A word that performs this conversion is \bc{SEG.OFF}, defined as

\begin{lstlisting}
    : >SEG.OFF          ( d -- seg off )
        OVER 15 AND     (   --   d off )
        -ROT D16/ DROP  (   -- off seg )
        SWAP ;
\end{lstlisting}

The 32-bit address is placed on the stack as a double-length integer, with the low-order (\ie\ offset part) above the segment part. The phrase \bc{OVER 15 AND} saves bits 0-3 (of the 32-bit address); \bc{-ROT D/16 DROP} then shifts the (32-bit) address right 4 bits and drops the least-significant part, to produce the offset. This conversion method produces offsets in the range 00-0F (hex), that clearly have nothing to do with the original offsets (that led to the 32-bit absolute address \textit{via} 16*seg + off ).

\subsection{A general typed-array definition}
\TallC{For} the new syntax to work the word \bc{\}} must compute the addresses of the n'th element of \bc{V\{} from the information on the stack, and \bc{\}\}} must do the same for \bc{M\{\{}. In order to encompass matrices of typed data we specify that the results of the phrases \bc{V\{ n \}}and \bc{M\{\{ m n \}\}} be to leave the generalized address on the parameter stack, \ie to leave the stack picture \regc{( -- seg off type)}, exactly as with \bc{SCALAR}s.

Before we can define \bc{\}}, however, we must specify the data: structure it operates on, \ie\ the array header.

Once again we begin with the user interface. We can opt for maximum generality or maximum simplicity. My first attempt fell into the first category, permitting the user to define a named segment of given length and to define an array in that segment. Lately I realized this generality accomplishes little, so have abandoned it. All arrays will be defined in the heap, named \bc{SUPERSEG} as above. To define a length-50 \bc{1ARRAY} of 4-byte number we will say

\begin{lstlisting}
    50 LONG REAL*4 1ARRAY V{
\end{lstlisting}

Now, before we work out the mechanics of \bc{1ARRAY}, we imagine that an array will be stored as in Fig. \ref{fig:05_04} below:

The proposed data structure consists of an 8-byte header (the \textbf{array descriptor}) in the dictionary (\regc{LISTS} in HS/FORTH), with the \textbf{body} of the array stored elsewhere. The array descriptor points to the absolute address of the array data (body).
% Fig. 5-4 
\begin{figure}
    \center
    \begin{tikzpicture}
        \draw (-3,-5) rectangle (12,3.0);
        \node[align=left, anchor=west] at (-2,2.5) {LISTS portion: (V\{ puts adr on};
        \node[align=left, anchor=west] at (-2.5,-1.8) {SUPERSEG portion};
        \foreach \n in {0,...,3}
        {
            \draw [xshift=1.2*\n cm,thick] (0,.4cm)--(0,-.0cm);
            \draw [xshift=1.2*\n cm,fill, opacity=0.2] (+.1,0.4) rectangle (+.4,-.0) {};
            \draw [xshift=1.2*\n cm,fill, opacity=0.2] (+.5,0.4) rectangle (+.8,-.0) {};
            \draw [xshift=1.2*\n cm,thick] (.9,.4cm)--(0.9,-.0cm);
            \coordinate (C\n) at (1.2*\n cm, .2);
            \coordinate (CB\n) at (1.2*\n cm, 0);
            \coordinate (CT\n) at (1.2*\n cm, 0.4);
            \coordinate (FT\n) at (1.2*\n +0.25, 0.5);
            \coordinate (ST\n) at (1.2*\n +0.5, 0.5);
        }
        \node[right=1.5cm of C3, anchor=north] (N0) {$\dots$};
        \node[right=2cm of C3] (N1) {$\xrightarrow{\hspace*{0.7cm}}$};
        \node[right=0 of N1] (N2) {to higher memory in LISTS};
        \node[draw, above= of C0.west,anchor=west] (N3) {type};
        \node[draw, above= of ST2] at (1.7,-0.08) (N4) {length};
        \node[draw, below= of C0.west] (N5) {adr};
        \node[align=center, draw, text width=5cm] at (7.6,1.5) (N6) {VHERE + SUPERSEG*16\\(32-bit address)};
        \draw[-latex] ($(N4)+(-0.25,-0.25)$) -- ++(0,-.5);
        \draw[-latex] ($(N3.west)+(0.2,-0.25)$)  -- ++(0,-.5) ;
        \draw[-latex] (N5.north)  -| (CB0) ;
        \draw[-latex] (N6.west)  -| (FT3) ;
        \draw[-latex] (N6.west)  -| (FT2) ;

        \foreach \n in {0,...,6}
        {
            \draw [yshift=-2.9cm, xshift=1.2*\n cm,thick] (0,.4cm)--(0,-.0cm);
            \draw [yshift=-2.9cm, xshift=1.2*\n cm,fill, opacity=0.2] (+.1,0.4) rectangle (+.4,-.0) {};
            \draw [yshift=-2.9cm, xshift=1.2*\n cm,fill, opacity=0.2] (+.5,0.4) rectangle (+.8,-.0) {};
            \draw [yshift=-2.9cm, xshift=1.2*\n cm,thick] (.9,.4cm)--(0.9,-.0cm);
            \coordinate (C\n) at (1.2*\n, -2.9+0.2);
            \coordinate (FB\n) at (1.2*\n +0.25, -2.9 -0.1);
            \coordinate (st\n) at (1.2*\n +0.75, -2.9 +0.5);
        }
        \node[right=1.5cm of C6, anchor=north] (n0) {$\dots$};
        \node[right=2cm of C6] (n1) {$\xrightarrow{\hspace*{0.7cm}}$};
        \node[draw, below=0.7cm of C6] (n3) {start of data = VHERE (from beg. of SUPERSEG)};
        \draw[->] (n3.west) -| (FB0);
        \node[above=0.6cm of st4, anchor=west] (n2) {to higher memory in SUPERSEG};
    \end{tikzpicture} 
    \label{fig:05_04}
    \caption{\textit{Structure of a 1-dimenslonal array in \regc{SUPERSEG}}}
\end{figure}

The array-defining word \bc{1ARRAY} must perform the following
tasks:

\begin{itemize}
    \item place the length and type of the data in the first two cells (4 bytes) of the array descriptor.
    \item place the 32-bit address (start of data) in the next two cells (4 bytes) of the array descriptor.
    \item allot the necessary storage in \bc{SUPERSEG}.
    \item at run-time, place the generalized address, length and type on the parameter stack.
\end{itemize}

The start of data is handled by \bc{VHERE}, a word that puts the next vacant (32-bit) address in \bc{SUPERSEG} on TOS.

We define \bc{VALLOT} to keep track of the storage used by arrays. \bc{VALLOT} increments the pointer in \bc{VHERE} (and aborts with a warning if the segment length is exceeded).

We first define some auxiliary words:
\begin{lstlisting}
    MEMORY 4 + @
    S-> D DCONSTANT MEM.START
        \ beg. of free memory
    40.960 DCONSTANT MAX.PARS \ 40960 = 655360/16
    : TOTAL.PARS MAX.PARS   MEM.START D- ;
        \ # pars avail. mem.
    1 SEGMENT SUPERSEG        \ named seg. 1 byte long
    TOTAL.PARS DROP
        FREE-SIZE             \ tell DOS and HS/FORTH

    DVARIABLE VHERE>
    : INIT.VHERE> 0.0 VHERE> D! ;
    INIT.VHERE>
    : VHERE ( -- d.offset ) VHERE> D@ ;
    : D4/   D2/ D2/ ;
    : D16/  D4/ D4/ ;

    : TOO-BIG? VHERE >SEG.OFF
        0 AND TOTAL.PARS D>
        ABORT" INSUFFICIENT ROOM IN SUPERSEG" ;
        \ check whether new value of VHERE> passes end
        \ of SUPERSEG

    : VALLOT ( d.#bytes -- )
        VHERE  D+ DDUP TOO.BIG?
        VHERE> D! ;

    \ Array-defining words
    \ Ex: 50 LONG REAL*4 1ARRAY V{
    \ V{            (   -- adr )
    \ V{ 17 } >FS   (:: -- V[17])

    : LONG DUP ;
    FIND D, 0= ?( :D, SWAP , , ; )
        \ conditionally compile D,
    :1ARRAY ( I I t -- )
        CREATE UNDER D,  \ t,I into 1st 4 bytes { -- I t )
        SUPERSEG @ 16 M* \ start of SUPERSEG
        VHERE D+ D,      \ abs. address -> next 4 bytes
        #BYTES M*        ( I t -- #bytes to allot)
        VALLOT ;         \ allot space in the segment
    \ run-time action: ( -- adr )
\end{lstlisting}

We also need some words to go with \bc{1ARRAY}:
\begin{lstlisting}
    : } (adr n -- seg.off[n] t )
        SWAP DUP@ R > \ type -> rstack
        4+  D@
        ROT R@ #BYTES M*
        D+ >SEG.OFF R > ; ( -- seg.off[n] t )
\end{lstlisting}

Finally, here is a useful diagnostic word

\begin{lstlisting}
    :7TYPE ( t-- ) \ it's ok for this to be slow!
        DUP 0 = IF DROP ." REAL*4"   EXIT THEN
        DUP 1 = IF DROP ." REAL*8"   EXIT THEN
        DUP 2 = IF DROP ." COMPLEX"  EXIT THEN
        DUP 3 = IF DROP ." DCOMPLEX" EXIT THEN
        ." NOT A DEfiNED DATA TYPE" ABORT ;
\end{lstlisting}

\subsection{2ARRAY and \}\} }
\TallC{We} now want to define arrays of higher dimensionality. For example, to define a 2-dimensional array we might say

\begin{lstlisting}
    90 LONG BY 90 WIDE COMPLEX 2ARRAY XA{{
\end{lstlisting}

This leads to the definitions
\begin{lstlisting}
    : BY ; \a do-nothing word for style
    : WIDE * ; (I w -- I*w)
    : 2ARRAY   (I*w t -- ) 1ARRAY ;
\end{lstlisting}

Now let us define \bc{\}\}} to fetch the double-indexed address:

\begin{lstlisting}
    : }} ( adr m n -- a[m*I+n] t)
        >R OVER 2+ @ ( -- adr m I*w)
        * R> + } ;
\end{lstlisting}

By correct factoring (putting some of the work into \bc{WIDE}) we achieved an easy definition of \bc{2ARRAY}. Careful factoring also let
us define \}\} in terms of \}.

\section{Tuning for speed}

\TallC{Some} of the words in our typed-data/matrix lexicons should be optimized or redefined in machine code. Accessing matrix elements imposes a non-trivial overhead on matrix operations. We can reduce the execution time with inline code, either in the traditional FORTH manner \textit{via} selected assembler definitions, or with a recursive-descent optimizer such as HS/FORTH's\sepfootnote{05_18}.

Experience teaches that optimization is most fruitful (most bang for the buck) applied to entire inner loops and other selected areas of code, rather than to access words \textit{per se}. By hand-coding the innermost loop in matrix inversion and FFT routines, one achieves programs that run in (asymptotically) minimum time on the 8086/8087 chip set.

\TallC{Significant} speed increases in data access could perhaps be obtained with multiple code field (MCF) words, as described by Shaw\sepfootnote{05_19}, and as implemented by HS/FORTH in the words \bc{VAR}, \bc{AT}, \bc{IS}, and variants thereof. The disadvantage of MCF style is that compile-time binding, while faster in execution, loses the flexibility of run-time binding. That is, data types would --as with FORTRAN-- be specified at compile-time, and lexicons would be recompiled to run with specific types. Run-time binding as described in this Chapter produces \textit{generic} words that can handle all four standard scientific data types, a major advantage over MCF.
\\
\leftbar[1\linewidth]
FORTH data structures, especially as defined in this Chapter, do little-- or no bounds checking, hence do not prevent accidentally overwriting key parts of the operating system.

The new fetch and store words were defined in high-level FORTH for safety. Adding bounds-checking to arrays, at least during the debug cycle, is \underline{strongly recommended} to avoid crashing, or even damaging, the system.
\endleftbar