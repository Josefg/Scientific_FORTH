Guano-WNW 231

2))PIVOT (M{{ col-- ::--)

IS COL ZECOL IS LPN

INITIAU

ﬂ: COL row COL}} (--aog.oﬂ[M{$oot,ool}}]t)

> 8 SF >F (87:--|1st.olt)

R> COL1+ §loop|in£gLCOL+1

DO I .
a{{ I }row{ COL }} (- - seg.oﬂ[M{ﬂI,col}}] t)
>FS GABS FS>F(87: — |old.olt Ineweltl)
xoup F< \test' newett > olden
IF FSWAP IISI.PIV THEN FDROP

LOOP \ondloop

FDROP ; \deanupfstack

\ Usage: A{{ 2 ”PIVOT

Mates:

0 We avoid ﬁlling us: the parameter stack with addresses that
have to be Eagle , by putting arguments in named storage
locations (J ). We anticiﬁate setting all the parameters in
the beginning using IN! 21". which can be extended later
if necessary.

0 We have used a word from the COMPLEX arithmetic lexicon,
namely XDUP( FOVER FOVER) to double-copy the contents

of the fstack, since the test F< drops both arguments and
leaves a ﬂag.

0 There is little to be gained by optimizing (and if we did we
should have had to avoid the generic Gx words because they
cannot be optimized by the PIS/FORTH recursive-descent
o timizer) because only}? /2 time is used, ne ligible (for large

compared with the [3 in the innermost oop.

The Gx operations are found in the ﬁle GLIBJ-TH.

To test the word ”PIVOT we can add some lines to the test file:

oncn .(weammmmo.1.2)

on

G! .(A{{O})PIVOT mv .) BL EMIT A{{0)}PNOT IPN.
on .(A{{1})PNOT IPIV .) BL em A{{1}}PNOT IPN .
on .(A{{2})PIVOT IPN .) BL am A({2}}PNOT new.
one:

Chapter 9 - Unear Algebra Scientiﬁc FORTH

The result is the additional screen output

A{{ o }}PIVOT IPIV 1
A{{1}}P|VOT IPIV 1
A{{ 2 }}PIVOT IPIV 2
OR

From our pseudocode expression (see Fig. 9-1, p. 227) of the
pivotal elimination algorithm we see that the next operation is to
multiply a row by a constant. To do this we deﬁne

OVAR ISTEP
\ include phrase

T #BYTES DROP IS ISTEP
\ in INITIALIZE

: DO(ROW*X)LOOP (seg off Lﬁn I.beg - - :: x - - x)
DO DDUP I + DDUP T >FS G'NP TFS>
ISTEP /LOOP DDROP ;

\ G*NP means “(generic) multiply, no pop”

:}}ROW*X M{{ row - :: x — x)
UNDER }row{ 0 A (- - r seg offt)
DROP ROT (- -seg off r)
ISTEP * Length ISTEP * SWAP \Ioop indices
DO(ROW*X)LOOP ; \Ioop

\Ex: A{{ 2 }ROW*X

Mates:

0 While DO(ROW‘X)LO0P and }}ROW‘X clear the param-
eter stack in approved FORTH fashion, they leave the constant
multiplier x on the fstack. That is, we antimpate next multipl -
ing the corresponding row of the inhomogeneous term b (in
the matrix equation A-x = b) by the same constant x.

e We assume INITIALIZE (includin INITJSTEP) has been
invoked before ”PIVOT or ”110 ‘X.

o Anticipating the (possible) need to optimize, we factor the loop
right out of the word. We also com me the addresses fast by
putting base addresses on the stac and then incrementing
them by the cell-size, in bytes.

aims-mum 233

Subtracting row 1 (times a constant) from row I is quite similar to
multiplying a row by a constant. The process can be broken down
into the pseudocoded actions in Fig, 9-3 below.

 

 

® no (K-JTOL)
x e@

M[I,K] e@ G'NP (::- - x M[l,K]'X)
M{J,K] 6@ GR- (::--XM[J,K]-X‘M[I,K])
M{J.K] Gl

LOOP

 

 

Fig. ”Steps In nm-rwx

A ﬁrst attempt might look as follows:

\auxiliarywords

:4DUP DOVER DOVER ;

(abcd--abcdabcd)

: + + (51.0152.02n — $1.01 +n 52.02+n)
DUP>R + ROT R> + -ROT;

:}}R1-R2"X (r1r2-- ::x--x)

> R > R

a R> row 0 DROP \M r10
aila@imioli one.» \Miiaoli
Length iSTEP " \ L'lSTEP (upper limit)

R> ISTEP * \ r2'ISTEP (lower limit)

DO 4DUP I ++ \duplicateaincbaseadrs

T >FS G'NP (2: — xx*M[r2,k])
DDUP T >FS GR-
T >FS \ I result
ISTEP [LOOP DDROP DDROP ;
\ INITIALIZE is assumed

As with HROW‘X we avoid the execution-speed penalty of
calculating addresses within the loop by not using the phrase

emvmim-Mmm.

ChapterO-LhaarNgabra .‘i‘clamlllcFORTHi

a{{ l }row{ J }}.

     
  
 
 
 

However, we are doing a lot of unnecessary work inside the loop
by making 5 choices based on datatype. There are also a lot ,
unnecessary moves to/from the ifstack. This is an example where
speed has been sacriﬁced to compactness of code: one program
solves equations of any (“scientiﬁc”) data format — REAL‘4,
REAL-8, COMPLEX‘8 and COMPIJEX' 16. i

Conversely, we can both eliminate the extra work and accelerate?
execution simply by deﬁning a separate inner loop for each data;
type, letting the choice take place once, outside the inner loop}
This multiplies the needed code fourfold (or more-fold, if we?
optimize).

I

The 4 inner loops are

: Re.LP ($1.01 52.02 L'istep r2*istep - - :2 x - - x)
FS>F >R
DO 4DUP I + + R32@L F*NP
DDUP R32@L FR- R32IL
4 /LOOP R> F>FS ;

: DRe.LP ($1.01 $2.02 L’istep r2'istep - - z: x - - x)
FS>F >R
DO 4DUP l + + R64@L F*NP
DDUP R64@L FR- R64!L
8 /LOOP R> F>FS ;

: X.LP (51.01 52.02 L*istep r2*istep - - z: x - - x)
FS>F >R
DO 4DUP l + + CP@L X‘NP
DDUP CP@L CPR- CPIL
8 /LOOP R> F>FS ;

: DX.LP ($1.01 52.02 L‘istep r2'istep - - :: x - - x)
FS>F >R
DO 4DUP I + + DCP@L X'NP
DDUP DCP@L CPR- DCPIL
16 /LOOP R> F>FS;

Not surprisingly, the loops contain some duplicate code. but this- '
is a small price to pay for the speed increase. A signiﬁcant furthe r3

 

thlIanVNobIatm—Alrlditsraaarvad.

increase can be obtained easily. using the HS/FOR’I'H recursive-
descent optimizer to deﬁne these inner loops, or by redeﬁning
the loops in assembler (for ultimate speed).

Now we can redeﬁne }}Rl-RZ‘X using vectored execution, via
HS/FORI'H’s CASE: . .. :CASE construct, or the equivalent high-
level version given here:

: CASE: CREATE
DOES> OVER + + @ EXECUTE ;
: ;CASE [COMPILE] ; ; IMMEDIATE

CASE: DO(R1-R2"X)LOOP
Re.LP DRe.LP X.LP DX.LP ;CASE

2}}R1-R2‘X (r1 r2-- ::x--x)

>R >R

2112a mm; 328: {main
Length I.STEP * \ ULSTEP (upper limit)
R> I.STEP ' \ r2'I.STEP (lower limit)

T DO(R1-R2'X)LOOP DDROP DDROP ;
\ We assume INITIALIZE has been invoked

Another word is needed to perform the same manipulation on
the inhomogeneous term. Since this latter process runs in 0(N )
time we need not concern ourselves unduly with efficiency.

:}V1—V2*X (r1 r2-- ::x--)
SWAP >R >R
b{ 0.0 } DROP DDUP \V[0] V[O]
R> irowilSTEP‘ + >FS G‘
R> row ISTEP " + DDUP >FS GR- FS> ;

Hats:

0 After }Vl-VZ‘X executes, the multiplier x is no longer needed,
so we drop it here by using 6‘ rather than G‘NP.

We now combine the words }SWAP, }}PIVOT, }}ROW'X.
}Rl-RZ'X and }Vl-VZ‘X to implement the triangularization por-
tion of pivotal elimination.

OJtlhnltNoblaIm-Almmawad.

236

Chapter 9 — Linear Algebra Scientiﬁc FORTH

Since the Gaussian elimination method makes it very easy to
compute the determinant of the matrix as we go, we might as well
do so, especially as it is only an 0(N) process. By evaluating the
detemiinants of simple cases, we realize the determinant is simp-
ly the product of all the pivot elements, multiplied by

(_1)#swaps. Hence we need to keep track of swaps, as well as to
multiply the determinant (at a given stage) by the next pivot
element. The need to do these things has been anticipated in
defining }SWAP above.

Computing the determinant also lets us test as we go, that the
equations can be solved: if at any stage the determinant is zero,
(at least) two of the equations must have been equivalent. Should
it be necessary to test the condition of the equations, this too
can be found as we proceed, by computing the determinant.

Here is a simple recipe for computing the determinant, with
checking to be sure it does not vanish identically. We use the
intelligent fstack (ifstack) deﬁned in Ch. 7.

DCOMPLEX SCALAR DET \ room for any type
:INIT_DEr T 'DEI' I \set Type
TG=1 DEI‘ GI; \setdet=

%1.E-10 FCONSTANT CONDITION

:DETERMINANT (- - 2: x - - x)
DET >FS G*NP
?SWAP IF GNEGATE THEN
FS.DUP GABS FS>F CONDITION F<
ABOR‘l“ determinant too small!‘ DET FS> ;

 

13.

The condition of a system of linear equations refers to how accurately they can be solved. Equa-
tions can be hard to solve precisely if the inverse matrix A“ has some large elements. Thus, the
error vector for a calculated solution, d= —-b-A szlc can be small, but the difference between the
exact solution and the calculated solution can be large, since (see {9&3 below)

xBar-ct ‘cnc =A W6
A test for ill-condition is whether the determinant gets small compared with the precision of the
arithmetic. See, 43.3., A. Ralston,A First Course in Numerical Analysis (McGraw-Hill Book Co.,
New York, 1965).

Ware—Wm 237

Now we deﬁne the word that triangularizes the matrix:

0VARb( \tokeepthesteokehort
: INITIAUZE (M{(V{o- ::--)

le

lSa

3 .TYPE IS T

a D.LEN is Length

a D.TYPE #BYTES DROP ISISTEP

iN .DET ; \setdet=1

:}/PIVOT }row{) DROP DDUP T >FS G' TFS>;
(segofIt--::x--)

:TRIANGULAFIIZE (M{{ V{ - -)

INITIAUZE
Length 0 DO \ loop 1 -— by rows
a{{l} PIVOT \ﬁndpivotincoil
' )row I l.PlV }SWAP \ exchange rows
a{{ I }row{ I }} >FS \pivot->iistack
DETERMINANT \ mic det
1/6 a{{ i }}ROW‘X \row i Ipivot
b{ l }/PIVOT \inhom. term /pivot
Length l1+ DO \ioop2-byrows
a l}row{J}} >FS \x->ifstack
a lJ}}R1-R2"X

\ row[i] = row[i]-row[j]'x
b{ I J }V1-V2*X
\ same for b{ and drop x
LOOP \ and loop 2
LOOP ; \end loop 1
\Usege: A{{ B{ TRIANGULARIZE

Now at last we can back-solve the triangularized equations to ﬁnd

the unknown x 's. The word for this is }BACK-SOLVE, deﬁned
as follows:

238

Chapter 9 - Linear Algebra Scientiﬁc FORTH
: }BACK-SOLVE (- - )
0 LENGTH 2- DO \ outer loop
T G=0
LENGTH I 1 + DO \ inner loop
a? J }row{ I }} > PS
b I }row{ } >FS G*G+\
LOOP \ inner loop
b{ l ){ } DROP DDUP T >FS \
GR- T FS > \
—1 +LOOP ; \outer loop J

Putting the entire program together we have the linear equation
solver given in the ﬁle SOLVEl. Examples of solving dense 3x3
and 4 X4 systems are included for testing purposes.

§§6 Timing

e should like to know how much time it will take to solve a

given system. (Of course it is also useful to know whether the
solution is correct!) We time the solution of 4‘ sets of N equations,
with 4 different values of N. The running time can be expressed
as a cubic polynomial in N with undetermined coefﬁcients:

TN=ao+a1N1+azN2+agN3 (19)

Evaluating 19 for 4 different values of N, and measuring the 4
times TNi , we produce 4 inhomogeneous linear equations in 4

unknowns: a, , i = 0, 1, 2, 3 . As luck would have it, we just hap-

pen to have on hand a linear equation solver, and are thus in a
position to determine these coefﬁcients numerically.

For this timing and testing chore we need an exactly soluble dense
system of linear equations, of arbitrary order. The simplest siqch
system involves a rank-1 matrix, that is, a matrix of the form :

 

14.

We employ the standard notation that a vector u is a column and an adjoint vector VIf '3 a row.
Their outer product, uvi, is a matrix. Given a column v, we construct its adjoint (row) VIt by
taking the complex conjugate of each element and placing it in the corresponding position in a

row-vector.

Gustavo—WNW 23.

A - I - u v' (20)
In terms of matrix elements,

Ai-du-uw' (20’)

The solution of the system A 1 I: b is simple: using the standard
notation

vi-x a (v, x) = 2 v. 'x. (21)
we have
X. = D: 'i' (V, X) U. (22)

The coefﬁcient (v, x) is just a (generally complex) number; we
compute it from 22 via

(V. X) = (V. Ii) + (V. u) (V. X) (23)
Ol’
_ (v, b)
(V. X) — 1 _ (v. u) . (24)

x, = o, + %u (25)

Everythingin 25 is determined, hence the solution is known in
closed form.

An example that embodies the above idea is given in the ﬁle
EX.20, where we make the special choices

Chapter 9 — Unear Algebra Scientiﬁc FORTH

N even

GAO-s
.s
01

We give the times only for the case of a highly optimized inner
loop (written in assembler), for the type REAL‘4:

 

Table 9-1 Execution times for various N (9.54 MHz 8086 + 8087 machine)

 

N Time
20 1.20
50 7.75
75 19.6
100 38.8

 

 

 

The coefﬁcients of the cubic polynomial extracted from the above
are (in seconds, 9.54 MHz 8086 machine, machine-coded inner

loop)

a0 = 032727304
a. = -0.01084850
32 = 000241636
33 = 000001539

from them we can extrapolate the time to solve a 350x350 real
system to be about 16 minutes. On a 25 MHz 80386/8038’7 ma-
chine with 32-bit addrgsing implemented, the time should
decrease ﬁve- to tenfold .

It is interesting to explore a bit further the extracted value of a3,

. . 1
which is 3-
ﬁned above as Re.LP and hand-coded in assembler for ultimate
speed). Converting to clock cycles on an IBM-PC compatible
(running at 9.54 MHz) we have an average of 440 clock cycles per
traversal (of this loop). Recall the operations that are needed: a
32-bit memory fetch, a ﬂoating-point multiply, another 32-bit
memory fetch, a ﬂoating-point subtraction, and a 32-bit store.
The initial fetch-and-multiply can be be compressed into a single
co-processor operation, as can the fetch-and-subtract. The times
in cycles for these basic operations are

the time needed to evaluate the innermost loop (de-

TaUe9—2Executiontlmesotlnnennostioopoperntions

 

Operation lime (cpu clock5)
memory FMUL 133
memory FSUBR 128
memory FSTP 100

overhead 61

 

Total 422

 

 

 

 

15. thnlthinkthatsolvinglmxlﬂhystems —keytomyPh.D.researchin1966— tookthebetter
putofanhornonanlnumnheseresuhsseeminaedible.

16. See WP.

emvmrm—umw.

242

§3 Matrix Inversion

§§1 Unear transformations

  
 
    
   
   
  

ChapterO-LlnaarAlgebra SciandﬂcFOﬂ

We see from Table 9-2 above that the time computed from t
cpu and coprocessor speciﬁcations (422 clocks) is close to
measured time (440 clocks). The slight difference doub
comes both from measurement error and from the fact th
timing of CISC chips is not an exact art (for example, there a
periodic interruptions for dynamic memory refreshment and f
the system clock).

Finally, we conﬁrm that little is to be gained by optimizing out
loops. Suppose, «5.3., we could halve a; by clever programmin
then we should cut the N =350 time from 16 to 13 minutes,
the time for N = 1000 by some 20 minutes in 5 hours, or 6%.

We introduce this subject with a brief discourse on liners
algebra of square matrices.

Suppose A is a square (NXN) matrix and x is an N-dimensiomi
vector (a column, or NXI matrix). We can think of the symbolic
operation

y= A -x (26)

 

as a linear transformation of the column x to a new column y. In
terms of matrix elements and components,

N—l
Ym= E Amnxn (27)

n-o

The transformation is linear because if x is the sum of 2 columns
(added component by component)

it = x“) + is), (23) _

l

we can calculate A 1 either by ﬁrst adding the two vectors ant:
then transforming, written

A-x-A-(am+am) (29)

or we could transform ﬁrst and then add the transformed vectors.
The identity of the results is called the distribatlvs law

A-x-A-(xm+a(2)) “amuse (30)
of linear transformations.

“2 Matrix m
Now, suppose we had several square matrices, A, B, 0,... We
could imagine performing successive linear transformations on a

vectorxvia
y= A -x (31a)
2= e -y (31b)
w = C -1 (31¢)

These can conveniently be written

w=c-[B-(A-x) . (32)

 

The concept of successive transformations leads to the idea of
multiplying two matrices to obtain a third:

D=B-A E=C-D (33)
In terms of matrix elements we have, for example
N-I
Diir ' 1% all All- (34)

The important point is that the (matrix) multiplications may be
performed in any order, so long as the left-to-right ordering of
the factors is maintained:

oJilthNobinm-Alnohtaraaawad.

Ci'iaptoro-LInaarAigabra Salem

   
  
  
  
  

c- (a -A) -=- (c .n) -A (35)

Equation 35 is known as the associative law of matrix multipli
tion. Finally, we note that —as hinted above— the left-to-ri
order of factors in matrix multiplication is signiﬁcant. That is, i
general,

A-B¢B-A (36) 1

We say that, unlike with ordinary or even complex arithme '
matrix multiplication —even of s uare matrices— does not i
general obey the commutative law .

§§3 Matrix Inversion
With this introduction, what does it mean to invert a matrix? First
of all, the concept can apply only to a square matrix. Given an
NXN matrix A, we seek another N XN matrix A with theproper-
ty that

 

A“-AaA-A"=I (37)

where I is the unit matrix deﬁned in the beginning of this chapte r
(Eq. 4 — 1’s on the main diagonal, 0’s everywhere else). We have:
implied in Eq. 37 that a matrix that is an inverse with respect to
left-multiplication is also an inverse with respect to right-multi-
plication. Put another way, we imply that

(r‘ )“ a A (38)

The condition that - given A— we can construct A'1 is the same.|
as the condition that we should be able to solve the linear equa-(
tion

A-I-b;

t namely, det(A) u 0.

5‘4 W invert matrices, W
We have developed a linear equation solver program already
—why should we be interested in a matrix inversion program?

Here is why: The time needed to solve a single system (that is,
with a given inhomogeneous term) of linear equations is condi-
tioned by the number of ﬂoating-point multiplications required:

about 143/111): number needed to invert the matrix is about N3,
roughly 3X as many, meaning roughly 3x as long to invert as to
solve, if the matrix is large. Clearly there is no advantage to
inverting unless we want to solve a number of equations with the
same coefﬁcient matrix but with different inhomogeneous terms.
In this case, we can write

x=A'Lb a”

and just recalculate for each h. Clearly this breaks even — relative
to solving 3 sets of equations — for 3 different b's and is superior
to re-solving for more than 3 Us

s55 Anmmpie

Let us now calculate the inverse of our 3x3 matrix from before.
The equation is (let c = A " be the inverse“)

105 000301 can 100
3 2 4 C10 C“ C12 = 0 1 0 (40)
116 CanCa 001

 

18. Note:ifCisaright-inverse,AC-l,itkalsoa|dl-inverse,CA=l.

OJUIUIVNanim-Alrldasraaarvad.

246

CMptarO-Urngebra SclentlﬂcFORT

It is easy to see that Eq. 40 is like 3 linear equations, the unkn
being each column of the matrix C. The brute-force method I
calculate A is then to work on the right-hand-side all at once
we triangularize, and back-solve. It is easy to see that
gularization by pivotal elimination leads to

1 2/3 Vs can cat can 0 V3 0
0 1 *‘1/2 C10 Ctr C12 = ‘3': V2 0 (41)
0 O 1 C20 C12 022 V13 “V13 #13 l

results of back-solving, column by column. This is N times a.

if

1
total time required for brute-force inversion is =5 gN3. By keep—

To construct the inverse we replace the right-hand side with thq

much work as back-solving a single set of equations, hence th

ing track of zero elements we could reduce this time by anothel
%N3, thereby obtaining the theoretical minimum (for Gaussian
elimination) of 0(N3). However, the brute-force method is un-
satisfactory for another reason: it takes twice the storage of more
sophisticated algorithms. Modern matrix packages therefore “8:“
LU decomposition for both linear equations and matrix inver~

sion.

 

§4 LU decomposition

We now investigate the LU decomposition algorithm”. Sup-
pose a given matrix A could be rewritten

awam 1m 0 0 }lmﬂm
A: 310311 = L'U = 1401110 0 #11 (42)
.................. 0 0

then the solution of

 

19.

Sec, 2.3., WH. Press, B.P. F'Iannery, SA. Teukolsky and W.T. Vetterling. Numerical Recipe:
(Cambridge University Press, Cambridge, 1986), p. 31H.

owe-mm 241

(L-u)-nL-(u-x) -b (43)
can be found in two steps: First. solve
Lty- b (44)
for
y: U-x (45)
via
looYo = bo
MOYO‘l‘ltIYI =01 (46)
130% + 421Y1+ la)’: = D:
etc.

which can be solved successively by forward substtution. Next
solve 45 successively (by back-substitution) for x:

l‘N—tN-t XN—1 = YN—t
I‘M-2 N-2 XN-z + I‘M—2 N-1 xN—i = YN—z (47)
etc.

The n'th term of Eq. 46 requires n multiplications and n additions.
Since we must sum n from 0 to N-l, we ﬁnd N(N—1)/2 =4 Nz/Z
multiplications and additions to solve all of 46. Similarly, solving

47 requires about N2/2 additions and multiplications. Thus, the
dominant time in solving must be the time to decompose accord-

ing to Eq. 42.

The decomposition time is =5 N 3/3, and the method for decom-
posing is described clearly in Numerical Recipes. The equations
to be solved are

N-l

[a luk/‘h =Araa (48)

constituting N2 + N equations for N2 unknowns. Thus we may
arbitrarily choose I... = 1.

eJuunMNobmm-nmm.

Chapters—UnearNgobra ScbndﬂcFORTM

The equations 48 are easy to solve if we do so in a sensible ordeﬁ
Clearly,

1mk50,m>k il
(49)
”knaos k<n

so we can divide up the work as follows: for each 11, write

 

 

m—l
.umn =Amn — kzo Amkﬂkn ’ m = 0,1,...,’l

(50)

n-l
1m =i(A..... — 2 Amp“) , m =n+1,n+2,...,N-1 -.
.unn i=0

 

Inspection of Eq. 50 makes clear that the terms on the right side!
are always computed before they are needed. We can store tln
computed elements A.“ and uh, in place of the corresponding
elements of the original matrix (on the diagonals we store #4..

since A”, = 1 is known).

To limit roundoff error we again pivot, which amounts to permutl
ing so the row with the largest diagonal element is the currenf
one. Much of the code developed for the Gauss eliminatios
method is applicable, as the ﬁle LU.FI'H shows.

