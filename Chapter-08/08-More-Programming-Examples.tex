\chapter{More Programming Examples}
\startcontents[chapters]
\printcontents[chapters]{}{1}{}

\TallC{In} this chapter we apply some of the FORTH tools we have been developing (complex arithmetic, typed data) to two standard problems in numerical analysis: numerical integration of a function over a definite interval; determining the function of a given form that most closely fits a set of data.

\section{Numerlcal Integration}
We begin by defining the definite integral of a function $f(x)$. Then we discuss some methods for (numerically) approximating the integral. This process is called \textbf{numerical integration} or \textbf{numerical quadrature}. Finally, we write some FORTH programs based on the various methods we describe.

\subsection{The Integral of a function}
The definite integral $\int_{a}^{b}f(x) dx$ is the area between the graph of the function and the x-axis as shown below in Fig. 8-1:

Fig. 8-1 \textit{The integral of a function is the area under the curve.}

We estimate the integral by breaking up the area into narrow
rectangles of width $w$ that approximate the height of the curve at that point and then adding the areas of the rectangles\sepfootnote{08_01}. For rectangles of non-zero width the method gives an approximation. If we calculate with rectangles that consistently protrude above the curve (assume for simplicity the curve lies above the x-axis), and with rectangles that consistently lie below the curve, we capture the exact area between two approximations. We say that we have \textbf{bounded} the integral above and below. In mathematical language,

\begin{equation}
\begin{align}
w \sum_{n=0}^{(b-a/w)} &min[f(a+nw),f(a+nw+w)] \\

&\leq \int_{a}^{b}f(x) dx \\

&\leq w \sum_{n=0}^{(b-a)/w} max[f(a+nw),f(a+nw+w)]
\end{align}
\end{equation}

It is easy to see that each rectangle in the upper bound is about $w\lvert f'(x)\rvert]$ too high\sepfootnote{08_02} on the average, hence overestimates the area by about $\frac{1}{2}w^2\lvert f'(x)\rvert]$. There are $(b-a)/w$ such rectangles, so if $\lvert f'(x)\rvert]$ remains finite over the interval $[a, b]$ the total discrepancy will be smaller than

\begin{align}
\frac{1}{2}w(b-a) max_{a\leq x \leq b} $\lvert f'(x)\rvert]$.
\end{align}

Similarly, the lower bound will be low by about the same amount. This means that if we halve $w$ (by taking twice as many points), the accuracy of the approximation will double. The mathematical definition of $\int_{a}^{b} f(x) dx$ is the number we get by taking the limit as the width $w$ of the rectangles becomes arbitrarily small. We know that such a limit exists because the actual area has been captured between lower and upper bounds that shrink together as we take more points.

\subsection{The fundamental theorem of calculus}
\TallC{Suppose} we think of $\int_{a}^{b}f(x) dx$ as a function --Call it $F(b)$-- of the upper limit, $b$. What would happen if we compared the area $F(b)$ with the area $F(b + \Delta b)$: We see that the difference between the two is (for small $\Delta b$)

\begin{align}
\Delta F(b) = F(b+\Delta b) - F(b) \approx f(b)\Delta b + O((\Delta b)^2) 
\end{align}

so that

\begin{align}
F'(b) = lim_{\Delta b \to 0} \frac{1}{\Delta b} (\int_{a}^{b+\Delta b} dx - \int_{a}^{b}f(x) dx )
\end{align}

Equation 3 is a fancy way to say that integration and differentiation are \textbf{inverse operations} in the same sense as multiplication and division, or addition and subtraction.

This fact lets us calculate a definite integral using the differential equation routine developed in Chapter 6. We can express the problem in the following form:

Solve the differential equation

\begin{align}
\frac{dF}{dx} = f(x)
\end{align}

from $x = a$ to $x = b$, subject to the initial condition

\begin{align}
F(a) = 0.
\end{align}

The desired integral is $F(b)$.

The chief disadvantage of using a differential equation solver to evaluate a definite integral is that it gives us no \textbf{error criterion}. We would have to solve the problem at least twice, with two different step sizes, to be sure the result is sufficxently precise\sepfootnote{08_03}.

\subsection{Monte-Carlo method}
\TallC{The} area under $f(x)$ is exactly equal to the average height $\bar{f}$ of $f(x)$ on the interval $[a, b]$, times the length, $b-a$, of the interval\sepfootnote{08_04}. How can we estimate $\bar{f}$? One method is to sample $f(x)$ at random, choosing $N$ points in $[a,b]$ with a random number generator. Then

\begin{align}
\bar{f} \approx \frac{1}{N} \sum_{n=1}^{N} f(x_n)
\end{align}

and
\begin{align}
\int_{a}^{b} f(x) dx \approx (b-a)\bar{f}
\end{align}

This random-sampling method is called the \textbf{Monte-Carlo} method (because of the element of chance).

\subsubsection{Uncertainty of the Monte-Carlo method}
The statistical notion of \textbf{variance} lets us estimate the accuracy of the Monte-Carlo method: The variance in $f(x)$ is

\begin{align}
Var(f) &= \int_{-\inf}^{+\inf} \rho(f) (f- \bar{f})^2 df \\
&\approx \frac{1}{N} \sum_{n=1}^{N} (f(x_n) - \bar{f})^2
\end{align}

(here $\rho(f)df$ is the probability of measuring a value of $f$ between $f$ and $f + df$).

Statistical theory says the variance in estimating \bar{f} by random sampling is

\begin{align}
Var(\bar{f}) = \frac{1}{N} Var(f)
\end{align}

\ie, the more points we take, the better estimate of $\bar{f}$ we obtain. Hence the uncertainty in the integral will be of order

\begin{align}
\Delta \left(int_{a}^{b}f(x)dx\right) \approx \frac{(b-a)\sqrt{Var(f)}}{\sqrt{N}}
\end{align}

and is therefore guaranteed to decrease as $\frac{1}{\sqrt{N}$}.

It is easy to see that the Monte-Carlo method converges slowly.

Since the error decreases only as $\frac{1}{\sqrt{N}}$, whereas even so crude a rule as adding up rectangles (as in \S\ 1 \S\ \S\ 1) has an error term that decreases as $1/N$, what is Monte-Carlo good for?

Monte-Carlo methods come into their own for multidimensional integrals, where they are much faster than multiple one-dimensional integration subroutines based on deterministic rules.

\subsubsection{A slmple Monte-Carlo program}
Following the function protocol and naming convention developed in Ch. 6 \S\ 1 \S\ \S\ 3.2, we invoke the integration routine \textit{via}

\begin{lstlisting}
    USE( F.name \% L.lim \% U.lim \% err )MONTE
\end{lstlisting}

We pass \bc{)MONTE} the name \bc{F.name} of the function $f(x)$, the limits of integration, and the absolute precision of the answer. The answer should be left on the ifstack. \bc{L.lim}, \bc{U.lim}, and \bc{err} stand for explicit floating point numbers that are placed on the 87stack by \bc{\%}\sepfootnote{08_05}. The word \bc{\%} appears explicitly because in a larger program --of which \bc{)MONTE} could be but a portion-- we might want to specify the parameters as numbers already on the 87stack. Since this is intended to be an illustrative program we keep the fstack simple by defining \bc{SCALAR}s to put the limits and precision into.

\begin{lstlisting}
    3 REAL*4 SCALARS A B-A E
\end{lstlisting}

The word \bc{INITIALIZE} will be responsible for storing these numbers.

The program uses one of the pseudo-random number generators (\textbf{prng}’s) from Ch. 3 \S\ 5. We need a word to transform prn's --uniformly distributed on the interval (0,1)-- to prn's on the interval (A, B):

\begin{lstlisting}
    : NEW.X RANDOM B-A G@ F* A G@ F+ ;
\end{lstlisting}

The program is described by the simple ﬂow diagram of Fig. 8-2 below:

\begin{lstlisting}
Diagram yo:
    INITIALIZE
    BEGIN
        (B-A)*sigma > E ?
    WHILE
        New.x f(x)
        N = N + 1
        \bar{f} Var(f)
    REPEAT
        I = (B-A)*<f>
\end{lstlisting}

Fig. 8-2 \textit{Flow diagram of Monte Carlo Integration}

From the ﬂow diagram we see we have to recompute $\bar{f} and $Var(\bar{f})$ at each step. From Eq. 5 we see that

\begin{align}
\bar{f}_{N+1} = \bar{f}_N + \frac{f(x_{N+1} - \bar{f}_{N}}{N + 1}
\end{align}

and

\begin{align}
Var_{N+1} = Var_{N} + \frac{(f_{N+1}-\bar{f}_{N})(f_{N+1} - \bar{f}_{N+1})-Var_{N}}{N+1}
\end{align}

Writing the program is almost automatic:

\begin{lstlisting}
    : USE( [COMPILE] ' CFA LITERAL ; IMMEDIATE
    3 REAL*4 SCALARS Av.F old.Av.F Var.F

    : DoAverage             ( n-- n+1 87:f--f )
        Av.F G@ dd.Av.F G!  \ save old.Av
        FDUP 1+             (--n+187:--fl)
        old.Av.F G@         ( 87:--f f old.Av.F )
        FUNDER F-
        DUP S->F F/ F+      ( 87:--f Av.F )
        Av.F G! ;           \ put away \ cont'd below

    : Do.Variance           (n--n 87:f-- )
        FDUP old.Av.F G@    (87:f f old.Av )
        FUNDER F- FSWAP
        Av.F G@ F- F*
        (87:[f-old.Av]*[f-Av] )
        Var.F G@ FUNDER F-
        DUP S->F F/ F+      (87:--Var’)
    Var.F G! ;

    :INITIALIZE             (:adr-- 87: a b e -- )
        IS adr.f
        E G!
        FOVER F- B-A G! A G!
        FINIT
        F=0 Var.F    G!
        F=0 Av.F     G!
        F=0 old.Av.F G!
        0 5 0 DO \ exercise 5 times
            NEW.X adr.f EXECUTE
            Do.Average Do.Variance
        LOOP ;

    : NotConverged? Var.F G@ FSORT
        B-A G@ F* E G@ F> ;
    : DEBUG DUP
        10 MOD              \ every 10 steps
      0= IF CR DUP.
        Av.F  G@ F.
        Var.F G@ F. THEN ;

    : )MONTE
        INITIALIZE
        BEGIN DEBUG NotConverged?
        WHILE NEW.X adr.f EXECUTE
            Do.Average Do.Variance
        REPEAT
        DROP Av.F G@ B-A G@ F* ;
\end{lstlisting}

The word \bc{DEBUG} is included to produce useful output every 10 points as an aid to testing. The final version of the program need not include \bc{DEBUG}, of course. Also it would presumably be prudent to \bc{BEHEAD} all the internal variables.

The chief virtue of the program we have just written is that it is easily generalized to an arbitrary number of dimensions. The generalization is left as an exercise.

\section{Adaptlve methods}
\TallC{Obviously}, to minimize the execution time of an integration subroutine requires that we minimize the number of times the function $f(x)$ has to be evaluated. There are two aspects to this:
\begin{itemize}
    \item First, we must evaluate $f(x)$ only once at each point $x$ in the interval.
    \item Second, we evaluate $f(x)$ more densely where it varies rapidly than where it varies slowly. Algorithms that can do this are called \textbf{adaptive}.
\end{itemize}

To apply adaptive methods to Monte Carlo integration, we need an algorithm that biases the sampling method so more points are chosen where the function varies rapidly. Techniques for doing this are known generically as \textbf{stratified sampling}\sepfootnote{08_06}. The difficulty of automating stratified sampling for general functions puts adaptive Monte Carlo techniques beyond the scope of this book.

However, adaptive methods can be applied quite easily to deterministic quadrature formulae such as the \textbf{trapezoidal rule} or \textbf{Simpson’s rule}. Adaptive quadrature is both interesting in its own right and illustrates a new class of programming techniques, so we pursue it in some detail.

\section{Adaptive Integration on the real line}
\TallC{We} are now going to write an adaptive program to integrate an arbitrary function $f(x)$, specified at run-time, over an arbitrary interval of the $x$-axis, with an absolute precision specified in advance. We write the integral as a function of several arguments, once again to be invoked following Ch. 6 \S\ 1.3.2:

\begin{lstlisting}
    USE( F.name % L.lim % U.lim % err )INTEGRAL
\end{lstlisting}

Now, how do we ensure that the routine takes a lot of points when the function $f(x)$ is rapidly varying, but few when $f(x)$ is smooth? The simplest method uses \textbf{ecursion}\sepfootnote{08_07}.

\subsection{Digression on recursive algorithms}
\TallC{We} have so far not discussed recursion, wherein a program calls itself directly or indirectly (by calling a second routine that then calls the first).

Since there is no way to know \textit{a priori} how many times a program will call itself, memory allocation for the arguments must be dynamic. That is, a recursive routine places its arguments on a stack so each invocation of the program can find them. This is the method employed in recursive compiled languages such as Pascal, C, or modern BASIC. Recursion is of course natural in FORTH since stacks are intrinsic to the language.

\TallC{We} illustrate with the problem of finding the greatest common divisor (gcd) of two integers. Euclid\sepfootnote{08_08} devised a rapid algorithm for finding the gcd\sepfootnote{08_09} which can be expressed symbolically as

\begin{align}
    gcd(u,v) = u, v= 0
    gcd(v, u mod v) else
\end{align}
    
That is, the problem of finding the gcd of $u$ and $v$ can be replaced by the problem of finding the gcd of two much smaller numbers. A FORTH word that does this is\sepfootnote{08_10}

\begin{lstlisting}
: GCD           ( u v--gcd )
    ?DUP 0 >    \ stopping criterion
    IF UNDER MOD RECURSE THEN ;
\end{lstlisting}

Here is a sample of \bc{GCD} in action, using \bc{TRACE}\sepfootnote{08_11} to exhibit the rstack (in hex) and stack (in decimal):

\begin{lstlisting}
784 48 TRACE GCD
                            rstack      stack
: GCD                                   784 48
    DUP                                 784 48  48
    0=                                  784 48   0
    0BRANCH< 8 >0                       784 48
    UNDER                               48  784 48
    MOD                                 48  16
    : GCD                               48  16
      DUP                   4B76        48  16  16
      0=                    4B76        48  16   0
      0BRANCH< 8 >0         4B76        48  16
      UNDER                 4B76        16  48  16
        MOD                 4B76        16   0
        : GCD               4B76        16   0
          DUP               4B76 4B76   16   0   0
          0=                4B76 4B76   16   0  65535
          0BRANCH< 8 >-1    4B76 4B76   16   0
          DROP              4B76 4B76   16
        EXIT                4B76        16
    EXIT                                16
\end{lstlisting}

Note how \bc{GCD} successively calls itself, placing the same address (displayed in hexadecimal notation) on the rstack, until the stopping criterion is satisfied.

Recursion can get into difficulties by exhausting the stack or rstack. Since the stack in \bc{GCD} never contains more than three numbers, only the rstack must be worried about in this example.

\TallC{Recursive} programming possesses an undeserved reputation or slow execution, compared with nonrecursive equivalent programs\sepfootnote{08_12}. Compiled languages that permit recursion --\eg, BASIC, C, Pascal-- generally waste time passing arguments to subroutines, \ie recursive routines in these languages are slowed by parasitic calling overhead. FORTH does not suffer from this speed penalty, since it uses the stack directly.

Nevertheless, not all algorithms should be formulated recursively. A disastrous example is the Fibonacci sequence

\begin{align}
F_0 = 0, F_1 = 1, F_n = F_{n-1} + F_{n-2}
\end{align}

expressed recursively in FORTH as
\begin{lstlisting}
    : FIB                   ( :n--F[n] )
        DUP 0> NOT
        IF DROP 0 EXIT THEN
        DUP 1 = 
        IF DROP 1 EXIT THEN \ n > 1
        1- DUP 1-           ( -- n-1 n-2 )
        RECURSE SWAP        ( -- F[n-2] n-1 )
        RECURSE + ;
\end{lstlisting}

This program is vastly slower than the nonrecursive version below, that uses an explicit \bc{DO} loop:
\begin{lstlisting}
    : FIB                   ( :n--F[n] )
        0 1 ROT             ( :0 1 n )
        DUP 0> NOT
        IF DDROP EXIT THEN
        DUP 1 =
        IF DROP PLUCK EXIT THEN
        1 DO UNDER + LOOP PLUCK ;
\end{lstlisting}
Why was recursion so bad for Fibonacci numbers? Suppose the running time for $F_n$ is $T_n; then we have

\begin{align}
T_{n} \approx T_{n-1} + T_{n-2} + \tau
\end{align}

where $\tau$ is the integer addition time. The solution of Eq. 12 is

\begin{align}
T_{n} = \tau\left[(\frac{1+\sqrt{5}}}{2})^n - 1\right]
\end{align}

That is, the execution time increases \textbf{exponentially} with the size of the problem. The reason for this is simple: recursion managed to replace the original problem by two of nearly the same size, \ie recursion nearly \textbf{doubled} the work at each step!

\TallC{The} preceding analysis of why recursion was bad suggests how recursion can be helpful: we should apply it whenever a given: problem can be replaced by --say-- two problems of \textbf{half} the original size, that can be recombined in n or fewer operations. An example is \textbf{mergesort}, where we divide the list to be sorted into two roughly equal lists, sort each and then merge them:

\begin{verbatim}
    subroutine sort(list[0,n])
        partition(list, list1, list2)
        sort(list1)
        sort(list2)
        merge(list1, list2, list)
    end
\end{verbatim}

In such cases the running time is

\begin{align}
T_{n} \approx T_{n/2} + T_{n/2} + n = 2T_{n/2} + n
\end{align}

for which the solution is

\begin{align}
T_{n} \approx n log_2 (n)
\end{align}

(In fact, the running time for \textit{mergesort} is comparable with the fastest sorting algorithms.) Algorithms that subdivide problems in this way are said to be of \textbf{divide and conquer} type.

\TallC{Adaptive} integration can be expressed as a divide and conquer algorithm, hence recursion can simplify the program. In pseudocode (actually QuickBasic\textregistered) we have the program shown
below.

\begin{verbatim}
    function simpson(f, a, b)
        c = (a + b)/2
        simpson = (f(a) + f(b) + 4*f(c)) * (b - a) /6
    end function

    function integral(f, a, b. error)
        c = (a + b)/2
        old.int = simpson(f, a, b)
        new.int = simpson(f, a, c) + simpson(f, c, b)
        if abs(old.int - new.int) < error then
            integral = (16*new.int - oid.int) /15
        else
            integral = integral(f, a, c, error/2) +
                       integral(f, c, b. error/2)
        end if
    end function
\end{verbatim}

Clearly, there is no obligation to use Simpson's rule on the sub-intervals: any favorite algorithm will do.

To translate the above into FORTH, we decompose into smaller parts. The name of the function representing the integrand (actually its execution address or \textbf{cfa}) is placed on the stack by \bc{USE(}, as in Ch. 8 \S\ 1.3.2 above. Thence it can be saved in a local variable --either on the rstack or in a \bc{VAR} or \bc{VARIABLE} that can be \bc{BEHEAD}ed-- so the corresponding phrases

\begin{lstlisting}
    R@   EXECUTE  \rstack
    name EXECUTE  \VAR
    name EXECUTE@ \VARIABLE
\end{lstlisting}

evaluate the integrand. Clearly the limits and error (or \textbf{tolerance}) must be placed on a stack of some sort, so the function can call itself. One simple possibility is to put the arguments on the 87stack itself. (Of course we then need a software fstack manager to extend the limited 87stack into memory, as discussed in Ch. 4 \S\ 7.) Alternatively, we could use the intelligent fstack (ifstack) discussed in Ch. 5 \S\ 2.5. We thus imagine the fstack to be as deep as necessary.

The program then takes the form\sepfootnote{08_13} shown on p. 171 below.

Note that in going from the pseudocode to FORTH we replaced \bc{)INTEGRAL} by \bc{RECURSE} inside the word \bc{)INTEGRAL}. The need for this arises from an ideosyncrasy of FORTH: normally words do not refer to themselves, hence a word being defined is hidden from the dictionary search mechanism (compiler) until the final \bc{;} is reached. The word \bc{RECURSE} unhides the current name, and compiles its \textbf{cfa} in the proper spot\sepfootnote{08_14}.

\begin{lstlisting}
    : USE( [COMPILE] ' CFA LITERAL ;
        IMMEDIATE
    
    : f(x) ( :cfa--cfa ) DUP EXECUTE ;
    
    : )integral (f:a b-- I )
        \ uses trapezoidal rule
        XDUP FR- F2/    ( 87:--a b [b-a]/2 )
        F-ROT f(x) FSWAP f(x) F+ F* ;
    
    : )Richardson   \ R-extrap. for trap. rule
        3 S->F F/ F+ ;  ( 87:I' I--Ii'' )

    DVARIABLE ERR   \ place to store err
    CREAT OLD.I  10 ALLOT
                    \ place to store I[a,b]

    : )INTEGRAL   ( :adr-- 87:a b err--I )
        ERR R32!
        XDUP )integral   ( 87:--a b I )
        OLD.I R80!
        XDUP F+ F2/      ( 87:--a b c=[a+b]/2 )
        FUNDER FSWAP     ( 87:--a c c b )
        XDUP )integral   ( 87:--a c c b I1 )
        F4P F4P          ( 87:--a c c b I1 a c )
        )integral F+     ( 87:--a c c b I1+I2 )
        FDUP OLD.I R80@ F<
        IF )Richardson
            FPLUCK FPLUCK FPLUCK FPLUCK
        ELSE FDROP FDROP ( 87:--a c c b )
            ERR R32@ F2/
            F-ROT F2P    ( 87:--a c err/2 c b err/2 )
            RECURSE      ( 87:--a c err/2 I[c,b] )
            F3R F3R F3R RECURSE F+
        THEN DROP ;
\end{lstlisting}

\subsubsectiot{Disadvantages of recursion in adaptive integration}
\TallC{The} main advantage of the recursive adaptive integration algorithm is its ease of programming. As we shall see, the recursive program is much shorter than the non-recursive one. For any reasonable integrand, the fstack (or ifstack) depth grows only as the square of the logarithm of the finest subdivision, hence never gets too large.

However, recursion has several disadvantages when applied to numerical quadrature:

\begin{itemize}
    \item The recursive program evaluates the function more times than necessary.
    \item It would be hard to nest the function \bc{)INTEGRAL} for multi-dimensional integrals.
\end{itemize}

Several solutions to these problems suggest themselves:
\begin{itemize}
    \item The best, as we shall see, is to eliminate recursion from the algorithm.
    \item We can reduce the number of function evaluations with a more precise quadrature formula on the sub-intervals.
    \item We can use "open" formulas like Gauss-Legendre, that omit the endpoints (see Appendix 8.1).
\end{itemize}

\subsubsection{Adaptive Integration without recursion}
\TallC{The} chief reason to write a non-recursive program is to avoid any repeated evaluation of the integrand. That is, the optimum
is not only the smallest number of points $x_n$ in (A, B] consistent with the desired precision, but to evaluate $f(x)$ once only at each $x_n$. This will be worthwhile when the integrand $f(x)$ is costly to evaluate.

To minimize evaluations of $f(x)$, we shall have to save values $f(x_n)$ that can be re-used as we subdivide the intervals.

The best place to store the $f(x_n)$’s is some kind of stack or array. Moreover, to make sure that a value of $f(x)$ computed at one mesh size is usable at all smaller meshes, we must subdivide into two equal sub-intervals; and the points $x_n$ must be equally spaced and include the end-points. Gaussian quadrature is thus out of the question since it invariably (because of the non-uniform spacings of the points) demands that previously computed $f(x_n)$’s are thrown away because they cannot be re-used.

The simplest quadrature formula that satisfies these criteria is the \textbf{trapezoidal rule} (see Appendix 8.2). This is the formula used in the following program.

To clarify what we are going to do, let us visualize the interval of integration, and mark the mesh points (where we evaluate $f(x)$ with \bc{+}:

Step 1: N=1

We now save (temporarily) $I_0$ and divide the interval in two, computing $I_{0}'$ and $I_1$, on the halves, as shown. This will be one fundamental operation in the algorithm.

Step 2: N = N + 1 = 2

We next compare $I'_0 + I_1$ with $I'_0$, The results can be expressed as a branch in a flow diagram, shown below.

Yes I No
Accumulate: Subdivide right-most
Move everything down

Fig. 8-3 \bc{SUBDIVIDE} \textit{branch in adaptive integration}

If the two integrals disagree, we subdivide again, as in Step 3 and
Step 4 below:

Step3: N=N+1=3

Step 4: N=N+1=4

Now suppose the last two sub-integrals $(I_3 + I'_2) in Step 4 agreed with their predecessor $(I_2); we then accumulate the part computed so far, and begin again with the (leftward) remainder of the interval, as in Step 5:
Step 5:

The flow diagram of the algorithm now looks like Fig. 8-4 below:

Fig. 8-4 \textit{Non-recursive adaptive quadrature}

and the resulting FORTH program is\sepfootnote{08_15}:

\begin{lstlisting}
\ COPYRIGHT 1991 JUUAN V. NOBLE
TASK IN1EGRAL
FIND CP@L 0= ? ( FLOAD COMPLEX )
\ define data-type tokens if not already
FIND REAL*4 0= ?(((
    0 CONSTANT REAL*4
    1 CONSTANT REAL*8
    2 CONSTANT COMPLEX
    3 CONSTANT DCOMPLEX )))

FIND 1ARRAY 0= ?( FLOAD MATRIX.HSF )
\ function usage
: USE( [OOMPILE] ' CFA ; IMMEDIATE

\ BEHEADing starts here
0 VAR N

: inc.N N 1 + IS N ;
: dec.N N 2 - IS N ;

0 VAR type

\ define "stack"
20 LONG REAL*8   1ARRAY X{
20 LONG REAL*4   1ARRAY E{
20 LONG DCOMPLEX 1ARRAY F{
20 LONG DCOMPLEX 1ARRAY I{

2 DCOMPIEXSCALARS old.I final.I
: )imegral ( n-- ) \ trapezoidal rule
    X{ OVER    } G@L
    X{ OVER 1- } G@L
    F- F2/
    F{ OVER    } G@L
    F{ OVER 1- } G@L
    type2 AND
    IF   X+ FROT X*F
    ELSE F+ F* THEN
    I{ SWAP 1- } G!L ;
0 VAR f.name
: f(x) fname EXECUTE ;

: INITIALIZE
    IS type   \ store type
    type F{ !
    type I{ ! \ set types for function
    type ' old.I   !
    type ' final.I ! \ and integral(s)
    type 1 AND X{  !
            \ set type for indep. var.
    E{ 0 } G!L \ store error
    X{ 1 } G!L \ store B
    X{ 0 } G!L \ store A
    IS f.name  \ ! cfa of f(x)
    X{ 0 } G@L f(x) F{ 0 } G!L
    X{ 1 } G@L f(x) F{  1} G!L
    1 IS N
    N )integral
    type 2 AND IF F=0 THEN
    F=0 final.I G!L
    FINIT ;

: E/2 E{ N 1- } G@L F2/ E{ N 1- } G!L ;

: }move.down     ( adr n--)
    } \#BYTES >R ( --seg off )
    DDUP R@ +
                 ( --s.seg S.off d.seg d.off )
    R> CMOVEL ;

: MOVEDOWN
    E{ N 1- }move.down
    X{ N    }move.down
    F{ N    }mova.dcmn ;

: new.X ( 87:--x' )
    X{ N } G@L  X{ N 1- } G@L
    F+ F2/ FDUP X{ N    } G!L ;

\ cont'd. ...

\ INTEGRAL cont'd
: GF.   1 > IF FSWAP E. THEN E. ;
: F@.   DUP>R G@L R> GF. ;
: .STACKS CR ." N"
     8 CTAB  ." X"
    19 CTAB  ." Re[F(X)]"
    31 CTAB  ." Im[F(X)]"
    45 CTAB  ." Re[I]"
    57 CTAB  ." Im[I]"
    71 CTAB  ." E"
    N 2 + 0 DO CR I .
         3 CTAB X{ I } F@
        16 CTAB F{ I } F@
        42 CTAB I{ I } F@
        65 CTAB E{ I } F@
    LOOP
    CR 5 SPACES ." old.I ="     old.I F@.
       5 SPACES ." final.I =" final.I F@. CR ;
CASE: <DEBUG> NEXT .STACKS ;CASE
0 VAR (DEBUG)
: DEBUG-ON  1 IS (DEBUG) 5 #PLACES
: DEBUG-OFF 0 IS (DEBUG) 7 #PLACES
: DEBUG (DEBUG) <DEBUG> ;

: SUBDIVIDE
    N 19 > ABORT" Too many subdivisions!"
    E/2 MOVE.DOWN
    I{ N 1- } DROP old.I #BYTES CMOVEL
        new.X f(x) F{ N } G!L
    N )integral N 1 + )integral ;

: CONVERGED? ( 87:--I[N]+I'[N-1]-I[N-1]:--f )
    I{ N } G@L I{ N 1- } G@L old.I G@L
    type 2 AND
    IF   CP- CP+ CPDUP CPABS
    ELSE  F-  F+  FDUP  FABS
    THEN
    E{ N 1- } G@L F2* F< ;

CASE: g*6 CP*F F* ;CASE
4 S->F 3 S->F F/ FCONSTANT F=4/3

: INTERPOLATE ( 87:I[N]=I'[N-1]-I[N-1]-- )
    F=4/3 type 2/ g*f
    old.I G@L final.I G@L
    type 2 AND
    IF   CP+ CP+
    ELSE  F+  F+ THEN
    final.I G!L ;
\ BEHEADing ends here

: )INTEGRAL ( 97:A B ERR--I[A,B] )
    INITIALIZE
    BEGIN N 0>
    WHILE   SUBDIVIDE   DEBUG
        CONVERGED?     inc.N
        IF INTERPOLATE dec.N
        ELSE type 2 AND
            IF   FDROP
            THEN FDROP
        THEN
    REPEAT final.I G@L ;
BEHEAD" N INTERPOLATE   \ optional
\ USE( F.name % A % B % E type )INTEGRAL
\end{lstlisting}

The nonrecursive program obviously requires much more code than the recursive version. This is the chief disadvantage of a nonrecursive method\sepfootnote{08_16}.

\subsubsection{Example of )INTEGRAL IN USE}
\TallC{The} debugging code ("\bc{DEBUG-ON}") lets us track the execution of the program by exhibiting the simulated stacks. Here is an example, $int_{1}^{2} dx \sqrt{x}$:
\begin{lstlisting}
USE( FSQRT \% 1. \% 2. \% 1.E-3 REAL*4 )INTEGRAL
\end{lstlisting}

N      X            F          I             E
0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 6.5973E-01 5.0000E-04
2 2.0000E+00  1.4142E+00 1.4983E-01 1.2500E-04
old.I = 1.2071E+00  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 3.1845E-01 2.5000E-04
2 1.7500E+00  1.3228E+00 3.4213E-01 2.5000E-04
3 2.0000E+00  1.4142E+00 1.7396E-01 1.2500E-04
old.I = 6.5973E-01  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 3.1845E-01 2.5000E-04
2 1.7500E+00  1.3228E+00 1.6826E-01 1.2500E-04
3 1.8750E+00  1.3693E+00 1.7396E-01 1.2500E-04
4 2.0000E+00  1.4142E+00 0.0000E+00 0.0000E+04
old.I = 3.4213E-01  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 1.5621E-01 1.2500E-04
2 1.6250E+00  1.2747E+00 1.6235E-01 1.2500E-04
3 1.7500E+00  1.3228E+00 1.7396E-01 1.2500E-04
old.I = 3.1845E-01  final.I = 3.4226E+00

N      X            F          I             E
0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 2.9284E-01 2.5000E-04
2 1.5000E+00  1.2247E+00 1.6235E-01 1.2500E-04
old.I = 5.5618E-01  final.I = 6.6087E+00

0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 1.4316E-01 1.2500E-04
2 1.3750E+00  1.1726E+00 1.4983E-01 1.2500E-04
3 1.5000E+00  1.2247E+00 1.7396E-01 1.2500E-04
old.I = 2.9284E-01  final.I = 6.6067E+00

0 1.0000E+00  1.0000E+00 1.2879E-01 1.2500E-04
1 1.1250E+00  1.0606E+00 1.3616E-01 1.2500E-04
2 1.2500E+00  1.1180E+00 1.4983E-01 1.2500E-04
old.I = 2.6475E-01  final.I = 9.5392E+00
1.2189E+00

\begin{align}
\int_{1}^{2}dx\sqrt{x} = \frac{2}{3} \left( 2^{3/2} - 1 \right)
\end{align}

Notice that, although $\sqrt{x}$ is perfectly finite at $x = 0$, its first derivative is not. This is not a problem in the above case, because the lower limit is 1.0.

It is an instructive exercise to run the above example with the limits (0.0, 1.0). The adaptive routine spends many iterations: approaching $x = 0$ (25 in the range [0., 0.0625] \textit{vs.} 25 in the range: [0.0625, 1.0] ). This is a concrete example of how an adaptive routine will unerringly locate the (integrable) singularities of at function by spending lots of time near them. The best answer to this problem is to separate out the bad parts of a function by hand, if possible, and integrate them by some other algorithm that take the singularities into account. By the same token, one shoul always integrate \textit{up to}, but not \textit{through}, a discontinuity in $f(x)$.

\subsection{Adaptive Integration in the Argand plane}
\TallC{We} often want to evaluate the complex integral

\begin{align}
I = \oint_{\Gamma}f(z}dz
\end{align}

where \Gamma is a \textbf{contour} (simple, closed, piecewise tinuous curve) in the complex $z$-plane, and $f(z)$ is an \textbf{analytic}\sepfootnote{08_17} function of $z$.

The easiest way to evaluate 16 is to parameterize $z$ as a function of a real variable $t$; as $t$ runs from A to B, $z(t) traces out the contour. For example, the parameterization

\begin{align}
z(t) = z_0 + R cos(t) + iR sin(t) , 0 \leq t \leq 2\pi
\end{align}

traces out a (closed) circle of radius R centered at $z = z_0$.

We assume that the derivative $\dot{z}(t)\equiv \frac{dz}{dt} can be defined; then the integral 16 can be re-written as one over a real interval, with a complex integrand:

\begin{align}
I = int_{A}^{B} \dot{z}(t)f\left( z(t) \right) dt
\end{align}

Now our previously defined adaptive function \bc{)INTEGRAL} can be applied directly, with \bc{F.name} the name of a \textit{complex} funcan

\begin{align}
g(t) = \dot{z}(t)f\left( z(t) \right),
\end{align}

of the \textit{real} variable $t$.

Here is an example of complex integration: we integrate the function $f(z) = e^{1/z}$ around the unit circle in the counter-clock-wise (positive) direction.

The calculus of residues (Cauchy's theorem) gives

\begin{align}
\oint_{\abs{z}=1}dz e^{1/z} = 2\pi i
\end{align}

We parameterize the unit circle as $z(t) = cos(2\pi t) + i sin(2\pi t)$, hence $\dot{z}(t) = 2\pi iz(t), and we might as well evaluate

\begin{align}
int_{0}^{1} dt z(t) e^{1/z(t)} \equiv 1.


For reasons of space, we exhibit only the first and last few iterationst

\begin{lstlisting}
    FIND FSINCOS 0= ?( FLOAD TRIG )
    : Z(T) F=PI F* F2 FSINCOS ;
    : XEXP FSINCOS FROT FEXP X*F ;
        ( 87:x y--e^x cos[y] e^x sin[y] )
    : G(T) Z(T) XDUP 1/X XEXP X* ;
    DEBUG_ON
    USE( G(T) \% 0 \% 1 \% 1.E-2 COMPLEX )INTEGRAL X.
\end{lstlisting}

N      X            F          I             E
0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 2.9284E-01 2.5000E-04
2 1.5000E+00  1.2247E+00 1.6235E-01 1.2500E-04
old.I = 5.5618E-01    final.I = 6.6087E+00

box this in yo!
Note:
    answer = 1

\section{Fitting functions to data}
\TallC{One} of the, most important applications of numerical analysis is the representation of numerical data in functional form. This includes fitting, smoothing, filtering, interpolating, \etc

A typical example is the problem of table lookup: a program re-
quires values of some mathematical function --$sin(x)$, say-- for arbitrary values of $x$. The function is moderately or extremely time-consuming to compute directly. According to the Intel timings for the 80x87 chip, this operation should take about 8 times longer than a floating point multiply. In some real-time applcations this may be too slow.

There are several ways to speed up the computation of a function. They are all based on compact representations of the funtion --either in tabular form or as coefficients of functions that are faster to evaluate. For example, we might represent $sin(x)$ by a simple polynomial\sepfootnote{08_18}

\begin{align}
sin(x) \approx x \left( 0.994108 - 0.147207x\right),
\end{align}

accurate to better than 1\% over the range $-\frac{\pi}{2}\leq x \leq \frac{\pi}{2}$, that requires but 3 multiplications and an addition to evaluate. This would be twice as fast as calculating $sin(x)$ on the 80x87 chip\sepfootnote{08_19}.

\TallC{To} achieve substantially greater speed requires table look-up. To locate data in an ordered table, we might employ binary search: that is, look at the $x$-value halfway down the table and see if the desired value is greater or less than that. On the average,$log_{2}(N)$ comparisons are required, where $N$ is the length of the table. For a table with 1\% precision, we might need 128 entries, \ie seven comparisons.

Binary search is unacceptably slow --is there a faster method? In fact, assuming an ordered table of equally-spaced abscissae the fastest way to locate the desired $x$-value is \textbf{hashing}, a method for \textit{computing} the address rather than finding it using comparisons. Suppose, as before, we need 1\% accuracy, \ie a 128-point table with $x$ in the range $[0,\pi/2]. To look up a value, we multiply $x$ by $256/\pi \backcong 81.5$, truncate to an integer and quadruple it to get a (4-byte) floating point address. These operations --including fetch to the 87stack-- take about 1.5-2 fp multiply times, hence the speedup is 4-fold.

The speedup factor does not seem like much, especially for a
function such as $sin(x)$ that is built into the fast co-processor. However, if we were speaking of a function that is considerably slower to evaluate (for example one requiring evaluation of an integral or solution of a differential equation) hashed table lookup with interpolation can be several orders of magnitude faster than direct evaluation.

\Tall{We} now consider how to represent data by mathematical functions. This can be useful in several contexts:

\begin{itemize}
    \item The theoretical form of the function, but with unknown parameters, may be known. One might like to \textit{determine} the parameters from the data. For example, one mi t have a lot of data on pendulums: their periods, masses, imensions, \etc The period of a pendulum is given, theoretically, by
    
    \begin{align}
    \tau = \frac{2\pi L}{g}^{1/2} f\left( \frac{L}{r},\frac{m_{bob}}{m_string},\mathellipsis}\right)
    \end{align}
    where L is the length of the string, g the acceleration of gravity, and $f$ is some function of ratios of typical lengths, masses, and other factors in the problem. In order to determine g accurately, one generally fits a function of all the measured factors, and tries to minirmze its deviation from the measured periods. That is, one might try
    
    \begin{align}
    \tau_{n} = \left(frac{2\pi L_{n}}{g}^{1/2}\right) \left[ 1 + \alpha frac{r_{n}}{L_{n}} + \beta\left(\frac{m_{bob}}{m_string}\right)_{n} + \mathellipsis\right]
    \end{lstlisting}
    for the n'th set of observations, with g, $\alpha$, $\beta$, $\mathellipsis$ the unknown parameters to be determined.
    
    \item Sometimes one knows that a phenomenon is basically smoothly varying; so that the wiggles and deviations in observations are noise or otherwise umnteresting. How can we filter out the noise without losing the significant part of the data? Several methods have been developed for this purpose, based on the same principle: the data are represente as a sum of functions from a \textbf{complete} set of functions, with unknown coefficients. That is, if $\varphi_{m}(x)$ are the functions, we say ($y_n$ are the data)

    \begin{align}
    y_n = \sum_{m=0}^{\inf}c_{m}\varphi_{m}(x_{n})
    \end{align}

    Such representations are theoretically possible under general conditions. Then to filter we keep on] a finite sum, retaining the first N (usually simplest and smoothest) functions from the set. An example of a complete set is \textbf{monomials}, $\varphi_{m}(x) = x^{m}$. Another is \textbf{sinusoidal (trigonometric) functions},
        $\sin(2\pi mx), \cos(2\pi mx), 0 \leq x \leq 1$,
    
    used in Fourier-series representation. \textbf{Gram polynomials}, discussed below, comprise a third useful complete set.
\end{itemize}

\TallC{The} representation in Eq. 25 is called \textbf{linear} because the unknown coefficients $c_{m}$ appear to their first power. Thus, if all the data were to double, we see immediately that the $c_{m}$'s would have to be multiplied by the same factor, 2. Sometimes, as in the example of the measurement of g above, the unknown parameters appear in more complicated fashion. The problem of fitting with these more general functional forms is called \textbf{nonlinear} for obvious reasons. The \textbf{simplex algorithm} of Ch. 8 \S\ 2.3 below is an example of a nonlinear fitting procedure.

We are now going to write programs to fit both linear and non-linear functions to data. The first and conceptually simplest of these is the \textbf{Fourier transform}, namely representing a function as a sum of sines and cosines.

\subsection{Fast Fourier transform}
\TallC{What} is a Fourier transform? Suppose we have a function that is \textbf{periodic} on the interval $0 \leq x \leq 2\pi}:

\begin{align}
f(x + 2\pi) = f(x) ;
\end{align}

Then under fairly general conditions the function can be expressed in the form

\begin{align}
f(x) = a_{0} + \sum_{n=1}^{\inf} (a_{n} \cos(nx) + b_{n} \sin(nx))
\end{align}

Another way to write Eq. 26 is

\begin{align}
f(x) = \sum_{-\inf}^{+\inf} c_{n}e^{inx}.
\end{align}

In either way of writing, the $c_{n}$ are called \textbf{Fourier coefficients} of the function $f(x)$. Looking, \eg at Eq. 27, we see that the \textbf{orthogonality} of the sinusoidal functions leads to the expression

\begin{align}
c_{n} = \frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-inx}dx
\end{align}

Evaluating Eq. 28 numerically requires --for given n-- at least 2n pomts\sepfootnote{08_20} Naively, for each $n = 0$ to N-1 we have to do a sum

\begin{align}
c_{n} \approx \sum_{k=1}^{2N}f_{k}e^{-2\pi inx}dx .
\end{align}

which means carrying out $2N^2$ complex multiplications.

The \textbf{fast Fourier transform (FFT)} was discovered by Runge and K\"{o}nig, rediscovered by Danielzslon and Lanaos and \textbf{\underline{re}}-rediscovered by Cooley and Tukey\sepfootnote{08_21}. The FFT algorithm can be expressed as three steps:

\begin{itemize}
    \item Discretize the interval, \ie evaluate $f(x)$ only for
    \begin{align}
        x_{k} = 2\pi \frac{k}{N} , 0 \leq x \leq N-1.
    \end{align}
    Call $f(x_k) \equiv f_k$.
    \item   Express the Fourier coefficients as
    \begin{align}
        c_{n} \approx \sum_{k=0}^{N-1}f_{k}e^{-2\pi inx/N} .
    \end{align}
    \item With $w_{n} = e^{-2\pi inx/N}$, Eq. 29 is an $N-1$’st degree polynomial in $w_{n}$. We evaluate the polynomial using a fast algorithm.
\end{itemize}

To evaluate rapidly the polynomial

\begin{align}
    c_{n} = P_{n}(w_{n}) \equiv \sum_{k=0}^{N-1}f_{k}(w_{n})^k
\end{align}

we divide it into two polynomials of order N/Z, dividing each of those in two, \etc This procedure is efficient only for $N=2^v$, with $v$ an integer, so this is the case we attack.

How does dividing a polynomial in two help us? If we segregate the odd from the even powers, we have, symbolically,

\begin{align}
P_{N}(w) = E_{N/2}(w^{2}) + w O_{N/2}(w^2).
\end{align}

Suppose the time to evaluate $P_N(w)$ is $T_{N}$. Then clearly,

where \lambda is the time to segregate the coefficients into odd and even, plus the time for 2 multiplications and a division. The solution of Eq. 31 is $\lambda(N-1)$. That is, it takes $O(N)$ time to evaluate a polynomial.

However, the discreteness of the Fourier transform helps us here. The reason is this: to evaluate the transform, we have to evaluate $P_{N}(w_{n}) for $N$ values of $w_{n}$. But $w_{n}^{2}$ takes on only $N/2$ values as n takes on N values. Thus to evaluate the Fourier transform for all N values of n, we can evaluate the two polynomials of order $N/2$ for half as many points.

Suppose we evaluated the polynomials the old-fashioned way: it would take $2(N/2)\equiv N$ multiplications to do both, but we need do this only $N/2$ times, and $N$ more (to combine them) so we have $N^{2}/2+N$ rather than $N^{2}$. We have gained a factor 2. Obviously it pays to repeat the procedure, dividing each of the sub-polynomials in two again, until only monomials are left.

Symbolically, the number of multiplications needed to evaluate, a polynomial for N (discrete) values of $w$ is

\begin{align}
\tau_{N} = N\lambda + 2\tau_{N/2}
\end{align}

whose solution is

\begin{align}
\tau_{N} = \lambda N \log_{2}(N) + 2\tau_{N/2}
\end{align}

Although the FFT algorithm can be programmed recursively, it
almost never is. To see why, imagine how the coefficients would be re-shufﬂed by Eq. 30: we work out the case for 16 coefficients, exhibiting them in Table 8-1 below, writing only the indices:

Table 8-1 Bit-reversal for re-ordering discrete data

Start   Step 1  Step 2  Step 3    Bin_0 Bin_3
0       0       0       0         0000  0000
1       2       4       8         0001  1000
2       4       8       4         0010  0100
3       6       12      12        0011  1100
4       8       2       2         0100  0010
5       10      6       10        0101  1010
6       12      10      6         0110  0110
7       14      14      14        0111  1110
0       1       1       1         1000  0001
9       3       5       9         1001  1001
10      5       9       5         1010  0101
11      7       13      13        1011  1101
12      9       3       3         1100  0011
13      11      7       11        1101  1011
14      13      11      7         1110  0111
15      15      15      15        1111  1111

The crucial columns are "Start" and "Step 3". Unfortunately, they are written in decimal notation, which conceals a fact that becomes glaringly obvious in binary notation. So we re-write them in binary in the columns Bin_0 and Bin_3, --and see that the final order can be obtained from the initial order simply by reversing the order of the bits, from left to right!

\TallC{A} standard FORTRAN program for complex FFT is shown below. We shall simply translate the FORTRAN into FORTH as expeditiously as possible, using some of FORTH's simplifications.

\TallC{One} such improvement is a word to reverse the bits in a given integer. Note how clumsily this was done in the FORTRAN program. Since practically every microprocessor permits right-shifting a register one bit at a time and feeding the overﬂow into another register from the right, \bc{B.R} can be programmed easily in machine code for speed. Our fast bit-reversal procedure \bc{B.R} may be represented pictorially as in Fig. 8-5 below.

\begin{verbatim}
  SUBROUTINE FOUR1(DATA, NN, ISIGN)
C
C  from Press, et al., Numerical Recipes, ibid., p. 394
C   
C  ISIGN DETERMINES WHETHER THE FFT
C  IS FORWARD OR BOCKWARD
C
C  DATA IS THE (COMPLEX) ARRAY OF DISCRETE INPUT 
  COMPLEX W, WP, TEMP, DATA(N)
  REAL*8 THETA
  J=0
  DO 11 I=0,N-1       \ begin bit.reversal
    IF (J.GT.I) THEN
      TEMP= DATA(J)
      DATA(J)=DATA(I)
      DATA(I)=TEMP
    ENDIF
    M=N/2
1     IF ((M.GE.1).AND.(J.GT.M)) THEN
      J=J-M
      M=M/2
      GO TO 1
    ENDIF
    J = J + M
11  CONTINUE              \ end bit.reversal
      
      MMAX=1              \ begin Danielson-Lancszos section
2     IF (N.GT.MMAX) THEN
           ISTEP=2*MMAX   \ executed lg(N) times
                          \ init trig recurrence
    THETA=3.14159265358979D0/(ISING*MMAX)
    W4=CEXP(THETA)
    W =DCMPLX(1.D0,0.D0)
    DO 13 M=1,MMAX,2      \ outer loop
      DO 12 I = M,N,ISTEP \ inner loop
        J = I+MMAX        \ total = N times
        TEMP = DATA(J)*W
        DATA(J) = DATA(I)-TEMP
        DATA(I) = DATA(I)+TEMP
12      CONTINUE              \ end inner loop
C
      W=W*WP                  \ trig recurrence
C
13  CONTINUE                  \ end outer loop
    MMAX=ISTEP
    GO TO 2
  ENDIF
  RETURN
\end{verbatim}

Fig. 8-5 \textit{Pictorial representation of bit-reversal}

Bit-reversal can be accomplished in high-level FORTH \textit{via}

\begin{lstlisting}
    : B.R       ( n--n' )   \ reverse order of bits
        0 SWAP  ( -- 0 n )  \ set up stack
        N.BITS 0 DO
            DUP 1 AND       \ pick out 1's bit
            ROT 2* +        \ left-shift 1, add 1's bit
            SWAP 2/         \ right-shift n
        LOOP DROP ;
\end{lstlisting}

\leftbar[1\linewidth]
Note: \bc{N.BITS} is a \bc{VAR}, previously set to $v = \log_{2}(N)$
\endleftbar

We will use \bc{B.R} to re-order the actual data array (even though this is slightly more time-consuming than setting up a list of scrambled pointers, leaving the data alone). We forego indirection for two reasons: first, we have to divide by N (N steps) when inverse-transforming, so we might as well combine this with bit-reversal; second, there are N steps in rearranging and dividing by N the input vector, whereas the FFT itself takes $N\log_{2}(N)$ steps, \ie the execution time for the preliminary N steps is unimportant.

Now, how do we go about evaluating the sub-polynomials to get the answer? First, let us write the polynomials (for our case N = 16) corresponding to taking the (bit-reversed) addresses off the stack in succession, as in Fig. 8-6 below.

Fig. 8-6 \textit{The order of evaluating a 16 pt. FFT}

We see that $w_{n}^{8}$ (for N=16) has only two possible values, $\pm1$. Thus we must evaluate not $16\times8$ terms like $f_{i} + w^{8}f_{i+8}$, but only $16\times2$. Similarly, we do not need to evaluate $16\times4$ terms of form $f_{i} + w^{4}f_{i+4}$f, but only $4\times4$, since there are only 4 possible values of $w^{4}$. Thus the total number of multiplications is

\begin{align*}
    2\times8 + 4\times4 + 8\times2 + 16\times1 = 64 \equiv 16\log_{2}16,
\end{align*}

as advertised. This is far fewer than $16\times16=256$, and the ratio improves with N -- for example a 1024 point FFT is 100 times faster than a slow FT.

We list the FFT program on page 191 below. Since \bc{\}FFT} transforms a one-dimensional array we retain the curly braces notation introduced in Ch. 5. We want to say something like

\begin{lstlisting}
    V{ n.pts FORWARD }FFT
\end{lstlisting}

where \bc{V\{} is the name of the (complex) array to be transformed, \bc{n.pts} (a power of 2) is the size of the array, and the ﬂag-setting words \bc{FORWARD} or \bc{INVERSE} determine whether we are taking a FFT or inverting one.

\TallC{Now} we test the program. Table 8-2 on page 192 contains the weekly stock prices of IBM stock, for the year 1983 (the 52 values have been made complex numbers by adding $0i$, and the table padded out to 64 entries (the nearest power of 2) with complex zeros)\sepfootnote{08_22}. The first two entries (2,64) are the type and length of the file. (The file reads from left to right.)

We FFl‘ Table 8- 2 using the phrase \bc{IBM{ 64 DIRECT }FFT}. The power spectrum of the resulting FFI‘ (Table 8-3) is shown in Fig. 8-7 on page 192 below.

\begin{lstlisting}
\ Complex Fast Fourier Transform

\end{lstlisting}

\TallC{How} do we know the FFT program actually worked? The simplest method is to inverse-transform the transform, and compare with the input file. The FFT and inverse FFT are given, respectively, in Tables 8-3 and 8-4 on page 193 below. Within roundoff error, Table 8-4 agrees with Table 8-2 on page 192.

Table 8-2 \textit{Weekly IBM common stock prices, 1983}

Fig. 8-7 \textit{Power spectrum of FFT of 1983 IBM prices (from Table 63)}

Table 8-3 \textit{FFT of IBM weekly stock prices, 1983}

Table 8-4 \textit{Reconstructed IBM prices (inverse FFT)

\subsection{Gram polynomials}
\TallC{Gram} polynomials are useful in fitting data by the linear least-
squares method. The usual method is based on the following
question: What is the “best" polynomial,

N
P1111) = 207. r". (34)

omvmunm-umm.

Cl'teptere—MoreProgrammingExamplee Sclendiic FORTH ,

(of order N) that I can use to fit some set of M pairs of data points, 1

1

{1"}, k=0, 1, ,M—1 1
k

(with M > N) where f(x) is measured at M distinct values of the
independent variable x ?

The usual answer, found by Gauss, is to minimize the squares of
the deviations (at the points xk ) of the fitting function PN (x) from
the data -- possibly weighted by the uncertainties of the data That
is, we want to minimize the statistic

2 M—1 N
X = 2 (ft _ 2 1’11in
k=0

2 1
“=0 ) 2 (35)

with repect to the N + 1 parameters yl| .
From the differential calculus we know that a function’g first
derivative vanishes at a minimum, hence we differentiate x with

respect to each yn independently, and set the results equal to
zero. This yields N + 1 linear equations in N + 1 unknowns:

2 Anm 7m =ﬂn 1 "=011, , N (36)

where (the symbol 9 means “is defined by”)

A M_1 71+m 1
Anm = 20 (x11 ; (37a)
' k
and
A ”'1 1
fin = ongfk— (37b)

In Chapter 9 we develop methods for solving linear equations.
Unfortunately, they cannot be applied to Eq. 36 for N 2 9 be-
cause the matrixAm approximates a Hilbert matrix,

 

const.
H“ - n+m+1 ’

MI-MWW 195

a particularly virulent example of an exponentially ill-
eondltlonetl matrlx. That is. the roundofl' error in solving 36
grows exponentially with N, and is generally unacceptable We

can avoid roundolf problems by expanding' in polynomials rather
than monomials:

2 g 2 i
z E. (1. lg) 7.12.0.1) of 138)
The matrix then becomes
14— 1
=20 pn(xk)pm(xk)_ of (39a)
and the inhomogeneous term is now
M-1 1
fin = 2 17110:)ka (39b)
k-o 1:

5 there any choice of the polynomials p,,(x) that will eliminate
roundofi'? The best kinds of linear equations are those with
nearly diagonal matrices. We note the sum in Eq. 39a is nearly an
integral, if M is large. If we choose the polynomials so they are
orthogonal with respect to the weight function
W(x) =fi01x. - x) 0(x — ..-),
k

where

0, <0
0(x)={1:20

thenA“, will be nearly diagonal, and well-conditioned.

196

Chapter 3 - More Progmmmhg Examples Scientific FOR!

Orthogonal polynomials play an important role in numerical an:

y%s and applied mathematics. They satisfy (Ithogmality rat
tions of the form

B _ = 1, m=n
I,drw(x)p..(x)p..(x) — a... — {0, ,m (40)
where the weight function w(x) is positive.

For a given w(x) and interval [A,B], we can construct orthogom
polynomials using the Gram-Schmidt orthogonalization process

Denote the integral in Eq. 40 by (p, , Pm ) to save having to writ
it many times. We start with

p—l = 0 )
-l/2
P007) = (fi dr W(x)) = const. ,

and assume the polynomials satisfy the 2-term upward recursia
relation

Pn+1(x) = (an + xbn ) pn(x) + cup..-1(x) (41)

Now apply Eq. 41: assume we have calculated pn and pr, an
want to calculate 17,“. Clearly, the orthogonality property give:

(Pn+1.Pn)=(Pn+hPn-1)=(Pn:Pn-1)=0a

and the assumed normalization gives

(p...p..)=1-

These relations yields two equations for the three unknowns, 0
bn and C n:

 

23.

Polynomials can be thought of as vectors in a space of infinitely many dimensions (“Hilbert'
space). Certain polynomials are like the vectors that point in the (mutually orthogonal) direc.
tions in ordinary 3-dlmensional space, and so are called «diagonal by analog.

awe-mew” 191

a. +0.0». xp.)-o
c. + b. 02.. xp.-.) - 0
We express a. and c, in terms orb, to get
p...(x) = b.[(x - (pi. xp. )) p.(x)
- 02.. xp.-. mam]

We determine the remaining parameter b. by again using the
normalization condition:

(42)

(pn4-1 v pn+l ) = 1

In practice, we pretend b.l =1 and evaluate Eq. 42; then we cal-
culate

b. = Gm. 5.... )‘W. (43)

multiply the (un-normalized) 1-3,,“ by b, , and continue.

The process of successive orthogonalization guarantees that p. is
orthogonal to all polynomials of lesser degree in the set. Why is
this so? By construction,p.,+1.l.p.. and p,,,. Is it tp, .2 ? We
need to ask whether

(pn v (x — an)pn-2) = 0-

But we know that any polynomial of degree N-l can be expressed
as a linear combination of independent polynomials of degrees
0, 1, . N-l. Thus

n-l

(at-amp“E 2 supra) (44)

x-o
and (by hypothesis) pI .L every term of the rhs of Eq. 44. hence
it follows (by mathematical induction) that

pa” J- {pn-thn-Si }.

OJUMVNohletm-Almm.

Were-Moro Programming!“ Scientific FORTH

Let us illustrate the process for Legendre polynomials, defined
by weight w(x) = 1, interval {-1.1}:

_ 1 1/2
PO" 2 I

..................................................................

These are in fact the first three (normalized) Legendre polynom-
ials, as any standard reference will confirm.

Now we can discuss Gram polynomials. While orthogonal poly-

nomials are usually defined with respect to an integral as in
Eq. 40, we might also define orthogonality in terms of a sum, as
in Eq. 39a. That is, suppose we define the polynomials such that.

”—1 { 1, m=n

kgopn(xk)pm(xk);:; E dnm = 0, matn (45)

Then we can construct the Gram polynomials, calculating the
coefficients by the algebraic steps of the Gram-Schmidt process.
except now we evaluate sums rather than integrals. Since p..(x)
satisfies 45 by construction, the coefficients y, in our fitting
polynomial are simply

M-l 1
7.. = E p..(xt) fig; (46)
k-O k
they can be evaluated without solving any coupled linear equa
tions, ill-conditioned or otherwise. Roundoff error thus becomes
irrelevant.

The algorithm for fitting data with Gram polynomials may in
expressed in ﬂow-diagram form:

neodhpomt...x.¢mw. 1.1/0.3.
DOn-ttoN-t (outerloop)
Construct an , cn :
DO k=o to M1 (inner loop)
pn+t(xlr) = (XI: " 8n)Pn(Xu) - crpn-1(Xtr)

sum = sum + (p0,,(x.)) 2w.

Com = an + ft Pn+1(Xk) Wk
LOOP (and inner loop)
cn+1 = cn+1 /sum

DO k =0 to M-1 (normalize)
’ Pn+1(xk) = pn+1(xk)/ mm
LOOP

LOOP

 

 

Fig. 8-5 Construction ofGram polynomials

The required storage is 5 vectors of length M to hold xb p,(xk),

p.,.1(.\:.,),f.l and Wu -:- 1/03 . We also need to store the coefficients
an, c. and the normalizations b. --that is, 3 vectors of length
N < <M-- in case they should be needed to interpolate. The time
involved is approximately 7M multiplications and additions for
each n, giving 7NM. Since N can be no greater than M-l (M data
determine at most a polynomial of degree M-l), the maximum
possible running time is 7M2, which is much less than the time to
solve M linear equations.

In practice, we would never wish to fit a polynomial of order

comparable to the number of data, since this would include the
noise as well as the significant information

emvuouoim-me.

200 ChapterB-MoreProgrammthxamplee SclentlﬂcFORTH

We therefore calculate a statistic called zz/(degrce of freedom)“:
With M data points and an N’th order polynomial, there are
M-N-l degrees of freedom. That is, we evaluate Eq. 38 for fixed
N, and divide by M-N-l. We then increase N by l and do it again
The value of N to stop at is the one where

2
= 1ng
”'2“ M-N-l

stops decreasing (with N) and begins to increase.
The best thing about the xfiN statistic is we can increase N without
having to do any extra work:

2 M-1

ZM,N = 2 (ft -§ l’nPn(xk))2Wk

- (47)
M-l

N
a 2 «oz — 2 (7..)2
k=0 n=0

The first term after E in Eq. 47 is independent of N, and the
second term is computed as we go. Thus we could turn the outel
loop (over N) into a BEGIN WHILE REPEAT loop, in

which N is incremented as long as of”; is larger than 02”“ .
(Incidentally, Eq. 47 guarantees that as we increase N the fitted
curve deviates less and less, on the average, from the measured
points. When N = M-l, in fact, the curve goes through the points
But as explained above, this is a meaningless fit, since all data
contain measurement errors. A fitted curve that passes closer
than 0., to more than about 1/3 of the points is suspect.)

The code for Gram polynomials is relatively easy to write using
the techniques developed in Ch. 5. The program is displayed in
full in Appendix 8.4.

 

24. That is, “chi-squared per degree of freedom".

MFOR‘M

DWI-MW!!!“ 201

WWW

ometimes we must fit data by a function that depends on
parameters in a nonlinear manner. An example is

F
‘- 1—7777) “3’

Although the dependence on the parameter F is linear, that on
the parameters a and X is decidedly nonlinear.

One way to handle a problem like fitting Eq. 48 might be to
transform the data, to make the dependence on the parameters
linear. In some cases this is possible, but in 48 no transformation
will render linear the dependence on all three parameters at once.

Thus we are frequently confronted with having to minimize

numerically a complicated function of several parameters. Let
us denote these by 00, 01 , , BN4 , and denote their possible
range of variation by R. Then we want to find those values of
{ 0}C R that minimize a positive function:

12 (so, ,UN_,) 101;?" 12(00,...9N-,) (49)

One way to accomplish the minimization is via calculus, using a
method known as steepest descents. The idea is to differentiate
the function 12 with respect to each 0,, and to set the resulting N
equations equal to zero, solving for the N 0’s. This is generally a
pretty tall order, hence various approximate, iterative techniques
have been developed. The simplest just steps along in 0-space,
along the direction of the local downhill gradient -sz, until a
minimum is foun . Then a new gradient is computed, and a new
minimum sought .

Aside fromthe labor of computing -V12, steepest descents has
two main drawbacks: first, it only guarantees to find a minimum,
not necessarily (In minimum -- if a function has several local

 

I

I
rninima, steepest descents will not necessarily find the smallest.
Worse, consider a function that has a minimum in the form of a
steep-sided gulley that winds slowly downhill to a declivity --
somewhat like a meandering river's channel. Steepest descents
will then spend all its time bouncing up and down the banks of
the gulley, rather than proceeding along its bottom, since thel
steepest gradient is always nearly perpendicular to the line of thei
channel. 1‘
Sometimes the function 12 is so complex that its gradient is tooI
expensive to compute. Can we find a minimum without evaluating}
partial derivatives? A standard way to do this is called the sins?
plex method. The idea is to construct a simplex -- a set of N +1
distinct and non-degenerate vertices in the N-dimensional 0-
space (“non-degenerate" means the geometrical object, formed
by connecting the N + 1 vertices with straight lines, ha non-zero
N-dimensional volume; for example, if N=2, the simplex is a
triangle.)

We evaluate the function to be minimized at each of the vertices,
and sort the table of vertices by the size of 12 at each vertex, the
best (smallest x2 ) on top, the worst at the bottom. The simplex
algorithm then chooses a new point in 0-space by the a strategy,
expressed as the ﬂow diagram, Fig. 8-8 on page 203 below, thd
in action somewhat resembles the behavior of an amoeba seeking
its food. The key word )MINIMIZE that implements the complex
decision tree in Fig. 8-8 (given here in pseudocode) is

: )MINIMIZE (n.'rter - - 87: rel.error - -)

INITIALIZE
BEGIN done? NOT NN.max < AND
WHILE

REFLECT r> =best?

IF r> =2worst?

IF r<worst? IF STORE.X THEN
HALVE r<worst?
IF STORE.X ELSE SHRINK THEN
ELSE STOREX THEN
ELSE DOUBLE r> =best?
IF STOREXP ELSE STORE.X THEN
THEN
N 1 + IS N SORT
REPEAT ;

summer-M Wat-MW!” 203

usedintheformat

USE( tnarne 20 96 1.6-4 )MINIMIZE

Fleshing out the details is a -by now-- familiar process, so we
leave the program perse to Appendix 85. We also include there
a FORTRAN subroutine for the simplex algorithm, taken from

 

 

 

INHIALIZE \ choose simplex? is am of param. space
BEGIN
Converged? NOT N N.rnax s
WHILE
'—- REFLECT\ worst point thru geoeenter of other points
DONE '0") S «best-Pt) 7
.~
I
Yes No l(x') z l(2worst.pl) ?
DOUBLE '/\’
N Y
xx") s «bestpo ? m r ’0”) < “3”“ 7
l .

Am REPEAT .
Store r'

HALVE
i
f(x") < I(worst.pl) ?
REPEAT A

Store r' SHRINK

REPEAT

 

 

Fig. 0-3 Flow dlagram of the slmplex algorithm

OJUthNohletm—Almraaervod.

204

Ghana 3 — More Programming Examples Scientific FORTH

Numerical Recipes 26, as an example of just how indecipherable
traditional languages can be.

53 Appendices

§§1 Gaussian quadrature

aussian quadrature formulae are based on the following idea:

if we let the points 6,, and weights wll be 2N free parameters
(n runs from 1 to N), what values of them most accurately repre-
sent an integral by the formula

B N
I = [A dx a(x)f(x) = E1 w,,f(§,,) ? (50)

In Eq. 50 0(x) is a (known) positive function and f(x) is the
function we want to integrate. This problem can actually be
solved, and leads to tables of points 5,, and weight coefficients
w,, specific to a particular interval [A,B] and weight function a(x).
Gauss-Legendre integration pertains to [-1,+1] and o(x)= 1.
(Note any interval can be transformed into [-1, + 1].)

The interval [0,00) and 0(x) = e"r leads to Gauss-Laguerre for-

2
mulae, whereas the interval (— oo,+ 00) and 0(x) = e”Jr leads to
Gauss-Hermite formulae.

Finally, we note that the more common integration formulae such
as Simpson’s rule or the trapezoidal rule can be derived on the
same basis as the Gauss methods, except that the points are spe-
cified in advance to be equally spaced and to include the end-
points of the interval. Only the weights w, can be determined as
free fitting parameters that give the best approximation to the”
integral.

For given N Gaussian formulae can be more accurate than equally-
spaced rules, as they have twice as many parameters to play with.

 

26.

Press, et a1. , Numerical Recipes, laid... p. 29“.

mum Owe-MW!” 206

Some FORTH words for 5-point Gauss-Legendre integration:

 

 

N swarm WANT ﬂ
1. ossamroreasss FCONSTANT x1 :mm (arse--0
it ow Pomsrmr wo soda (arse-[mew [sat/2)
N 0.4mm FWANT wt FCNER Fm IOF' F-ROT (87:-Jab)
% QWID WTANT W2 XDIP in” FIX) wt F' F3R+
XDIP x1 FPEGATE rude
:acde (arse-[mam [eat/2) Poo m F‘ F3R+
FOVERF—leFtNDERF+; Mia“ F(X)\~2F' F$+
:reeede (a7:abx--a+b"x) F'F+; XDUP )eFNEGATEreaede
':F3R+(e7:abcx--a+xbc)F3RF+ F-ROT; Pot) waF' F3R+
“2 The trapezoidal rule
ecall (Ch. 8, §1.1) how we approximated the area under a curve
Rby capturing it between rectangles consistently higher- and
lower than the curve; and calculating the areas of the two sets of
rectangles. In practice we use a better approximation: we average
the rectangular upper and lower bounds. The errors tend to
cancel, resulting m
B- A w
w ( 2y (f(A +nw) + f(A +nw+w))
n-O
(51)
-ff 2drf(x)+é(¥) w3max lf”(x)|
A<r<B
Now the error is much smaller28 - of order w2 -- so if we double
the number of points, we ecrease the error four-fold. Yet this
so-called trapezoidal rule requires no more effort (in terms of
the number of function evaluations) than the rectangle rule.
27. See, eg, Abramowitz and Stegm, HMF, p.885.
‘28. we) istheaecondderivative ofﬂx),l.e. the first derivative are).
29. Calledaobecausetheeuwejkfisapptofimatedbystraight lineaepnentsbetweensuecessive

printer": +w.‘l‘husweevaluatetheareasoftrapemidsratherthanrectangles.

“3 Richardson
In the program )INTEGRAL in Ch.8 §1§§5.3 the word INTER.
PO LATE performs Richardson extrapolation for the trapezoid»
a1 "rule. The idea' is this. If we use a given rule, accurate to order
w’I ,to calculate the integral on an interval [a,b], then presumably
the error of using the formula on each half of the interval and
adding the results, will be smaller by 2'". For the trapezoidal rule,
n =2, hence we expect the error from summing two half-inter-
vals to be 4x smaller than that from the whole interval.

Thus, we can write (10 = I: , I'o = I£a+by2 , I, = ﬂaw/2 )
I0 = Iexact + R (523)

Equation 52b is only an approximation because the R that ap-
pears in it is not exactly the same as R in Eq. 52a. We will pretend
the two R’s are equal, however, and eliminate R from the two
equations (8.52a,b) ending with an expression for I...“ :

1...... ==1§(I’o + I. - 1.) (53)

Equation 53 is exactly what appears in INTERPOLATE.


us MW: Orampolynomtata
: Here is a FORTH program for implementing the algorithm
I derived in \S\ 2 \S\ \S\ 2 above.

\begin{lstlisting}
\         CODEed words to simplify fstack management
\ ---------------------------------------------------------------
CODE G(N+1)  4 FMUL.  FCHS.  2 FLD.  6 FSUB.  2 FMUL.  1 FADDP.
             DX DS MOV.  DS POP.   R64  DS: [BX] FST.
             DS DX MOV.  BX POP.   END-CODE
( seg off --  :: s a b w x g[n] g[n-1] -- s a b w x g[n] g[n+1] )

CODE  B(N+1)  2 FXCH.  2 FMUL.  3 FMUL.  1 FXCH.  1 FMUL.
      DX DS MOV.  DS POP.  R64  DS: [BX]  FADD.  DS: [BX] FSTP.
      DS DX MOV.  BX POP.  END-CODE
( seg off --  :: s a b w x g[n] g[n+1] -- s a b w g[n+1] wxg[n+1] )

CODE  A(N+1)  1 FMUL.  DX DS MOV.  DS POP.  R64  DS: [BX] FADD.
              DS: [BX] FSTP.    DS DX MOV.  BX POP.  END-CODE
( seg off --  :: s a b w g[n+1] wxg[n+1] -- s a b w g[n+1] )

CODE C(N+1)  1 FXCH. 2 FMUL".  2 FMUL.  2 FXCH.  1 FMULP.
     DX DS MOV.  DS POP.  R64  DS: [BX] FADD.  DS: [BX] FSTP.
     DS DX MOV.  BX POP.  3 FADDP.  END-CODE
( seg off --   :: s a b w g[n+1] f -- s=s+wg[n+1]**2  a  b )
\ ===============================================================

\ ===============================================================
\            Gram polynomial coding
\ ---------------------------------------------------------------
\ Usage:      A{ B{ C{ G{{ Nmax }FIT
REAL*8 SCALAR  DELTA    0 VAR Nmax

: FIRST.AB's   F=0 a{ 0 0} G!    F=0 b{ 0 0} G!  ;

: INIT.DELTA   ( :: -- g{{ 1 I }} )    F=0  F=0
     M 0 DO  w{ I 0} G@  y{ I 0} G@   F**2  FOVER  F* 
             ( ::s s' -- s s' w wf^2)
             FROT  F+   F-ROT  F+  FSWAP ( :: -- s=s+w  s'=s'+wf^2 )
     LOOP    DELTA G!  FSQRT  1/F  ;

: FIRST.G's  ( :: g{{ 1 I }} -- g{{ 1 I }} ) 
     M 0 DO  F=0  g{{ 0 I }} G!  FDUP  g{{ 1 I }} G!  LOOP  ;

: SECOND.AB's   ( :: g{{ 1 I }} -- g{{ 1 I }} )
     F=0 b{ 1 0}  G!   FDUP  F**2
     F=0  M 0 DO  w{ I 0} G@ x{ I 0} G@  F*  F+  LOOP  
     F*  a{ 1 0} G! ;

: FIRST.&.SECOND.C's  ( :: g{{ 1 I }} -- )
     F=0 c{ 0 0} G!  F=0
     M 0 DO  w{ I 0} G@  y{ I 0} G@  F*  F+   LOOP
     F*  c{ 1 0} G! ;

: INITIALIZE   FINIT   IS Nmax  IS g{{  IS c{ IS b{  IS a{
     FIRST.AB's   INIT.DELTA   FIRST.G's  SECOND.AB's
     FIRST.&.SECOND.C's  ;

0 VAR N   0 VAR N+1
: inc.N   N+1  DUP  IS N   1+  IS N+1 ;

: DISPOSE   >R  DDUP  R@ G@   R> ;  \ just a convenient word
: inc.OFF   #BYTES DROP + ;  \ increment the offset by 1 item

: R64.ZERO.L   ( seg off --)  DDUP  F=0 R64!L  ( R32!L ) ;  

: START.Next.G   ( -- [c{n+1}] [a{n+1}] [b{n+1}]  :: -- s=0 a{n} b{n} )
        FINIT   F=0   c{ N+1 0} DROP  R64.ZERO.L
        a{ N 0} DISPOSE   inc.OFF  R64.ZERO.L
        b{ N 0} DISPOSE   inc.OFF  R64.ZERO.L  ;

: SET.FSTACK   
    w{ I' 0} G@  x{ I' 0} G@   g{{ N I' }} G@  g{{ N 1- I' }} G@   ;

: }@*!  ( adr n --  :: x -- x)  FDUP  0}  DISPOSE  F*  G! ;
\ just another convenient word

: NORMALIZE    ( :: sum -- )  
      1/F  a{ N+1 }@*!   FSQRT   b{ N+1 }@*!   c{ N+1 }@*!
      M 0 DO  FDUP  g{{ N+1 I }}  DISPOSE   F*  G!  LOOP   FDROP  ;

CODE 6DUP  OPT" 6 PICK  6 PICK  6 PICK  6 PICK  6 PICK  6 PICK " END-CODE
CODE 6DROP OPT" DDROP DDROP DDROP " END-CODE

: Next.G   START.Next.G 
      M 0 DO  6DUP    SET.FSTACK
              g{{ N+1 I }} DROP   G(N+1)   B(N+1)   A(N+1)
              y{ I 0} G@   C(N+1)
      LOOP    6DROP   FDROP FDROP   NORMALIZE  ;

: New.DELTA   ( :: -- old.delta new.delta)
      DELTA G@  FDUP   c{ N 0} G@  F**2  F-   FDUP  DELTA G! ;

: NOT.ENUF.G's?   New.DELTA    M N+1 -   S->F   FDUP  F=1 F- 
            ( CR  .FS  ."  NEXT ITERATION?" ?YN  0= IF ABORT THEN )
              ( :: -- d d' m-n-1  m-n-2 )  
              FROT  F\   F-ROT   F/   ( :: -- d'/[m-n-2]  d/[m-n-1] )
              FOVER  F0>  IF  F<=  ELSE  FDROP FDROP 0 THEN  ;


: }FIT  ( X{ Y{ S{ Nmax A{ B{ C{ G{{ -- )
        INITIALIZE    1 IS N   2 IS N+1
        BEGIN      NOT.ENUF.G's?     N Nmax <  AND 
        WHILE      Next.G   inc.N
        REPEAT  FINIT ;
\ ============================================== end of code ;

: RECONSTRUCT  M 0  DO  CR  x{ I 0} G@ F.  y{ I 0} G@ F. 
                 F=0  N+1 1+  1  DO 
                        c{ I 0} G@  g{{ I J }} G@  F* F+ 
                      LOOP   F. 
               LOOP    ;
\end{lstlisting}

\subsubsection{Non-linear least squares: simplex method}
\TallC{A} FORTRAN program for the simplex method is given belton page 209. The FORTH version, as discussed in \S\ 2 \S\ \S\ 3 given on pages 210 and 211.