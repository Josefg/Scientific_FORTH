% Chapter 8
\chapter{More Programming Examples}

\TallC{In} this chapter we apply some of the FORTH tools we have been developing (complex arithmetic, typed data) to two standard problems in numerical analysis: numerical integration of a function over a definite interval; determining the function of a given form that most closely fits a set of data.

\section{Numerical Integration}
We begin by defining the definite integral of a function $f(x)$. Then we discuss some methods for (numerically) approximating the integral. This process is called \textbf{numerical integration} or \textbf{numerical quadrature}. Finally, we write some FORTH programs based on the various methods we describe.

\subsection{The Integral of a function}
The definite integral $\int_{a}^{b}f(x) dx$ is the area between the graph of the function and the x-axis as shown below in Fig. 8-1:

Fig. 8-1 \textit{The integral of a function is the area under the curve.}

We estimate the integral by breaking up the area into narrow rectangles of width $w$ that approximate the height of the curve at that point and then adding the areas of the rectangles\sepfootnote{08_01}. For rectangles of non-zero width the method gives an approximation. If we calculate with rectangles that consistently protrude above the curve (assume for simplicity the curve lies above the x-axis), and with rectangles that consistently lie below the curve, we capture the exact area between two approximations. We say that we have \textbf{bounded} the integral above and below. In mathematical language,

\begin{equation}
    \begin{split}
    w \sum_{n=0}^{(b-a/w)} & \min[f(a+nw),f(a+nw+w)] \\
    & \leq \int_{a}^{b}f(x) dx \\
    & \leq w \sum_{n=0}^{(b-a)/w} \max[f(a+nw),f(a+nw+w)]
    \end{split}
\end{equation}

It is easy to see that each rectangle in the upper bound is about $w\lvert f'(x)\rvert]$ too high\sepfootnote{08_02} on the average, hence overestimates the area by about $\frac{1}{2}w^2\lvert f'(x)\rvert$. There are $(b-a)/w$ such rectangles, so if $\lvert f'(x)\rvert$ remains finite over the interval $[a, b]$ the total discrepancy will be smaller than

\begin{equation*}
\frac{1}{2}w(b-a) \max_{a\leq x \leq b} \lvert f'(x)\rvert.
\end{equation*}

Similarly, the lower bound will be low by about the same amount. This means that if we halve $w$ (by taking twice as many points), the accuracy of the approximation will double. The mathematical definition of $\int_{a}^{b} f(x) dx$ is the number we get by taking the limit as the width $w$ of the rectangles becomes arbitrarily small. We know that such a limit exists because the actual area has been captured between lower and upper bounds that shrink together as we take more points.

\subsection{The fundamental theorem of calculus}
\TallC{Suppose} we think of $\int_{a}^{b}f(x) dx$ as a function --call it $F(b)$-- of the upper limit, $b$. What would happen if we compared the area $F(b)$ with the area $F(b + \Delta b)$: We see that the difference between the two is (for small $\Delta b$)

\begin{equation}
\Delta F(b) = F(b+\Delta b) - F(b) \approx f(b)\Delta b + O((\Delta b)^2) 
\end{equation}

so that

\begin{equation}\label{eq:08_03}
F'(b) = \lim_{\Delta b \to 0} \frac{1}{\Delta b} \left(\int_{a}^{b+\Delta b} dx - \int_{a}^{b}f(x) dx \right)
\end{equation}

Equation \ref{eq:08_03} is a fancy way to say that integration and differentiation are \textbf{inverse operations} in the same sense as multiplication and division, or addition and subtraction.

This fact lets us calculate a definite integral using the differential equation routine developed in Chapter 6. We can express the problem in the following form:

Solve the differential equation

\begin{equation}
\frac{dF}{dx} = f(x)
\end{equation}

from $x = a$ to $x = b$, subject to the initial condition

\begin{equation*}
F(a) = 0.
\end{equation*}

The desired integral is $F(b)$.

The chief disadvantage of using a differential equation solver to evaluate a definite integral is that it gives us no \textbf{error criterion}. We would have to solve the problem at least twice, with two different step sizes, to be sure the result is sufficxently precise\sepfootnote{08_03}.

\subsection{Monte-Carlo method}
\TallC{The} area under $f(x)$ is exactly equal to the average height $\bar{f}$ of $f(x)$ on the interval $[a, b]$, times the length, $b-a$, of the interval\sepfootnote{08_04}. How can we estimate $\bar{f}$? One method is to sample $f(x)$ at random, choosing $N$ points in $[a,b]$ with a random number generator. Then

\begin{equation}
\bar{f} \approx \frac{1}{N} \sum_{n=1}^{N} f(x_n)
\end{equation}

and
\begin{equation}
\int_{a}^{b} f(x) dx \approx (b-a)\bar{f}
\end{equation}

This random-sampling method is called the \textbf{Monte-Carlo} method (because of the element of chance).

\subsubsection{Uncertainty of the Monte-Carlo method}
The statistical notion of \textbf{variance} lets us estimate the accuracy of the Monte-Carlo method: The variance in $f(x)$ is

\begin{equation}
    \begin{split}
    \text{Var}(f) &= \int_{-\infty}^{+\infty} \rho(f) \left(f- \bar{f}\right)^2 df \\
    & \approx \frac{1}{N} \sum_{n=1}^{N} \left(f(x_n) - \bar{f}\right)^2
    \end{split}
\end{equation}

(here $\rho(f)df$ is the probability of measuring a value of $f$ between $f$ and $f + df$).

Statistical theory says the variance in estimating $\bar{f}$ by random sampling is

\begin{equation}
\text{Var}(\bar{f}) = \frac{1}{N} \text{Var}(f)
\end{equation}

\ie, the more points we take, the better estimate of $\bar{f}$ we obtain. Hence the uncertainty in the integral will be of order

\begin{equation}
\Delta \left(\int_{a}^{b}f(x)dx\right) \approx \frac{(b-a)\sqrt{\text{Var}(f)}}{\sqrt{N}}
\end{equation}

and is therefore guaranteed to decrease as $\frac{1}{\sqrt{N}}$.

It is easy to see that the Monte-Carlo method converges slowly.

Since the error decreases only as $\frac{1}{\sqrt{N}}$, whereas even so crude a rule as adding up rectangles (as in \S1\S\S1) has an error term that decreases as $1/N$, what is Monte-Carlo good for?

Monte-Carlo methods come into their own for multidimensional integrals, where they are much faster than multiple one-dimensional integration subroutines based on deterministic rules.

\subsubsection{A slmple Monte-Carlo program}
Following the function protocol and naming convention developed in Ch. 6 \S1\S\S3.2, we invoke the integration routine \textit{via}

\begin{lstlisting}
    USE( F.name \% L.lim \% U.lim \% err )MONTE
\end{lstlisting}

We pass \bc{)MONTE} the name \bc{F.name} of the function $f(x)$, the limits of integration, and the absolute precision of the answer. The answer should be left on the ifstack. \bc{L.lim}, \bc{U.lim}, and \bc{err} stand for explicit ﬂoating point numbers that are placed on the 87stack by \bc{\%}\sepfootnote{08_05}. The word \bc{\%} appears explicitly because in a larger program --of which \bc{)MONTE} could be but a portion-- we might want to specify the parameters as numbers already on the 87stack. Since this is intended to be an illustrative program we keep the fstack simple by defining \bc{SCALAR}s to put the limits and precision into.

\begin{lstlisting}
    3 REAL*4 SCALARS A B-A E
\end{lstlisting}

The word \bc{INITIALIZE} will be responsible for storing these numbers.

The program uses one of the pseudo-random number generators (\textbf{prng}’s) from Ch. 3 \S\ 5. We need a word to transform prn's --uniformly distributed on the interval (0,1)-- to prn's on the interval (A, B):

\begin{lstlisting}
    : NEW.X RANDOM B-A G@ F* A G@ F+ ;
\end{lstlisting}

The program is described by the simple flow diagram of Fig. 8-2 below:

\begin{lstlisting}
Diagram yo:
    INITIALIZE
    BEGIN
        (B-A)*sigma > E ?
    WHILE
        New.x f(x)
        N = N + 1
        \bar{f} Var(f)
    REPEAT
        I = (B-A)*<f>
\end{lstlisting}

Fig. 8-2 \textit{Flow diagram of Monte Carlo Integration}

From the flow diagram we see we have to recompute $\bar{f}$ and $Var(\bar{f})$ at each step. From Eq. 5 we see that

\begin{equation}
\bar{f}_{N+1} = \bar{f}_N + \frac{f(x_{N+1} - \bar{f}_{N}}{N + 1}
\end{equation}

and

\begin{equation}
Var_{N+1} = Var_{N} + \frac{(f_{N+1}-\bar{f}_{N})(f_{N+1} - \bar{f}_{N+1})-Var_{N}}{N+1}
\end{equation}

Writing the program is almost automatic:

\begin{lstlisting}
    : USE( [COMPILE] ' CFA LITERAL ; IMMEDIATE
    3 REAL*4 SCALARS Av.F old.Av.F Var.F

    : DoAverage             ( n-- n+1 87:f--f )
        Av.F G@ dd.Av.F G!  \ save old.Av
        FDUP 1+             (--n+187:--fl)
        old.Av.F G@         ( 87:--f f old.Av.F )
        FUNDER F-
        DUP S->F F/ F+      ( 87:--f Av.F )
        Av.F G! ;           \ put away \ cont'd below

    : Do.Variance           (n--n 87:f-- )
        FDUP old.Av.F G@    (87:f f old.Av )
        FUNDER F- FSWAP
        Av.F G@ F- F*
        (87:[f-old.Av]*[f-Av] )
        Var.F G@ FUNDER F-
        DUP S->F F/ F+      (87:--Var’)
    Var.F G! ;

    :INITIALIZE             (:adr-- 87: a b e -- )
        IS adr.f
        E G!
        FOVER F- B-A G! A G!
        FINIT
        F=0 Var.F    G!
        F=0 Av.F     G!
        F=0 old.Av.F G!
        0 5 0 DO \ exercise 5 times
            NEW.X adr.f EXECUTE
            Do.Average Do.Variance
        LOOP ;

    : NotConverged? Var.F G@ FSORT
        B-A G@ F* E G@ F> ;
    : DEBUG DUP
        10 MOD              \ every 10 steps
      0= IF CR DUP.
        Av.F  G@ F.
        Var.F G@ F. THEN ;

    : )MONTE
        INITIALIZE
        BEGIN DEBUG NotConverged?
        WHILE NEW.X adr.f EXECUTE
            Do.Average Do.Variance
        REPEAT
        DROP Av.F G@ B-A G@ F* ;
\end{lstlisting}

The word \bc{DEBUG} is included to produce useful output every 10 points as an aid to testing. The final version of the program need not include \bc{DEBUG}, of course. Also it would presumably be prudent to \bc{BEHEAD} all the internal variables.

The chief virtue of the program we have just written is that it is easily generalized to an arbitrary number of dimensions. The generalization is left as an exercise.

\section{Adaptlve methods}
\TallC{Obviously}, to minimize the execution time of an integration subroutine requires that we minimize the number of times the function $f(x)$ has to be evaluated. There are two aspects to this:
\begin{itemize}
    \item First, we must evaluate $f(x)$ only once at each point $x$ in the interval.
    \item Second, we evaluate $f(x)$ more densely where it varies rapidly than where it varies slowly. Algorithms that can do this are called \textbf{adaptive}.
\end{itemize}

To apply adaptive methods to Monte Carlo integration, we need an algorithm that biases the sampling method so more points are chosen where the function varies rapidly. Techniques for doing this are known generically as \textbf{stratified sampling}\sepfootnote{08_06}. The difficulty of automating stratified sampling for general functions puts adaptive Monte Carlo techniques beyond the scope of this book.

However, adaptive methods can be applied quite easily to deterministic quadrature formulae such as the \textbf{trapezoidal rule} or \textbf{Simpson’s rule}. Adaptive quadrature is both interesting in its own right and illustrates a new class of programming techniques, so we pursue it in some detail.

\section{Adaptive Integration on the real line}
\TallC{We} are now going to write an adaptive program to integrate an arbitrary function $f(x)$, specified at run-time, over an arbitrary interval of the $x$-axis, with an absolute precision specified in advance. We write the integral as a function of several arguments, once again to be invoked following Ch. 6 \S\ 1.3.2:

\begin{lstlisting}
    USE( F.name % L.lim % U.lim % err )INTEGRAL
\end{lstlisting}

Now, how do we ensure that the routine takes a lot of points when the function $f(x)$ is rapidly varying, but few when $f(x)$ is smooth? The simplest method uses \textbf{ecursion}\sepfootnote{08_07}.

\subsection{Digression on recursive algorithms}
\TallC{We} have so far not discussed recursion, wherein a program calls itself directly or indirectly (by calling a second routine that then calls the first).

Since there is no way to know \textit{a priori} how many times a program will call itself, memory allocation for the arguments must be dynamic. That is, a recursive routine places its arguments on a stack so each invocation of the program can find them. This is the method employed in recursive compiled languages such as Pascal, C, or modern BASIC. Recursion is of course natural in FORTH since stacks are intrinsic to the language.

\TallC{We} illustrate with the problem of finding the greatest common divisor (gcd) of two integers. Euclid\sepfootnote{08_08} devised a rapid algorithm for finding the gcd\sepfootnote{08_09} which can be expressed symbolically as

\begin{equation}
    gcd(u,v) = u, v= 0
    gcd(u,v) (v, u mod v) else
\end{equation}
    
That is, the problem of finding the gcd of $u$ and $v$ can be replaced by the problem of finding the gcd of two much smaller numbers. A FORTH word that does this is\sepfootnote{08_10}

\begin{lstlisting}
: GCD           ( u v--gcd )
    ?DUP 0 >    \ stopping criterion
    IF UNDER MOD RECURSE THEN ;
\end{lstlisting}

Here is a sample of \bc{GCD} in action, using \bc{TRACE}\sepfootnote{08_11} to exhibit the rstack (in hex) and stack (in decimal):

\begin{lstlisting}
784 48 TRACE GCD
                            rstack      stack
: GCD                                   784 48
    DUP                                 784 48  48
    0=                                  784 48   0
    0BRANCH< 8 >0                       784 48
    UNDER                               48  784 48
    MOD                                 48  16
    : GCD                               48  16
      DUP                   4B76        48  16  16
      0=                    4B76        48  16   0
      0BRANCH< 8 >0         4B76        48  16
      UNDER                 4B76        16  48  16
        MOD                 4B76        16   0
        : GCD               4B76        16   0
          DUP               4B76 4B76   16   0   0
          0=                4B76 4B76   16   0  65535
          0BRANCH< 8 >-1    4B76 4B76   16   0
          DROP              4B76 4B76   16
        EXIT                4B76        16
    EXIT                                16
\end{lstlisting}

Note how \bc{GCD} successively calls itself, placing the same address (displayed in hexadecimal notation) on the rstack, until the stopping criterion is satisfied.

Recursion can get into difficulties by exhausting the stack or rstack. Since the stack in \bc{GCD} never contains more than three numbers, only the rstack must be worried about in this example.

\TallC{Recursive} programming possesses an undeserved reputation or slow execution, compared with nonrecursive equivalent programs\sepfootnote{08_12}. Compiled languages that permit recursion --\eg, BASIC, C, Pascal-- generally waste time passing arguments to subroutines, \ie recursive routines in these languages are slowed by parasitic calling overhead. FORTH does not suffer from this speed penalty, since it uses the stack directly.

Nevertheless, not all algorithms should be formulated recursively. A disastrous example is the Fibonacci sequence

\begin{equation}
F_0 = 0, F_1 = 1, F_n = F_{n-1} + F_{n-2}
\end{equation}

expressed recursively in FORTH as
\begin{lstlisting}
    : FIB                   ( :n--F[n] )
        DUP 0> NOT
        IF DROP 0 EXIT THEN
        DUP 1 = 
        IF DROP 1 EXIT THEN \ n > 1
        1- DUP 1-           ( -- n-1 n-2 )
        RECURSE SWAP        ( -- F[n-2] n-1 )
        RECURSE + ;
\end{lstlisting}

This program is vastly slower than the nonrecursive version below, that uses an explicit \bc{DO} loop:
\begin{lstlisting}
    : FIB                   ( :n--F[n] )
        0 1 ROT             ( :0 1 n )
        DUP 0> NOT
        IF DDROP EXIT THEN
        DUP 1 =
        IF DROP PLUCK EXIT THEN
        1 DO UNDER + LOOP PLUCK ;
\end{lstlisting}
Why was recursion so bad for Fibonacci numbers? Suppose the running time for $F_n$ is $T_n$; then we have

\begin{equation}
T_{n} \approx T_{n-1} + T_{n-2} + \tau
\end{equation}

where $\tau$ is the integer addition time. The solution of Eq. 12 is

\begin{equation}
T_{n} = \tau\left[(\frac{1+\sqrt{5}}{2})^n - 1\right]
\end{equation}

That is, the execution time increases \textbf{exponentially} with the size of the problem. The reason for this is simple: recursion managed to replace the original problem by two of nearly the same size, \ie recursion nearly \textbf{doubled} the work at each step!

\TallC{The} preceding analysis of why recursion was bad suggests how recursion can be helpful: we should apply it whenever a given: problem can be replaced by --say-- two problems of \textbf{half} the original size, that can be recombined in n or fewer operations. An example is \textbf{mergesort}, where we divide the list to be sorted into two roughly equal lists, sort each and then merge them:

\begin{verbatim}
    subroutine sort(list[0,n])
        partition(list, list1, list2)
        sort(list1)
        sort(list2)
        merge(list1, list2, list)
    end
\end{verbatim}

In such cases the running time is

\begin{equation}
T_{n} \approx T_{n/2} + T_{n/2} + n = 2T_{n/2} + n
\end{equation}

for which the solution is

\begin{equation}
T_{n} \approx n log_2 (n)
\end{equation}

(In fact, the running time for \textit{mergesort} is comparable with the fastest sorting algorithms.) Algorithms that subdivide problems in this way are said to be of \textbf{divide and conquer} type.

\TallC{Adaptive} integration can be expressed as a divide and conquer algorithm, hence recursion can simplify the program. In pseudocode (actually QuickBasic\textregistered) we have the program shown
below.

\begin{verbatim}
    function simpson(f, a, b)
        c = (a + b)/2
        simpson = (f(a) + f(b) + 4*f(c)) * (b - a) /6
    end function

    function integral(f, a, b. error)
        c = (a + b)/2
        old.int = simpson(f, a, b)
        new.int = simpson(f, a, c) + simpson(f, c, b)
        if abs(old.int - new.int) < error then
            integral = (16*new.int - oid.int) /15
        else
            integral = integral(f, a, c, error/2) +
                       integral(f, c, b. error/2)
        end if
    end function
\end{verbatim}

Clearly, there is no obligation to use Simpson's rule on the sub-intervals: any favorite algorithm will do.

To translate the above into FORTH, we decompose into smaller parts. The name of the function representing the integrand (actually its execution address or \textbf{cfa}) is placed on the stack by \bc{USE(}, as in Ch. 8 \S\ 1.3.2 above. Thence it can be saved in a local variable --either on the rstack or in a \bc{VAR} or \bc{VARIABLE} that can be \bc{BEHEAD}ed-- so the corresponding phrases

\begin{lstlisting}
    R@   EXECUTE  \rstack
    name EXECUTE  \VAR
    name EXECUTE@ \VARIABLE
\end{lstlisting}

evaluate the integrand. Clearly the limits and error (or \textbf{tolerance}) must be placed on a stack of some sort, so the function can call itself. One simple possibility is to put the arguments on the 87stack itself. (Of course we then need a software fstack manager to extend the limited 87stack into memory, as discussed in Ch. 4 \S\ 7.) Alternatively, we could use the intelligent fstack (ifstack) discussed in Ch. 5 \S\ 2.5. We thus imagine the fstack to be as deep as necessary.

The program then takes the form\sepfootnote{08_13} shown on p. 171 below.

Note that in going from the pseudocode to FORTH we replaced \bc{)INTEGRAL} by \bc{RECURSE} inside the word \bc{)INTEGRAL}. The need for this arises from an ideosyncrasy of FORTH: normally words do not refer to themselves, hence a word being defined is hidden from the dictionary search mechanism (compiler) until the final \bc{;} is reached. The word \bc{RECURSE} unhides the current name, and compiles its \textbf{cfa} in the proper spot\sepfootnote{08_14}.

\begin{lstlisting}
    : USE( [COMPILE] ' CFA LITERAL ;
        IMMEDIATE
    
    : f(x) ( :cfa--cfa ) DUP EXECUTE ;
    
    : )integral (f:a b-- I )
        \ uses trapezoidal rule
        XDUP FR- F2/    ( 87:--a b [b-a]/2 )
        F-ROT f(x) FSWAP f(x) F+ F* ;
    
    : )Richardson   \ R-extrap. for trap. rule
        3 S->F F/ F+ ;  ( 87:I' I--Ii'' )

    DVARIABLE ERR   \ place to store err
    CREAT OLD.I  10 ALLOT
                    \ place to store I[a,b]

    : )INTEGRAL   ( :adr-- 87:a b err--I )
        ERR R32!
        XDUP )integral   ( 87:--a b I )
        OLD.I R80!
        XDUP F+ F2/      ( 87:--a b c=[a+b]/2 )
        FUNDER FSWAP     ( 87:--a c c b )
        XDUP )integral   ( 87:--a c c b I1 )
        F4P F4P          ( 87:--a c c b I1 a c )
        )integral F+     ( 87:--a c c b I1+I2 )
        FDUP OLD.I R80@ F<
        IF )Richardson
            FPLUCK FPLUCK FPLUCK FPLUCK
        ELSE FDROP FDROP ( 87:--a c c b )
            ERR R32@ F2/
            F-ROT F2P    ( 87:--a c err/2 c b err/2 )
            RECURSE      ( 87:--a c err/2 I[c,b] )
            F3R F3R F3R RECURSE F+
        THEN DROP ;
\end{lstlisting}

\subsubsection{Disadvantages of recursion in adaptive integration}
\TallC{The} main advantage of the recursive adaptive integration algorithm is its ease of programming. As we shall see, the recursive program is much shorter than the non-recursive one. For any reasonable integrand, the fstack (or ifstack) depth grows only as the square of the logarithm of the finest subdivision, hence never gets too large.

However, recursion has several disadvantages when applied to numerical quadrature:

\begin{itemize}
    \item The recursive program evaluates the function more times than necessary.
    \item It would be hard to nest the function \bc{)INTEGRAL} for multi-dimensional integrals.
\end{itemize}

Several solutions to these problems suggest themselves:
\begin{itemize}
    \item The best, as we shall see, is to eliminate recursion from the algorithm.
    \item We can reduce the number of function evaluations with a more precise quadrature formula on the sub-intervals.
    \item We can use "open" formulas like Gauss-Legendre, that omit the endpoints (see Appendix 8.1).
\end{itemize}

\subsubsection{Adaptive Integration without recursion}
\TallC{The} chief reason to write a non-recursive program is to avoid any repeated evaluation of the integrand. That is, the optimum
is not only the smallest number of points $x_n$ in (A, B] consistent with the desired precision, but to evaluate $f(x)$ once only at each $x_n$. This will be worthwhile when the integrand $f(x)$ is costly to evaluate.

To minimize evaluations of $f(x)$, we shall have to save values $f(x_n)$ that can be re-used as we subdivide the intervals.

The best place to store the $f(x_n)$’s is some kind of stack or array. Moreover, to make sure that a value of $f(x)$ computed at one mesh size is usable at all smaller meshes, we must subdivide into two equal sub-intervals; and the points $x_n$ must be equally spaced and include the end-points. Gaussian quadrature is thus out of the question since it invariably (because of the non-uniform spacings of the points) demands that previously computed $f(x_n)$’s are thrown away because they cannot be re-used.

The simplest quadrature formula that satisfies these criteria is the \textbf{trapezoidal rule} (see Appendix 8.2). This is the formula used in the following program.

To clarify what we are going to do, let us visualize the interval of integration, and mark the mesh points (where we evaluate $f(x)$ with \bc{+}:

Step 1: N=1

We now save (temporarily) $I_0$ and divide the interval in two, computing $I_{0}'$ and $I_1$, on the halves, as shown. This will be one fundamental operation in the algorithm.

Step 2: N = N + 1 = 2

We next compare $I'_0 + I_1$ with $I'_0$, The results can be expressed as a branch in a flow diagram, shown below.

Yes I No
Accumulate: Subdivide right-most
Move everything down

Fig. 8-3 \bc{SUBDIVIDE} \textit{branch in adaptive integration}

If the two integrals disagree, we subdivide again, as in Step 3 and
Step 4 below:

Step3: N=N+1=3

Step 4: N=N+1=4

Now suppose the last two sub-integrals $(I_3 + I'_2)$ in Step 4 agreed with their predecessor $(I_2)$; we then accumulate the part computed so far, and begin again with the (leftward) remainder of the interval, as in Step 5:
Step 5:

The flow diagram of the algorithm now looks like Fig. 8-4 below:

Fig. 8-4 \textit{Non-recursive adaptive quadrature}

and the resulting FORTH program is\sepfootnote{08_15}:

\begin{lstlisting}
\ COPYRIGHT 1991 JUUAN V. NOBLE
TASK IN1EGRAL
FIND CP@L 0= ? ( FLOAD COMPLEX )
\ define data-type tokens if not already
FIND REAL*4 0= ?(((
    0 CONSTANT REAL*4
    1 CONSTANT REAL*8
    2 CONSTANT COMPLEX
    3 CONSTANT DCOMPLEX )))

FIND 1ARRAY 0= ?( FLOAD MATRIX.HSF )
\ function usage
: USE( [OOMPILE] ' CFA ; IMMEDIATE

\ BEHEADing starts here
0 VAR N

: inc.N N 1 + IS N ;
: dec.N N 2 - IS N ;

0 VAR type

\ define "stack"
20 LONG REAL*8   1ARRAY X{
20 LONG REAL*4   1ARRAY E{
20 LONG DCOMPLEX 1ARRAY F{
20 LONG DCOMPLEX 1ARRAY I{

2 DCOMPIEXSCALARS old.I final.I
: )imegral ( n-- ) \ trapezoidal rule
    X{ OVER    } G@L
    X{ OVER 1- } G@L
    F- F2/
    F{ OVER    } G@L
    F{ OVER 1- } G@L
    type2 AND
    IF   X+ FROT X*F
    ELSE F+ F* THEN
    I{ SWAP 1- } G!L ;
0 VAR f.name
: f(x) fname EXECUTE ;

: INITIALIZE
    IS type   \ store type
    type F{ !
    type I{ ! \ set types for function
    type ' old.I   !
    type ' final.I ! \ and integral(s)
    type 1 AND X{  !
            \ set type for indep. var.
    E{ 0 } G!L \ store error
    X{ 1 } G!L \ store B
    X{ 0 } G!L \ store A
    IS f.name  \ ! cfa of f(x)
    X{ 0 } G@L f(x) F{ 0 } G!L
    X{ 1 } G@L f(x) F{  1} G!L
    1 IS N
    N )integral
    type 2 AND IF F=0 THEN
    F=0 final.I G!L
    FINIT ;

: E/2 E{ N 1- } G@L F2/ E{ N 1- } G!L ;

: }move.down     ( adr n--)
    } \#BYTES >R ( --seg off )
    DDUP R@ +
                 ( --s.seg S.off d.seg d.off )
    R> CMOVEL ;

: MOVEDOWN
    E{ N 1- }move.down
    X{ N    }move.down
    F{ N    }mova.dcmn ;

: new.X ( 87:--x' )
    X{ N } G@L  X{ N 1- } G@L
    F+ F2/ FDUP X{ N    } G!L ;

\ cont'd. ...

\ INTEGRAL cont'd
: GF.   1 > IF FSWAP E. THEN E. ;
: F@.   DUP>R G@L R> GF. ;
: .STACKS CR ." N"
     8 CTAB  ." X"
    19 CTAB  ." Re[F(X)]"
    31 CTAB  ." Im[F(X)]"
    45 CTAB  ." Re[I]"
    57 CTAB  ." Im[I]"
    71 CTAB  ." E"
    N 2 + 0 DO CR I .
         3 CTAB X{ I } F@
        16 CTAB F{ I } F@
        42 CTAB I{ I } F@
        65 CTAB E{ I } F@
    LOOP
    CR 5 SPACES ." old.I ="     old.I F@.
       5 SPACES ." final.I =" final.I F@. CR ;
CASE: <DEBUG> NEXT .STACKS ;CASE
0 VAR (DEBUG)
: DEBUG-ON  1 IS (DEBUG) 5 #PLACES
: DEBUG-OFF 0 IS (DEBUG) 7 #PLACES
: DEBUG (DEBUG) <DEBUG> ;

: SUBDIVIDE
    N 19 > ABORT" Too many subdivisions!"
    E/2 MOVE.DOWN
    I{ N 1- } DROP old.I #BYTES CMOVEL
        new.X f(x) F{ N } G!L
    N )integral N 1 + )integral ;

: CONVERGED? ( 87:--I[N]+I'[N-1]-I[N-1]:--f )
    I{ N } G@L I{ N 1- } G@L old.I G@L
    type 2 AND
    IF   CP- CP+ CPDUP CPABS
    ELSE  F-  F+  FDUP  FABS
    THEN
    E{ N 1- } G@L F2* F< ;

CASE: g*6 CP*F F* ;CASE
4 S->F 3 S->F F/ FCONSTANT F=4/3

: INTERPOLATE ( 87:I[N]=I'[N-1]-I[N-1]-- )
    F=4/3 type 2/ g*f
    old.I G@L final.I G@L
    type 2 AND
    IF   CP+ CP+
    ELSE  F+  F+ THEN
    final.I G!L ;
\ BEHEADing ends here

: )INTEGRAL ( 97:A B ERR--I[A,B] )
    INITIALIZE
    BEGIN N 0>
    WHILE   SUBDIVIDE   DEBUG
        CONVERGED?     inc.N
        IF INTERPOLATE dec.N
        ELSE type 2 AND
            IF   FDROP
            THEN FDROP
        THEN
    REPEAT final.I G@L ;
BEHEAD" N INTERPOLATE   \ optional
\ USE( F.name % A % B % E type )INTEGRAL
\end{lstlisting}

The nonrecursive program obviously requires much more code than the recursive version. This is the chief disadvantage of a nonrecursive method\sepfootnote{08_16}.

\subsubsection{Example of )INTEGRAL IN USE}
\TallC{The} debugging code ("\bc{DEBUG-ON}") lets us track the execution of the program by exhibiting the simulated stacks. Here is an example, $int_{1}^{2} dx \sqrt{x}$:
\begin{lstlisting}
USE( FSQRT \% 1. \% 2. \% 1.E-3 REAL*4 )INTEGRAL
\end{lstlisting}

N      X            F          I             E
0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 6.5973E-01 5.0000E-04
2 2.0000E+00  1.4142E+00 1.4983E-01 1.2500E-04
old.I = 1.2071E+00  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 3.1845E-01 2.5000E-04
2 1.7500E+00  1.3228E+00 3.4213E-01 2.5000E-04
3 2.0000E+00  1.4142E+00 1.7396E-01 1.2500E-04
old.I = 6.5973E-01  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 3.1845E-01 2.5000E-04
2 1.7500E+00  1.3228E+00 1.6826E-01 1.2500E-04
3 1.8750E+00  1.3693E+00 1.7396E-01 1.2500E-04
4 2.0000E+00  1.4142E+00 0.0000E+00 0.0000E+04
old.I = 3.4213E-01  final.I = 0.0000E+00

0 1.0000E+00  1.0000E+00 5.5618E-01 5.0000E-04
1 1.5000E+00  1.2247E+00 1.5621E-01 1.2500E-04
2 1.6250E+00  1.2747E+00 1.6235E-01 1.2500E-04
3 1.7500E+00  1.3228E+00 1.7396E-01 1.2500E-04
old.I = 3.1845E-01  final.I = 3.4226E+00

N      X            F          I             E
0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 2.9284E-01 2.5000E-04
2 1.5000E+00  1.2247E+00 1.6235E-01 1.2500E-04
old.I = 5.5618E-01  final.I = 6.6087E+00

0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 1.4316E-01 1.2500E-04
2 1.3750E+00  1.1726E+00 1.4983E-01 1.2500E-04
3 1.5000E+00  1.2247E+00 1.7396E-01 1.2500E-04
old.I = 2.9284E-01  final.I = 6.6067E+00

0 1.0000E+00  1.0000E+00 1.2879E-01 1.2500E-04
1 1.1250E+00  1.0606E+00 1.3616E-01 1.2500E-04
2 1.2500E+00  1.1180E+00 1.4983E-01 1.2500E-04
old.I = 2.6475E-01  final.I = 9.5392E+00
1.2189E+00

\begin{equation}
\int_{1}^{2}dx\sqrt{x} = \frac{2}{3} \left( 2^{3/2} - 1 \right)
\end{equation}

Notice that, although $\sqrt{x}$ is perfectly finite at $x = 0$, its first derivative is not. This is not a problem in the above case, because the lower limit is 1.0.

It is an instructive exercise to run the above example with the limits (0.0, 1.0). The adaptive routine spends many iterations: approaching $x = 0$ (25 in the range [0., 0.0625] \textit{vs.} 25 in the range: [0.0625, 1.0] ). This is a concrete example of how an adaptive routine will unerringly locate the (integrable) singularities of at function by spending lots of time near them. The best answer to this problem is to separate out the bad parts of a function by hand, if possible, and integrate them by some other algorithm that take the singularities into account. By the same token, one shoul always integrate \textit{up to}, but not \textit{through}, a discontinuity in $f(x)$.

\subsection{Adaptive Integration in the Argand plane}
\TallC{We} often want to evaluate the complex integral

\begin{equation}
I = \oint_{\Gamma}f(z)dz
\end{equation}

where $\Gamma$ is a \textbf{contour} (simple, closed, piecewise tinuous curve) in the complex $z$-plane, and $f(z)$ is an \textbf{analytic}\sepfootnote{08_17} function of $z$.

The easiest way to evaluate 16 is to parameterize $z$ as a function of a real variable $t$; as $t$ runs from A to B, $z(t)$ traces out the contour. For example, the parameterization

\begin{equation}
z(t) = z_0 + R cos(t) + iR sin(t) , 0 \leq t \leq 2\pi
\end{equation}

traces out a (closed) circle of radius R centered at $z = z_0$.

We assume that the derivative $\dot{z}(t)\equiv \frac{dz}{dt}$ can be defined; then the integral 16 can be re-written as one over a real interval, with a complex integrand:

\begin{equation}
I = \int_{A}^{B} \dot{z}(t)f\left( z(t) \right) dt
\end{equation}

Now our previously defined adaptive function \bc{)INTEGRAL} can be applied directly, with \bc{F.name} the name of a \textit{complex} funcan

\begin{equation}
g(t) = \dot{z}(t)f\left( z(t) \right),
\end{equation}

of the \textit{real} variable $t$.

Here is an example of complex integration: we integrate the function $f(z) = e^{1/z}$ around the unit circle in the counter-clock-wise (positive) direction.

The calculus of residues (Cauchy's theorem) gives

\begin{equation}
\oint_{ \vert z \vert =1} dz e^{1/z} = 2\pi i
\end{equation}

We parameterize the unit circle as $z(t) = cos(2\pi t) + i sin(2\pi t)$, hence $\dot{z}(t) = 2\pi iz(t)$, and we might as well evaluate

\begin{equation}
int_{0}^{1} dt z(t) e^{1/z(t)} \equiv 1.
\end{equation}

For reasons of space, we exhibit only the first and last few iterationst

\begin{lstlisting}
    FIND FSINCOS 0= ?( FLOAD TRIG )
    : Z(T) F=PI F* F2 FSINCOS ;
    : XEXP FSINCOS FROT FEXP X*F ;
        ( 87:x y--e^x cos[y] e^x sin[y] )
    : G(T) Z(T) XDUP 1/X XEXP X* ;
    DEBUG_ON
    USE( G(T) \% 0 \% 1 \% 1.E-2 COMPLEX )INTEGRAL X.
\end{lstlisting}

N      X            F          I             E
0 1.0000E+00  1.0000E+00 2.6475E-01 2.5000E-04
1 1.2500E+00  1.1180E+00 2.9284E-01 2.5000E-04
2 1.5000E+00  1.2247E+00 1.6235E-01 1.2500E-04
old.I = 5.5618E-01    final.I = 6.6087E+00

box this in yo!
Note:
    answer = 1

\section{Fitting functions to data}
\TallC{One} of the, most important applications of numerical analysis is the representation of numerical data in functional form. This includes fitting, smoothing, filtering, interpolating, \etc

A typical example is the problem of table lookup: a program requires values of some mathematical function --$sin(x)$, say-- for arbitrary values of $x$. The function is moderately or extremely time-consuming to compute directly. According to the Intel timings for the 80x87 chip, this operation should take about 8 times longer than a floating point multiply. In some real-time applcations this may be too slow.

There are several ways to speed up the computation of a function. They are all based on compact representations of the funtion --either in tabular form or as coefficients of functions that are faster to evaluate. For example, we might represent $sin(x)$ by a simple polynomial\sepfootnote{08_18}

\begin{equation}
sin(x) \approx x \left( 0.994108 - 0.147207x\right),
\end{equation}

accurate to better than 1\% over the range $-\frac{\pi}{2}\leq x \leq \frac{\pi}{2}$, that requires but 3 multiplications and an addition to evaluate. This would be twice as fast as calculating $sin(x)$ on the 80x87 chip\sepfootnote{08_19}.

\TallC{To} achieve substantially greater speed requires table look-up. To locate data in an ordered table, we might employ binary search: that is, look at the $x$-value halfway down the table and see if the desired value is greater or less than that. On the average,$log_{2}(N)$ comparisons are required, where $N$ is the length of the table. For a table with 1\% precision, we might need 128 entries, \ie seven comparisons.

Binary search is unacceptably slow --is there a faster method? In fact, assuming an ordered table of equally-spaced abscissae the fastest way to locate the desired $x$-value is \textbf{hashing}, a method for \textit{computing} the address rather than finding it using comparisons. Suppose, as before, we need 1\% accuracy, \ie a 128-point table with $x$ in the range $[0,\pi/2]$. To look up a value, we multiply $x$ by $256/\pi \cong 81.5$, truncate to an integer and quadruple it to get a (4-byte) floating point address. These operations --including fetch to the 87stack-- take about 1.5-2 fp multiply times, hence the speedup is 4-fold.

The speedup factor does not seem like much, especially for a
function such as $sin(x)$ that is built into the fast co-processor. However, if we were speaking of a function that is considerably slower to evaluate (for example one requiring evaluation of an integral or solution of a differential equation) hashed table lookup with interpolation can be several orders of magnitude faster than direct evaluation.

\TallC{We} now consider how to represent data by mathematical functions. This can be useful in several contexts:

\begin{itemize}
    \item The theoretical form of the function, but with unknown parameters, may be known. One might like to \textit{determine} the parameters from the data. For example, one mi t have a lot of data on pendulums: their periods, masses, imensions, \etc The period of a pendulum is given, theoretically, by
    
    \begin{equation}
    \tau = \frac{2\pi L}{g}^{1/2} f\left( \frac{L}{r},\frac{m_{\text{bob}}}{m_{\text{string}},\mathellipsis}\right)
    \end{equation}
    where L is the length of the string, g the acceleration of gravity, and $f$ is some function of ratios of typical lengths, masses, and other factors in the problem. In order to determine g accurately, one generally fits a function of all the measured factors, and tries to minirmze its deviation from the measured periods. That is, one might try
    
    \begin{equation}
    \tau_{n} = \left(frac{2\pi L_{n}}{g}^{1/2}\right) \left[ 1 + \alpha frac{r_{n}}{L_{n}} + \beta\left(\frac{m_{bob}}{m_string}\right)_{n} + \mathellipsis\right]
    \end{equation}
    for the n'th set of observations, with g, $\alpha$, $\beta$, $\mathellipsis$ the unknown parameters to be determined.
    
    \item Sometimes one knows that a phenomenon is basically smoothly varying; so that the wiggles and deviations in observations are noise or otherwise umnteresting. How can we filter out the noise without losing the significant part of the data? Several methods have been developed for this purpose, based on the same principle: the data are represente as a sum of functions from a \textbf{complete} set of functions, with unknown coefficients. That is, if $\varphi_{m}(x)$ are the functions, we say ($y_n$ are the data)

    \begin{equation}
    y_n = \sum_{m=0}^{\infty}c_{m}\varphi_{m}(x_{n})
    \end{equation}

    Such representations are theoretically possible under general conditions. Then to filter we keep on] a finite sum, retaining the first N (usually simplest and smoothest) functions from the set. An example of a complete set is \textbf{monomials}, $\varphi_{m}(x) = x^{m}$. Another is \textbf{sinusoidal (trigonometric) functions},
        $\sin(2\pi mx), \cos(2\pi mx), 0 \leq x \leq 1$,
    
    used in Fourier-series representation. \textbf{Gram polynomials}, discussed below, comprise a third useful complete set.
\end{itemize}

\TallC{The} representation in Eq. 25 is called \textbf{linear} because the unknown coefficients $c_{m}$ appear to their first power. Thus, if all the data were to double, we see immediately that the $c_{m}$'s would have to be multiplied by the same factor, 2. Sometimes, as in the example of the measurement of g above, the unknown parameters appear in more complicated fashion. The problem of fitting with these more general functional forms is called \textbf{nonlinear} for obvious reasons. The \textbf{simplex algorithm} of Ch. 8 \S\ 2.3 below is an example of a nonlinear fitting procedure.

We are now going to write programs to fit both linear and non-linear functions to data. The first and conceptually simplest of these is the \textbf{Fourier transform}, namely representing a function as a sum of sines and cosines.

\subsection{Fast Fourier transform}
\TallC{What} is a Fourier transform? Suppose we have a function that is \textbf{periodic} on the interval $0 \leq x \leq 2\pi$:

\begin{equation}
f(x + 2\pi) = f(x) ;
\end{equation}

Then under fairly general conditions the function can be expressed in the form

\begin{equation}
f(x) = a_{0} + \sum_{n=1}^{\infty} (a_{n} \cos(nx) + b_{n} \sin(nx))
\end{equation}

Another way to write Eq. 26 is

\begin{equation}
f(x) = \sum_{-\infty}^{+\infty} c_{n}e^{inx}.
\end{equation}

In either way of writing, the $c_{n}$ are called \textbf{Fourier coefficients} of the function $f(x)$. Looking, \eg at Eq. 27, we see that the \textbf{orthogonality} of the sinusoidal functions leads to the expression

\begin{equation}
c_{n} = \frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-inx}dx
\end{equation}

Evaluating Eq. 28 numerically requires --for given n-- at least 2n pomts\sepfootnote{08_20} Naively, for each $n = 0$ to N-1 we have to do a sum

\begin{equation}
c_{n} \approx \sum_{k=1}^{2N}f_{k}e^{-2\pi inx}dx .
\end{equation}

which means carrying out $2N^2$ complex multiplications.

The \textbf{fast Fourier transform (FFT)} was discovered by Runge and K\"{o}nig, rediscovered by Danielzslon and Lanaos and \textbf{\underline{re}}-rediscovered by Cooley and Tukey\sepfootnote{08_21}. The FFT algorithm can be expressed as three steps:

\begin{itemize}
    \item Discretize the interval, \ie evaluate $f(x)$ only for
    \begin{equation}
        x_{k} = 2\pi \frac{k}{N} , 0 \leq x \leq N-1.
    \end{equation}
    Call $f(x_k) \equiv f_k$.
    \item   Express the Fourier coefficients as
    \begin{equation}
        c_{n} \approx \sum_{k=0}^{N-1}f_{k}e^{-2\pi inx/N} .
    \end{equation}
    \item With $w_{n} = e^{-2\pi inx/N}$, Eq. 29 is an $N-1$’st degree polynomial in $w_{n}$. We evaluate the polynomial using a fast algorithm.
\end{itemize}

To evaluate rapidly the polynomial

\begin{equation}
c_{n} = P_{n}(w_{n}) \equiv \sum_{k=0}^{N-1}f_{k}(w_{n})^k
\end{equation}

we divide it into two polynomials of order N/Z, dividing each of those in two, \etc This procedure is efficient only for $N=2^v$, with $v$ an integer, so this is the case we attack.

How does dividing a polynomial in two help us? If we segregate the odd from the even powers, we have, symbolically,

\begin{equation}
P_{N}(w) = E_{N/2}(w^{2}) + w O_{N/2}(w^2).
\end{equation}

Suppose the time to evaluate $P_N(w)$ is $T_{N}$. Then clearly,

\begin{equation}\label{eq:08_31}
T_{N} = \lambda + 2T_{N/2}
\end{equation}

where $\lambda$ is the time to segregate the coefficients into odd and even, plus the time for 2 multiplications and a division. The solution of Eq. \ref{eq:08_31} is $\lambda(N-1)$. That is, it takes $O(N)$ time to evaluate a polynomial.

However, the discreteness of the Fourier transform helps us here. The reason is this: to evaluate the transform, we have to evaluate $P_{N}(w_{n})$ for $N$ values of $w_{n}$. But $w_{n}^{2}$ takes on only $N/2$ values as n takes on N values. Thus to evaluate the Fourier transform for all N values of n, we can evaluate the two polynomials of order $N/2$ for half as many points.

Suppose we evaluated the polynomials the old-fashioned way: it would take $2(N/2)\equiv N$ multiplications to do both, but we need do this only $N/2$ times, and $N$ more (to combine them) so we have $N^{2}/2+N$ rather than $N^{2}$. We have gained a factor 2. Obviously it pays to repeat the procedure, dividing each of the sub-polynomials in two again, until only monomials are left.

Symbolically, the number of multiplications needed to evaluate, a polynomial for N (discrete) values of $w$ is

\begin{equation}
\tau_{N} = N\lambda + 2\tau_{N/2}
\end{equation}

whose solution is

\begin{equation}
\tau_{N} = \lambda N \log_{2}(N) + 2\tau_{N/2}
\end{equation}

Although the FFT algorithm can be programmed recursively, it almost never is. To see why, imagine how the coefficients would be re-shuffled by Eq. 30: we work out the case for 16 coefficients, exhibiting them in Table 8-1 below, writing only the indices:

Table 8-1 Bit-reversal for re-ordering discrete data

Start   Step 1  Step 2  Step 3    $Bin_0 Bin_3$
0       0       0       0         0000  0000
1       2       4       8         0001  1000
2       4       8       4         0010  0100
3       6       12      12        0011  1100
4       8       2       2         0100  0010
5       10      6       10        0101  1010
6       12      10      6         0110  0110
7       14      14      14        0111  1110
0       1       1       1         1000  0001
9       3       5       9         1001  1001
10      5       9       5         1010  0101
11      7       13      13        1011  1101
12      9       3       3         1100  0011
13      11      7       11        1101  1011
14      13      11      7         1110  0111
15      15      15      15        1111  1111

The crucial columns are "Start" and "Step 3". Unfortunately, they are written in decimal notation, which conceals a fact that becomes glaringly obvious in binary notation. So we re-write them in binary in the columns $Bin_0$ and $Bin_3$, --and see that the final order can be obtained from the initial order simply by reversing the order of the bits, from left to right!

\TallC{A} standard FORTRAN program for complex FFT is shown below. We shall simply translate the FORTRAN into FORTH as expeditiously as possible, using some of FORTH's simplifications.

\TallC{One} such improvement is a word to reverse the bits in a given integer. Note how clumsily this was done in the FORTRAN program. Since practically every microprocessor permits right-shifting a register one bit at a time and feeding the overflow into another register from the right, \bc{B.R} can be programmed easily in machine code for speed. Our fast bit-reversal procedure \bc{B.R} may be represented pictorially as in Fig. 8-5 below.

\begin{verbatim}
  SUBROUTINE FOUR1(DATA, NN, ISIGN)
C
C  from Press, et al., Numerical Recipes, ibid., p. 394
C   
C  ISIGN DETERMINES WHETHER THE FFT
C  IS FORWARD OR BOCKWARD
C
C  DATA IS THE (COMPLEX) ARRAY OF DISCRETE INPUT 
  COMPLEX W, WP, TEMP, DATA(N)
  REAL*8 THETA
  J=0
  DO 11 I=0,N-1       \ begin bit.reversal
    IF (J.GT.I) THEN
      TEMP= DATA(J)
      DATA(J)=DATA(I)
      DATA(I)=TEMP
    ENDIF
    M=N/2
1     IF ((M.GE.1).AND.(J.GT.M)) THEN
      J=J-M
      M=M/2
      GO TO 1
    ENDIF
    J = J + M
11  CONTINUE              \ end bit.reversal
      
      MMAX=1              \ begin Danielson-Lancszos section
2     IF (N.GT.MMAX) THEN
           ISTEP=2*MMAX   \ executed lg(N) times
                          \ init trig recurrence
    THETA=3.14159265358979D0/(ISING*MMAX)
    W4=CEXP(THETA)
    W =DCMPLX(1.D0,0.D0)
    DO 13 M=1,MMAX,2      \ outer loop
      DO 12 I = M,N,ISTEP \ inner loop
        J = I+MMAX        \ total = N times
        TEMP = DATA(J)*W
        DATA(J) = DATA(I)-TEMP
        DATA(I) = DATA(I)+TEMP
12      CONTINUE              \ end inner loop
C
      W=W*WP                  \ trig recurrence
C
13  CONTINUE                  \ end outer loop
    MMAX=ISTEP
    GO TO 2
  ENDIF
  RETURN
\end{verbatim}

Fig. 8-5 \textit{Pictorial representation of bit-reversal}

Bit-reversal can be accomplished in high-level FORTH \textit{via}

\begin{lstlisting}
    : B.R       ( n--n' )   \ reverse order of bits
        0 SWAP  ( -- 0 n )  \ set up stack
        N.BITS 0 DO
            DUP 1 AND       \ pick out 1's bit
            ROT 2* +        \ left-shift 1, add 1's bit
            SWAP 2/         \ right-shift n
        LOOP DROP ;
\end{lstlisting}

\leftbar[1\linewidth]
Note: \bc{N.BITS} is a \bc{VAR}, previously set to $v = \log_{2}(N)$
\endleftbar

We will use \bc{B.R} to re-order the actual data array (even though this is slightly more time-consuming than setting up a list of scrambled pointers, leaving the data alone). We forego indirection for two reasons: first, we have to divide by N (N steps) when inverse-transforming, so we might as well combine this with bit-reversal; second, there are N steps in rearranging and dividing by N the input vector, whereas the FFT itself takes $N\log_{2}(N)$ steps, \ie the execution time for the preliminary N steps is unimportant.

Now, how do we go about evaluating the sub-polynomials to get the answer? First, let us write the polynomials (for our case N = 16) corresponding to taking the (bit-reversed) addresses off the stack in succession, as in Fig. 8-6 below.

Fig. 8-6 \textit{The order of evaluating a 16 pt. FFT}

We see that $w_{n}^{8}$ (for N=16) has only two possible values, $\pm1$. Thus we must evaluate not $16\times8$ terms like $f_{i} + w^{8}f_{i+8}$, but only $16\times2$. Similarly, we do not need to evaluate $16\times4$ terms of form $f_{i} + w^{4}f_{i+4}$f, but only $4\times4$, since there are only 4 possible values of $w^{4}$. Thus the total number of multiplications is

\begin{align*}
    2\times8 + 4\times4 + 8\times2 + 16\times1 = 64 \equiv 16\log_{2}16,
\end{align*}

as advertised. This is far fewer than $16\times16=256$, and the ratio improves with N -- for example a 1024 point FFT is 100 times faster than a slow FT.

We list the FFT program on page 191 below. Since \bc{\}FFT} transforms a one-dimensional array we retain the curly braces notation introduced in Ch. 5. We want to say something like

\begin{lstlisting}
    V{ n.pts FORWARD }FFT
\end{lstlisting}

where \bc{V\{} is the name of the (complex) array to be transformed, \bc{n.pts} (a power of 2) is the size of the array, and the flag-setting words \bc{FORWARD} or \bc{INVERSE} determine whether we are taking a FFT or inverting one.

\TallC{Now} we test the program. Table 8-2 on page 192 contains the weekly stock prices of IBM stock, for the year 1983 (the 52 values have been made complex numbers by adding $0i$, and the table padded out to 64 entries (the nearest power of 2) with complex zeros)\sepfootnote{08_22}. The first two entries (2,64) are the type and length of the file. (The file reads from left to right.)

We FFl‘ Table 8- 2 using the phrase \bc{IBM{ 64 DIRECT }FFT}. The power spectrum of the resulting FFI‘ (Table 8-3) is shown in Fig. 8-7 on page 192 below.

\begin{lstlisting}
\ Complex Fast Fourier Transform

\end{lstlisting}

\TallC{How} do we know the FFT program actually worked? The simplest method is to inverse-transform the transform, and compare with the input file. The FFT and inverse FFT are given, respectively, in Tables 8-3 and 8-4 on page 193 below. Within roundoff error, Table 8-4 agrees with Table 8-2 on page 192.

Table 8-2 \textit{Weekly IBM common stock prices, 1983}

Fig. 8-7 \textit{Power spectrum of FFT of 1983 IBM prices (from Table 63)}

Table 8-3 \textit{FFT of IBM weekly stock prices, 1983}

Table 8-4 \textit{Reconstructed IBM prices (inverse FFT)}

\subsection{Gram polynomials}
\TallC{Gram} polynomials are useful in fitting data by the linear least-squares method. The usual method is based on the following question: What is the "best" polynomial,

\begin{equation}
P_N(x) = \sum_{n=0}^N \gamma_n x^n
\end{equation}

(of order N) that I can use to fit some set of M pairs of data points,

\begin{equation}
    \left \{
      \begin{tabular}{c}
      $x_{k}$ \\
      $f_{k}$
      \end{tabular}
    \right \}
    , k=0,1, ... , M-1
\end{equation}

(with M > N) where f(x) is measured at M distinct values of the independent variable x ?

The usual answer, found by Gauss, is to minimize the \textbf{squares} of the \textbf{deviations} (at the points $x_k$) of the fitting function $P_N(x)$ from the data --possibly weighted by the uncertainties of the data That is, we want to minimize the \textbf{statistic}

\begin{equation}
\chi^2 = \sum_{k=0}^{M-1} \left( f_k - \sum_{n=0}^{N} \gamma_n x_{k}^n \right) \frac{1}{\sigma_{k}^{2}}
\end{equation}

with repect to the $N + 1$ parameters $\gamma_{n}$.

From the differential calculus we know that a function's first derivative vanishes at a minimum, hence we differentiate $\chi^{2}$ with respect to each $\gamma_{n}$ independently, and set the results equal to zero. This yields $N + 1$ linear equations in N + 1 unknowns:

\begin{equation}
\sum_{m} A_{nm} \gamma_{m} = \beta_{n}, n = 0, 1, ... , N
\end{equation}

where (the symbol $\triangleq$ means “is defined by”)

\begin{equation}
A_{nm} \triangleq \sum{k=0}^{M-1}(x_{k})^{n+m} \frac{1}{\sigma_{k}^{2}}
\end{equation}

and

\begin{equation}
\beta_{nm} \triangleq \sum_{k=0}^{M-1}x_{k}^{n}f_{k}\frac{1}{\sigma_{k}^{2}}
\end{equation}

In Chapter 9 we develop methods for solving linear equations. Unfortunately, they cannot be applied to Eq. 36 for $N \geq 9$ cause the matrix $A_{nm}$ approximates a Hilbert matrix,

\begin{equation}
H_{nm} = \frac{const.}{n+m+1} ,
\end{equation}

a particularly virulent example of an exponentially \textbf{ill-conditioned matrix}. That is, the round off error in solving 36 grows exponentially with $N$, and is generally unacceptable We can avoid roundolf problems by expanding in polynomials rather than monomials:

\begin{equation}\label{eq:08_38}
\chi^{2} = \sum_{k=0}^{M-1} (f_{k} - \sum_{n=0}^{N} \gamma_{n}p_{n}(x_{k}) )^2 \frac{1}{\sigma_{k}^{2}} .
\end{equation}

The matrix then becomes

\begin{subequations}
    \begin{equation}\label{eq:08_39a}
    A_{nm} = \sum_{k=0}^{M-1} p_{n}(x_{k})p_{m}(x_{k}) \frac{1}{\sigma_{k}^{2}}
    \end{equation}
and the inhomogeneous term is now
    \begin{equation}
    \beta_{n} = \sum_{k=0}^{M-1} p_{n}(x_{k})f_{k} \frac{1}{\sigma_{k}^{2}}
    \end{equation}
\end{subequations}

\TallC{Is} there any choice of the polynomials $p_{n}(x)$ that will eliminate roundoff? The best kinds of linear equations are those with nearly diagonal matrices. We note the sum in Eq. \ref{eq:08_39a} is nearly an integral, if M is large. If we choose the polynomials so they are \textbf{orthogonal} with respect to the weight function

\begin{equation*}
w(x) = \frac{1}{\sigma_{k}^{2}} \theta(x_{k} - x ) \theta(x - x_{k-1}),
\end{equation*}

where

\begin{equation*}
\theta(x) =
    \begin{cases}
        0, x < 0 \\
        1, x \geq 0
    \end{cases}
\end{equation*}

then $A_{nm}$, will be nearly diagonal, and well-conditioned.

\TallC{Orthogonal} polynomials play an important role in numerical analysis and applied mathematics. They satisfy \textbf{orthogonality relations}\sepfootnote{08_23} of the form

\begin{equation}\label{eq:08_40}
\int_{A}^{B} dx w(x)p_{n}(x)p_{m}(x) = \delta_{nm} \equiv 
    \begin{cases}
        1, m   = n \\
        0, m \ne n 
    \end{cases}
\end{equation}

where the \textbf{weight function} $w(x)$ is positive.

For a given $w(x)$ and interval $[A,B]$, we can construct orthogonal polynomials using the \textbf{Gram-Schmidt orthogonalization} process.

Denote the integral in Eq. \ref{eq:08_40} by $(p_{n}, p_{m})$ to save having to writing it many times. We start with


  \begin{align*}
p_{-1} & = 0, \\
p_{0}(x) & = (\int_{A}^{B}dx w(x))^{-1/2} = \text{const.} ,
  \end{align*}

and assume the polynomials satisfy the 2-term upward recursion relation

\begin{equation}
p_{n+1}(x) = (a_{n} + xb_{n} ) p_{n}(x) + c_{n}p_{n-1}
\end{equation}

Now apply Eq. 41: assume we have calculated $p_{n}$ and $p_{n-1}$ and want to calculate $p_{n+1}$. Clearly, the orthogonality property gives

\begin{equation}
(p_{n+1},p_{n}) = (p_{n+1},p_{n-1}) = (p_{n},p_{n-1}) = 0,
\end{equation}

and the assumed normalization gives
\begin{equation}
(p_{n},p_{n}) = 1.
\end{equation}

These relations yields two equations for the three unknowns, $a_{n}$, $b_{n}$, and $c_{n}$:

\begin{align*}
a_{n} + b_{n} (p_n, xp_{n})   & = 0 \\
c_{n} + b_{n} (p_n, xp_{n-1}) & = 0
\end{align*}

We express $a_{n}$ and $c_{n}$, in terms of $b_{n}$, to get

\begin{equation}
p_{n+1}(x) = b_{n} \left[ (x - (p_{n}, xp_{n})) p_{n}(x) - (p_{n} , x p_{n-1})p_{n-1}(x) \right]
\end{equation}

We determine the remaining parameter $b_{n}$ by again using the normalization condition:

\begin{equation*}
(p_{n+1} , p_{n+1}) = 1
\end{equation*}

In practice, we pretend $b_{n}=1$ and evaluate Eq. 42; then we calculate

\begin{equation}
b_{n} = ( \bar{p}_{n+1}, \bar{p}_{n})^{-1/2},
\end{equation}

multiply the (un-normalized) $\bar{p}_{n+1}$ by $b_{n}$, and continue.

The process of successive orthogonalization guarantees that $p_{n}$ is orthogonal to all polynomials of lesser degree in the set. Why is this so? By construction, $p_{n+1} \perp p_{n}$ and $p_{n+1}$. Is it $\perp p_{n-2}$? We need to ask whether

\begin{equation*}
(p_{n}, (x-a_{n})p_{n-2}) = 0.
\end{equation*}

But we know that any polynomial of degree N-l can be expressed as a linear combination of independent polynomials of degrees $0, 1, ... , N-1$. Thus

\begin{equation}\label{eq:08_44}
(x - a_{n}) p_{n-2} \equiv \sum_{k=0}^{n-1} \mu_{k} p_{k}(x)
\end{equation}

and (by hypothesis) $p_{n} \perp$ every term of the rhs of Eq. \ref{eq:08_44}, hence it follows (by mathematical induction) that

\begin{equation}
p_{n+1} \perp \{ p_{n-2} , p_{n-3} , ... \}.
\end{equation}

Let us illustrate the process for Legendre polynomials, defined by weight $w(x) = 1$, interval $[-1,1]$:

\begin{align*}
p_{0} & = (\frac{1}{2})^{1/2} , \\
p_{1} & = (\frac{3}{2})^{1/2} x , \\
p_{2} & = (\frac{5}{2})^{1/2} (\frac{3}{2}x^{2} - \frac{1}{2}) ,
\end{align*}

These are in fact the first three (normalized) Legendre polynomials, as any standard reference will confirm.

\TallC{Now} we can discuss Gram polynomials. While orthogonal polynomials are usually defined with respect to an \textbf{integral} as in Eq. 40, we might also define orthogonality in terms of a \textbf{sum}, as in Eq. 39a. That is, suppose we define the polynomials such that

\begin{equation}\label{eq:08_45}
\sum_{k=0}^{M-1}p_{n}(x_{k})p_{m}(x_{k})\frac{1}{\delta_{k}^{2}} \equiv \delta_{nm} =
    \begin{cases}
        1, m   = n \\
        0, m \ne n 
    \end{cases}
\end{equation}

Then we can construct the Gram polynomials, calculating the
coefficients by the algebraic steps of the Gram-Schmidt process, except now we evaluate sums rather than integrals. Since $p_{n}(x)$ satisfies \ref{eq:08_45} by construction, the coefficients $\gamma_{n}$, in our fitting polynomial are simply

\begin{equation}
\gamma_{n} = \sum_{k=0}^{M-1} p_{n}(x_{k})f_{k}\frac{1}{\delta_{k}^{2}} ;
\end{equation}

they can be evaluated without solving any coupled linear equations, ill-conditioned or otherwise. Roundoff error thus becomes irrelevant.

The algorithm for fitting data with Gram polynomials may in expressed in flow-diagram form:

*** Diagram ***
neodhpomt...x.¢mw. 1.1/0.3.
DOn-ttoN-t (outerloop)
Construct an , cn :
DO k=o to M1 (inner loop)
pn+t(xlr) = (XI: " 8n)Pn(Xu) - crpn-1(Xtr)

sum = sum + (p0,,(x.)) 2w.

Com = an + ft Pn+1(Xk) Wk
LOOP (and inner loop)
cn+1 = cn+1 /sum

DO k =0 to M-1 (normalize)
’ Pn+1(xk) = pn+1(xk)/ mm
LOOP

LOOP
*** Diagram ***

Fig. 8-5 \textit{Construction of Gram polynomials}

The required storage is 5 vectors of length M to hold $x_{k}$, $p_{n}(x_{k})$, $f_{k}$, and $w_{k} \equiv 1\/\sigma_{k}^{2}$. We also need to store the coefficients $a_{n}$, $c_{n}$, and the normalizations $b_{n}$ --that is, 3 vectors of length $N<<M$-- in case they should be needed to interpolate. The time involved is approximately 7M multiplications and additions for each n, giving 7NM. Since N can be no greater than M-l (M data determine at most a polynomial of degree M-l), the maximum possible running time is $7M^{2}$, which is much less than the time to solve M linear equations.

In practice, we would never wish to fit a polynomial of order comparable to the number of data, since this would include the noise as well as the significant information.

We therefore calculate a statistic called $\chi^{2}$/(textit{degrce of freedom})\sepfootnote{08_24} With M data points and an N'th order polynomial, there are M-N-l degrees of freedom. That is, we evaluate Eq. \ref{08_38} for fixed N, and divide by M-N-l. We then increase N by l and do it again The value of N to stop at is the one where

\begin{equation*}
\delta_{M,N}^{2} = \frac{\chi_{M,N}^{2}}{M-N-1}
\end{equation*}

stops decreasing (with N) and begins to increase.

The best thing about the $\chi_{M,N}^{2}$ statistic is we can increase N without having to do any extra work:

\begin{equation}\label{eq:08_47}
    \begin{split}
        \chi_{M,N}^{2} &= \sum_{k=0}^{M-1} (f_{k} - \sum_{n} \gamma_{n}p_{n}(x_{k}))^2 w_{k} \\
        &\equiv \sum_{k=0}^{M-1} (f_{k})^{2} - \sum_{n-0}^{N} (\gamma_{n})^2
    \end{split}
\end{equation}

The first term after $\equiv$ E in Eq. \ref{eq:08_47} is independent of N, and the second term is computed as we go. Thus we could turn the outer loop (over N) into a \bc{BEGIN ... WHILE ... REPEAT} loop, in which N is incremented as long as $\sigma_{M,N}^{2}$ is larger than $\sigma_{M,N+1}^{2}$. (Incidentally, Eq. \ref{eq:08_47} guarantees that as we increase N the fitted curve deviates less and less, on the average, from the measured points. When $N = M-1$, in fact, the curve goes through the points. But as explained above, this is a meaningless fit, since all data contain measurement errors. A fitted curve that passes closer than $\sigma_{k}$ to more than about $\frac{1}{3}$ of the points is suspect.)

The code for Gram polynomials is relatively easy to write using the techniques developed in Ch. 5. The program is displayed in full in Appendix 8.4.

\subsubsection{Simplex algorithm}
\TallC{Sometimes} we must fit data by a function that depends on parameters in a \textbf{nonlinear} manner. An example is

\begin{equation}\label{eq:08_48}
f_{k} \approx \frac{F}{1+e^{a(x_{k}-X)}}
\end{equation}

Although the dependence on the parameter F is linear, that on the parameters a and X is decidedly nonlinear.

One way to handle a problem like fitting Eq. \ref{eq:08_48} might be to transform the data, to make the dependence on the parameters linear. In some cases this is possible, but in \ref{eq:08_48} no transformation will render linear the dependence on all three parameters at once.

\TallC{Thus} we are frequently confronted with having to minimize numerically a complicated function of several parameters. Let us denote these by $\theta_{0}, \theta_{1}, ... , \theta{N-1}$, and denote their possible range of variation by \textbf{R}. Then we want to find those values of $\{ \theta \} \subset$ \textbf{R} that minimize a positive function:

\begin{equation}
\chi^{2} (\bar{\theta}_{0}, \bar{\theta}_{1}, ... , \bar{\theta}{N-1}) = min_{\{ \theta \} \subset R} \chi^{2} (\theta_{0}, \theta_{1}, ... , \theta{N-1})
\end{equation}

One way to accomplish the minimization is via calculus, using a method known as \textbf{steepest descents}. The idea is to differentiate the function $\chi^{2}$ with respect to each $\theta_{k}$ and to set the resulting N equations equal to zero, solving for the N $\theta$'s. This is generally a pretty tall order, hence various approximate, iterative techniques have been developed. The simplest just steps along in $\theta$-space, along the direction of the local downhill gradient $-\nabla\chi^{2}$, until a minimum is found. Then a new gradient is computed, and a new minimum sought\sepfootnote{08_25}.

Aside fromthe labor of computing $-\nabla\chi^{2}$, steepest descents has two main drawbacks: first, it only guarantees to find \underline{\textit{a}} minimum, not necessarily \underline{\textit{the}} minimum -- if a function has several local minima, steepest descents will not necessarily find the smallest. Worse, consider a function that has a minimum in the form of a steep-sided gulley that winds slowly downhill to a declivity -- somewhat like a meandering river's channel. Steepest descents will then spend all its time bouncing up and down the banks of the gulley, rather than proceeding along its bottom, since the steepest gradient is always nearly perpendicular to the line of the channel.

Sometimes the function $\chi_{2}$ is so complex that its gradient is too expensive to compute. Can we find a minimum \textit{without} evaluating partial derivatives? A standard way to do this is called the \textbf{simplex method}. The idea is to construct a \textbf{simplex} -- a set of N + 1 distinct and \textbf{non-degenerate} vertices in the N-dimensional $\theta$-space ("non-degenerate" means the geometrical object, formed by connecting the N + 1 vertices with straight lines, has non-zero N-dimensional volume; for example, if N=2, the simplex is a triangle.)

We evaluate the function to be minimized at each of the vertices, and sort the table of vertices by the size of $\chi^{2}$ at each vertex, the best (smallest $\chi^{2}$) on top, the worst at the bottom. The simplex algorithm then chooses a new point in $\theta$-space by the a strategy, expressed as the ﬂow diagram, Fig. 8-8 on page 203 below, that in action somewhat resembles the behavior of an amoeba seeking its food. The key word \bc{)MINIMIZE} that implements the complex decision tree in Fig. 8-8 (given here in pseudocode) is

\begin{lstlisting}
: )MINIMIZE (n.iter -- 87: rel.error -- )
    INITIALIZE
    BEGIN done? NOT N N.max < AND .
    WHILE
        REFLECT r> =best?
        IF r> =2worst?
            IF r<worst? IF STORE.X THEN
                HALVE r<worst?
                IF STORE.X ELSE SHRINK THEN
            ELSE STOREX THEN
        ELSE DOUBLE r> =best?
        IF STOREXP ELSE STORE.X THEN
        THEN
        N 1+ IS N SORT
    REPEAT ;
\end{lstlisting}

used in the format

\begin{lstlisting}
    USE( tnarne 20 96 1.6-4 )MINIMIZE
\end{lstlisting}

Fleshing out the details is a --by now-- familiar process, so we leave the program \textit{per se} to Appendix 8.5. We also include there a FORTRAN subroutine for the simplex algorithm, taken from

\begin{lstlisting}
INHIALIZE \ choose simplex? is am of param. space
BEGIN
Converged? NOT N N.rnax s
WHILE
'—- REFLECT\ worst point thru geoeenter of other points
DONE '0") S «best-Pt) 7
.~
I
Yes No l(x') z l(2worst.pl) ?
DOUBLE '/\’
N Y
xx") s «bestpo ? m r ’0”) < “3”“ 7
l .

Am REPEAT .
Store r'

HALVE
i
f(x") < I(worst.pl) ?
REPEAT A

Store r' SHRINK

REPEAT

Fig. 8-8 \textit{Flow dlagram of the slmplex algorithm}
\end{lstlisting}

\textit{Numerical Recipes}\sepfootnote{08_26}, as an example of just how indecipherable traditional languages can be.

\section{Appendices}

\subsection{Gaussian quadrature}

\TallC{Gaussian} quadrature formulae are based on the following idea: if we let the points $\xi_{n}$ and weights $w_{n}$ be 2N free parameters ($n$ runs from 1 to N), what values of them most accurately represent an integral by the formula

\begin{equation}\label{eq:08_50}
I = \int_{A}^{B} dx \sigma(x)f(x) \approx \sum_{n=1}^{N} w_{n}f(\xi_{n} ?
\end{equation})

In Eq. \ref{eq:08_50} $\sigma(x)$ is a (known) positive function and $f(x)$ is the function we want to integrate. This problem can actually be solved, and leads to tables of points $\xi_n$ and weight coefficients $w_{n}$ specific to a particular interval [A,B] and weight function $\sigma(x)$. \textbf{Gauss-Legendre} integration pertains to [-1,+1] and $\sigma(x) = 1$. (Note any interval can be transformed into [-1, + 1].)

The interval $[0, \infty)$ and $\sigma(x) = e^{x}$ leads to \textbf{Gauss-Laguerre} formulae, whereas the interval $(-\infty, +\infty)$ and $\sigma(x) = e^{-x^{2}}$ leads to \textbf{Gauss-Hermite} formulae.

Finally, we note that the more common integration formulae such as Simpson’s rule or the trapezoidal rule can be derived on the same basis as the Gauss methods, except that the points are specified in advance to be equally spaced and to include the end-points of the interval. Only the weights $w_{n}$ can be determined as free fitting parameters that give the best approximation to the integral.

For given N Gaussian formulae can be more accurate than equally-spaced rules, as they have twice as many parameters to play with.


Some FORTH words for 5-point Gauss-Legendre integration:

\begin{lstlisting}
N swarm WANT ﬂ
1. ossamroreasss FCONSTANT x1 :mm (arse--0
it ow Pomsrmr wo soda (arse-[mew [sat/2)
N 0.4mm FWANT wt FCNER Fm IOF' F-ROT (87:-Jab)
% QWID WTANT W2 XDIP in” FIX) wt F' F3R+
XDIP x1 FPEGATE rude
:acde (arse-[mam [eat/2) Poo m F‘ F3R+
FOVERF—leFtNDERF+; Mia“ F(X)\~2F' F$+
:reeede (a7:abx--a+b"x) F'F+; XDUP )eFNEGATEreaede
':F3R+(e7:abcx--a+xbc)F3RF+ F-ROT; Pot) waF' F3R+
\end{lstlisting}

\subsection{The trapezoidal rule}

\TallC{Recall} (Ch. 8, \S1.1) how we approximated the area under a curve by capturing it between rectangles consistently higher-- and lower than the curve; and calculating the areas of the two sets of rectangles. In practice we use a better approximation: we average the rectangular upper and lower bounds. The errors tend to
cancel, resulting in\sepfootnote{08_27}

\begin{equation}
  \begin{split}
    \frac{w}{2} \sum_{n=0}^{(B-A)/w} \left( f(A+nw) + f(A+nw+w)\right) \\
    & \approx \int_{A}^{B} dx f(x) + \frac{1}{12} \left( \frac{(B-A)}{w} \right) w^{3} \text{max}_{A<x<B} \vert f''(x) \vert
  \end{split}
\end{equation}

Now the error is much smaller\sepfootnote{08_28} --of order $w^{2}$-- so if we double the number of points, we ecrease the error four-fold. Yet this so-called \textbf{trapezoidal rule} requires no more effort (in terms of the number of function evaluations) than the rectangle rule.


\subsection{Richardson extrapolation}
\TallC{In the program} \bc{)INTEGRAL} in Ch.8 \S1\S\S5.3 the word \bc{INTERPOLATE} performs Richardson extrapolation for the trapezoidal rule. The idea is this. If we use a given rule, accurate to order $w^{n}$, to calculate the integral on an interval [a,b], then presumably the error of using the formula on each half of the interval and adding the results, will be smaller by $2^{-n}$. For the trapezoidal rule, $n = 2$, hence we expect the error from summing two half-intervals to be $4\times$ smaller than that from the whole interval.

Thus, we can write $(I_{0} = \int_{a}^{b} , I_{0}' = \int_{a}^{(a+b)/2}, I_{1} = \int_{(a+b)/2}^{b})$

\begin{subequations}
    \begin{equation}\label{eq:08_52a}
        I_{0} = I_{\text{exact}} + R
    \end{equation}
    \begin{equation}\label{eq:08_52b}
        I_{0}' + I_{1} = I_{\text{exact}} + \frac{R}{4}
    \end{equation}
\end{subequations}

Equation \ref{eq:08_52b} is only an approximation because the R that appears in it is not exactly the same as R in Eq. \ref{eq:08_52a}. We will pretend the two R’s are equal, however, and eliminate R from the two equations (8.52a,b) ending with an expression for $I_{\text{exact}}$:

\begin{equation}\label{eq:08_53}
    I_{\text{exact}} \approx \frac{4}{3}(I_{0}' + I_{1} - I_{0})
\end{equation}

Equation \ref{eq:08_53} is exactly what appears in \bc{INTERPOLATE}.


\subsection{Linear least-squares: Gram polynomials}
Here is a FORTH program for implementing the algorithm
I derived in \S2\S\S2 above.

\begin{lstlisting}
\         CODEed words to simplify fstack management
\ ---------------------------------------------------------------
CODE G(N+1)  4 FMUL.  FCHS.  2 FLD.  6 FSUB.  2 FMUL.  1 FADDP.
             DX DS MOV.  DS POP.   R64  DS: [BX] FST.
             DS DX MOV.  BX POP.   END-CODE
( seg off --  :: s a b w x g[n] g[n-1] -- s a b w x g[n] g[n+1] )

CODE  B(N+1)  2 FXCH.  2 FMUL.  3 FMUL.  1 FXCH.  1 FMUL.
      DX DS MOV.  DS POP.  R64  DS: [BX]  FADD.  DS: [BX] FSTP.
      DS DX MOV.  BX POP.  END-CODE
( seg off --  :: s a b w x g[n] g[n+1] -- s a b w g[n+1] wxg[n+1] )

CODE  A(N+1)  1 FMUL.  DX DS MOV.  DS POP.  R64  DS: [BX] FADD.
              DS: [BX] FSTP.    DS DX MOV.  BX POP.  END-CODE
( seg off --  :: s a b w g[n+1] wxg[n+1] -- s a b w g[n+1] )

CODE C(N+1)  1 FXCH. 2 FMUL".  2 FMUL.  2 FXCH.  1 FMULP.
     DX DS MOV.  DS POP.  R64  DS: [BX] FADD.  DS: [BX] FSTP.
     DS DX MOV.  BX POP.  3 FADDP.  END-CODE
( seg off --   :: s a b w g[n+1] f -- s=s+wg[n+1]**2  a  b )
\ ===============================================================

\ ===============================================================
\            Gram polynomial coding
\ ---------------------------------------------------------------
\ Usage:      A{ B{ C{ G{{ Nmax }FIT
REAL*8 SCALAR  DELTA    0 VAR Nmax

: FIRST.AB's   F=0 a{ 0 0} G!    F=0 b{ 0 0} G!  ;

: INIT.DELTA   ( :: -- g{{ 1 I }} )    F=0  F=0
     M 0 DO  w{ I 0} G@  y{ I 0} G@   F**2  FOVER  F* 
             ( ::s s' -- s s' w wf^2)
             FROT  F+   F-ROT  F+  FSWAP ( :: -- s=s+w  s'=s'+wf^2 )
     LOOP    DELTA G!  FSQRT  1/F  ;

: FIRST.G's  ( :: g{{ 1 I }} -- g{{ 1 I }} ) 
     M 0 DO  F=0  g{{ 0 I }} G!  FDUP  g{{ 1 I }} G!  LOOP  ;

: SECOND.AB's   ( :: g{{ 1 I }} -- g{{ 1 I }} )
     F=0 b{ 1 0}  G!   FDUP  F**2
     F=0  M 0 DO  w{ I 0} G@ x{ I 0} G@  F*  F+  LOOP  
     F*  a{ 1 0} G! ;

: FIRST.&.SECOND.C's  ( :: g{{ 1 I }} -- )
     F=0 c{ 0 0} G!  F=0
     M 0 DO  w{ I 0} G@  y{ I 0} G@  F*  F+   LOOP
     F*  c{ 1 0} G! ;

: INITIALIZE   FINIT   IS Nmax  IS g{{  IS c{ IS b{  IS a{
     FIRST.AB's   INIT.DELTA   FIRST.G's  SECOND.AB's
     FIRST.&.SECOND.C's  ;

0 VAR N   0 VAR N+1
: inc.N   N+1  DUP  IS N   1+  IS N+1 ;

: DISPOSE   >R  DDUP  R@ G@   R> ;  \ just a convenient word
: inc.OFF   #BYTES DROP + ;  \ increment the offset by 1 item

: R64.ZERO.L   ( seg off --)  DDUP  F=0 R64!L  ( R32!L ) ;  

: START.Next.G   ( -- [c{n+1}] [a{n+1}] [b{n+1}]  :: -- s=0 a{n} b{n} )
        FINIT   F=0   c{ N+1 0} DROP  R64.ZERO.L
        a{ N 0} DISPOSE   inc.OFF  R64.ZERO.L
        b{ N 0} DISPOSE   inc.OFF  R64.ZERO.L  ;

: SET.FSTACK   
    w{ I' 0} G@  x{ I' 0} G@   g{{ N I' }} G@  g{{ N 1- I' }} G@   ;

: }@*!  ( adr n --  :: x -- x)  FDUP  0}  DISPOSE  F*  G! ;
\ just another convenient word

: NORMALIZE    ( :: sum -- )  
      1/F  a{ N+1 }@*!   FSQRT   b{ N+1 }@*!   c{ N+1 }@*!
      M 0 DO  FDUP  g{{ N+1 I }}  DISPOSE   F*  G!  LOOP   FDROP  ;

CODE 6DUP  OPT" 6 PICK  6 PICK  6 PICK  6 PICK  6 PICK  6 PICK " END-CODE
CODE 6DROP OPT" DDROP DDROP DDROP " END-CODE

: Next.G   START.Next.G 
      M 0 DO  6DUP    SET.FSTACK
              g{{ N+1 I }} DROP   G(N+1)   B(N+1)   A(N+1)
              y{ I 0} G@   C(N+1)
      LOOP    6DROP   FDROP FDROP   NORMALIZE  ;

: New.DELTA   ( :: -- old.delta new.delta)
      DELTA G@  FDUP   c{ N 0} G@  F**2  F-   FDUP  DELTA G! ;

: NOT.ENUF.G's?   New.DELTA    M N+1 -   S->F   FDUP  F=1 F- 
            ( CR  .FS  ."  NEXT ITERATION?" ?YN  0= IF ABORT THEN )
              ( :: -- d d' m-n-1  m-n-2 )  
              FROT  F\   F-ROT   F/   ( :: -- d'/[m-n-2]  d/[m-n-1] )
              FOVER  F0>  IF  F<=  ELSE  FDROP FDROP 0 THEN  ;


: }FIT  ( X{ Y{ S{ Nmax A{ B{ C{ G{{ -- )
        INITIALIZE    1 IS N   2 IS N+1
        BEGIN      NOT.ENUF.G's?     N Nmax <  AND 
        WHILE      Next.G   inc.N
        REPEAT  FINIT ;
\ ============================================== end of code ;

: RECONSTRUCT  M 0  DO  CR  x{ I 0} G@ F.  y{ I 0} G@ F. 
                 F=0  N+1 1+  1  DO 
                        c{ I 0} G@  g{{ I J }} G@  F* F+ 
                      LOOP   F. 
               LOOP    ;
\end{lstlisting}

\subsubsection{Non-linear least squares: simplex method}
\TallC{A FORTRAN} program for the simplex method is given belton page 209. The FORTH version, as discussed in \S2\S\S3 given on pages 210 and 211.

\begin{lstlisting}
\ FUNCTION MINIMIZATION BY THE SIMPLEX METHOD
\ VERSION OF 18:57:29  5/31/1990

TASK AMOEBA

\ ------------------------------------------------ FINITE STATE MACHINE
: WIDE   0 ;
: STATE.TAB:   ( width 0 -- )   CREATE   ,  ,   ]   DOES>   ( col# -- )
               UNDER  D@        ( -- adr col# width state )
               *  +   1+  4*    ( -- adr offset )
               OVER +           ( -- adr adr' )
               DUP@  SWAP 2+    ( -- adr [adr']  adr'+2 )
               EXECUTE@         ( -- adr [adr']  state')
               ROT !   EXECUTE   ;

0 CONSTANT >0
1 CONSTANT >1
2 CONSTANT >2
3 CONSTANT >3
4 CONSTANT >4
\ -------------------------------------------- END FINITE STATE MACHINE

\ --------------------------------------------------- FUNCTION NOTATION
    VARIABLE <F>
    : USE(   [COMPILE]  '   CFA  <F> ! ;
    : F(X)   <F>  EXECUTE@  ;
    BEHEAD'  <F>
\ ----------------------------------------------- END FUNCTION NOTATION

\ ----------------------------------------------------- DATA STRUCTURES
2 VAR Ndim
0 VAR N
0 VAR N.max
0 VAR I.worst
0 VAR I.best

CREATE SIMPLEX{{  Ndim   4  ( bytes )  *  Ndim 1+  ( # points )  *  ALLOT
CREATE F{  Ndim 1+  4  ( bytes )  *   ALLOT    \ residuals

DVARIABLE  Residual
DVARIABLE  Residual'
DVARIABLE  Epsilon

CREATE X{  Ndim     4  ( bytes )  *   ALLOT    \ trial point
CREATE XP{ Ndim     4  ( bytes )  *   ALLOT    \ 2nd trial point
CREATE Y{  Ndim     4  ( bytes )  *   ALLOT    \ geocenter

: }    ( adr n -- adr + 4n)   4*  +  ;
: }}   ( adr m n -- adr + [m*Ndim + n]*4 )   SWAP  Ndim *  +   }  ;
\ ------------------------------------------------- END DATA STRUCTURES

\ -------------------------------------------------------- ACTION WORDS
: RESIDUALS   Ndim 1+  0  DO  SIMPLEX{{ I 0 }}  F(X)  F{ I }  R32!
              LOOP  ;

: CENTER  ( -- )   FINIT  Ndim S->F
        Ndim 0  DO                            \ loop over components
                                              ( :: -- N )
             F=0                              \ loop over vectors
             Ndim 1+ 0  DO   I I.worst  <>  IF
                  SIMPLEX{{ I J }}  R32@  F+   THEN
             LOOP   FOVER  F/   Y{ I }  R32!  \ put away
        LOOP    FDROP  ;

: DONE!    CR  ." We're finished."  ;
: TOO.MANY   CR  ." Too many iterations."  ;

: V.MOVE   ( src.adr  dest.adr -- )
        Ndim 0  DO   OVER  I }   OVER  I }  2 MOVE   LOOP   DDROP  ;

: STORE   ( adr1 adr2 -- )   0 }  SIMPLEX{{ I.worst 0 }} V.MOVE
                             F{ I.worst }  2  MOVE  ;

: ST0  Residual  X{  STORE ;
: ST1  Residual'  XP{  STORE ;

: New.F     X{   F(X)   Residual   R32!  ;

FIND CPDUP  0= ?( : CPDUP  FOVER FOVER ;  )

: Worst.Point   FINIT    F{ 0 } R32@   0
                Ndim 1+  1  DO   F{ I }  R32@  CPDUP   F<
                     IF   FSWAP  DROP  I   THEN   FDROP
                LOOP   FDROP  ;

: Best.Point    FINIT    F{ 0 } R32@   0
                Ndim 1+  1  DO   F{ I }  R32@  CPDUP   F>
                     IF   FSWAP  DROP  I   THEN   FDROP
                LOOP   FDROP  ;

: 2worst        FINIT  F{ I.best }  R32@  0
                Ndim 1+  0  DO
                     I I.worst <>
                     IF   F{ I }  R32@   CPDUP   F<=
                          IF   FSWAP  DROP  I   THEN  FDROP
                     THEN
                LOOP   FDROP  ;

: SCALE     ( :: scale -- )
            Ndim 0  DO  DUP  I }  R32@       ( :: -- sc x )
                        Y{ I }  R32@  FUNDER  F-   ( :: -- sc y x-y)
                        FROT  FUNDER  F*           ( :: -- y sc [x-y]*sc)
                        FROT  F+                   ( :: -- sc y+[x-y]*sc)
                        X{ I }  R32!
            LOOP  DROP  FDROP  New.F  ;

FIND F=1/2  0= ?( : F=1/2  F=1 FNEGATE F=1 FSCALE FPLUCK ; )
FIND F=2    0= ?( : F=2    F=1 FDUP FSCALE FPLUCK ;  )
\ -------------------------------------------------- DEBUGGING CODE
0 VAR DBG
: DEBUG-ON  -1 IS DBG ;
: DEBUG-OFF 0 IS DBG ;

: .V   3 SPACES  Ndim 0  DO   DUP I }  R32@  F.   LOOP  DROP  ;
: .M   ( -- )   Ndim 1+ 0  DO  DBG  CR  IF  I .  2 SPACES  THEN
                     SIMPLEX{{ I 0 }}  .V
                     DBG  IF  F{ I }  R32@ F.  THEN
                LOOP  CR
                DBG  IF CR  X{ .V   Residual R32@ F. THEN ;

: .F   Ndim 1+  0 DO  CR   F{ I }   R32@  F.   LOOP ;
\ ---------------------------------------------- END DEBUGGING CODE
: REFL      CENTER   F=1  FNEGATE  SIMPLEX{{ I.worst 0 }}
            SCALE    DBG IF  CR ." REFLECTING"  THEN  .M    ;


: DBL     Residual Residual'  2 MOVE       \ swap points, resid.
          X{  XP{  V.MOVE
          FINIT  F=2   FNEGATE  X{  SCALE
          DBG IF  CR ." DOUBLING"  THEN  .M  ;

: HALVE   FINIT  F=1/2   SIMPLEX{{ I.worst 0 }}  SCALE
          DBG IF  CR ." HALVING"  THEN  .M  ;

: SHRINK   SIMPLEX{{ I.best 0 }}  Y{  V.MOVE
           Ndim 1+  0  DO
                   Ndim 0  DO  SIMPLEX{{ J I }}  DUP
                            R32@   Y{ I }  R32@    FUNDER  F-
                            F=1/2  F*   F+     R32!
                   LOOP
           LOOP   RESIDUALS
           DBG  IF  CR ." SHRINKING"   THEN  .M  ;
\ ---------------------------------------------------- END ACTION WORDS

SYNONYM >N NEXT
\ ---------------------------------------------------------- TEST WORDS
: (test)   FINIT  Residual  R32@  F{ SWAP }  R32@   F<  NOT ;
: best>=    I.best   (test)  ;
: worst>=   I.worst  (test)  ;
: 2worst>=  2worst   (test)  ;

: done?   F{ I.worst } R32@  F{ I.best }  R32@  CPDUP  F-  F2*
          F-ROT  F+  F/   FABS   Epsilon  R32@   F<  ;
\ ------------------------------------------------------ END TEST WORDS

4 WIDE STATE.TAB:  New.Point
\ Input:   |  best<   |  2worst< |  worst<  |  worst>= |
\ State:   ---------------------------------------------
  ( 0 )       DBL >1     ST0  >4    ST0  >2     >N  >2
  ( 1 )       ST0 >4     ST1  >4    ST1  >4    ST1  >4
  ( 2 )       >N  >4     >N   >4    HALVE >3   HALVE >3
  ( 3 )       ST0 >4     ST0  >4    ST0  >4    SHRINK >4    ;

: cat->col#  ( -- col#)  1  2worst>=   -
                         worst>=  -     best>=  AND  ;

: )MINIMIZE   ( n.iter --  :: error -- )
       IS N.max   Epsilon R32!   0 IS N
       RESIDUALS  Worst.Point IS I.worst
                  Best.Point IS I.best
       BEGIN    done?  NOT    N N.max  <  AND   WHILE
            Worst.Point IS I.worst
            Best.Point  IS I.best
            REFL   ' New.Point  0!
            BEGIN   cat->col#    New.Point
                    ' New.Point @   4 =    UNTIL
            N 1+  IS N
       REPEAT   DONE!  ;


\ EXAMPLE FUNCTIONS
: F1  ( adr -- :: F )  DUP  0 }   R32@  F**2   1 }  R32@  F**2   F2*  F+  ;
\ f1 = x^2 + 2*y^2

: F2        ( adr -- :: F )  DUP  0 }   R32@  FDUP F**2  1 }  R32@  F**2
            F2* F+  36 S->F  F-  F**2  F2/  FSWAP  6 S->F  F*  F-  ;
\  f2 = .5 * (x * x + 2 * y * y - 36) ^ 2 - 6 * x

: F3        ( adr -- :: F )  DUP DUP  0 }   R32@  FDUP F**2  1 }  R32@  F**2
          F2* F+  36 S->F  F-  F**2  F2/  FSWAP  6 S->F  F*  F-
          DUP 1 } R32@  0 } R32@  F/  FATAN  F2*  FCOS  F**2  F*  ;
\ f3 = [(x^2 2*y^2 - 36)^2 - 6*x] * cos(2*atan(y/x)) ^ 2

\ Usage:    USE( MYFUNC  10  1.E-3 )MINIMIZE

FLOATS

5. SIMPLEX{{ 0 0 }} R32!     -3.  SIMPLEX{{ 0 1 }} R32!
5. SIMPLEX{{ 1 0 }} R32!     3.  SIMPLEX{{ 1 1 }} R32!
-10. SIMPLEX{{ 2 0 }} R32!    1.  SIMPLEX{{ 2 1 }} R32!
\end{lstlisting}