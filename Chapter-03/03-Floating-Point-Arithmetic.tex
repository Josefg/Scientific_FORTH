
\chapter{Floating Point Arithmetic}

\tableofcontents

\TallC{This} chapter discusses various aspects of floating point arithmetic in
FORTH. Our approach assumes the central processor (CPU) has a dedicated floating
point co-processor (FPU)
available to it, such as the 80x87 for the Intel 80x86 family; the
built-in FPU on the 80486; the 68881/2 for Motorola 680x0
machines; various add-ins and clones like the Weitek, Cyrix, IIT
and AMD chips; or digital signal processing and array-processing
co-processor boards.

If no floating point co-processor were available, one could
employ co-processor emulation routines. This is the approach
taken in commercial software written in FORTH, such as the
(unfortunately now-deceased) VP-Planner spreadsheet. Since
this text is not a \textit{vade mecum} for writing commercial software, but
rather a handbook for using FORTH to solve computational
problems in science and engineering, we consider a co-processor
essential.

\section{Organization of floating point arithmetic}

\TallC{FORTH} was originally invented as a language for controlling 
machinery. It is still used extensively for this purpose, with the
machines in question being as varied as industrial robots,
laboratory instruments, the Hubble Space Telescope, special
effects motion picture cameras, and other computers. The floating point
co-processor in a typical computer is a machine, and any
numerical calculation with floating point or complex numbers,
e.g. can be organized in terms of loading operands into the
coprocessor, and transferring results from it to memory. That is,
FORTH can control the FPU through the calculation, as indicated in Fig. 3-1
below:

\tikzstyle{line} = [draw, -latex']
\tikzstyle{block} = [rectangle, draw, above right, align=center]

\begin{figure}
\begin{tikzpicture}[scale=1]
\draw [black] (.4, .3) rectangle (10.04, 6.42);

\node [block] at (2.24, 4.50) (data) {DATA \\ STORE};
\node [block] at (5.28, 4.70)  (fpu) {FPU};
\node [block] at (7.72, 4.50) (res) {RESULT \\ STORE};
\node [block] at (1.75, 1.10) (pgm) {PROGRAM \\ STORE};
\node [block] at (5.30, 1.30) (cpu) {CPU};
\node [block] at (4.60, 2.55)  (op) {OPRATIONS\\ \& \\ ADDRESSES};
\path [line]  (fpu) -- (res) ;
\path [line]  (data) -- (fpu) ;
\path [line]  (op) -- (fpu) ;
\path [line]  (cpu) -- (op) ;
\path [line]  (pgm) -- (cpu) ;

\end{tikzpicture}
\caption{Fig. 3—1 Data flow diagram of a CPU controlling an FPU through a calculation.}
\end{figure}

We assume a stack for floating point numbers separate from the 
parameter or return stacks. We call this the fstack, and assume i
has arbitrary depth. (We denote it by :: in stack comments.)

\leftbar[1\linewidth] 
Whether the fstack should be distinct from the parameter stack
is currently a subject of lively debate within the FORTH com-
munity. One faction wishes to combine the two. The other
faction, including the author and most other FORl'l-l number-
cmnchers, believes that to organize a floating point- intensive
calculation as data ﬂow through a dedicated coprocessor, the
parameter stack must be reserved for addresses, loop indices and
ﬂags. The data fed to the coprocessor therefore has to stay
elsewhere, i.e. in the data store and the fstack.
\endleftbar

The words we shall need fall into the categories of fstack 
manipulation, special constants, arithmetic, tests and mathematical functions.
Their names are nearly standard\sepfootnote{03_01}.

\subsection{Fstack manipulation}
\TallC{The} fstack words are\sepfootnote{03_02}
\begin{verbatim}
F@     ( addr -- :: -- x )
Fl     ( addr -- :: x -- )
FDUP   ( :: x -- xx )
FSWAP  ( :: yx -- xy )
FDROP  ( :: x -- )
FROT   ( :: zyx -- yxz )
FOVER  ( :: yx -- yxy )
S->F   ( n-- :: --n)
D->F   ( d-- :: -- d )
F->S   ( :: x-- : -- int[x] )
F->D (::x--:--dint[x])
% (place a FP# from input stream on fstack)
\end{verbatim}
 
To these we sometimes add\sepfootnote{03_03}

\begin{verbatim}
: FUNDER FSWAP FOVER ;
: FPLUCK FSWAP FDROP ;
Fnx (n = 2 - 6 | defined in code for speed)
FnR (n = 3 - 7 | defined in code for speed)
\end{verbatim}

The Intel mathematics co-processors 80x87 (8087/80287/80387)
and their clones incorporate a stack of limited depth (in fact 8
deep), the 87stack It is far faster to get a number from the 875tack
than from memory. Thus, as Palmer and Morse\sepfootnote{03_04} emphasize,
optimizing for speed demands maximum use of the 87fstack to store intermediate
results, frequently used constants, etc.

In Ch. 4 @7 we show how to extend the 87fstack into memory. The
cost of unlimited fstack-depth is reduced speed when the 87stack
spills over to memory.

\subsection{Special constants}
\TallC{In}
defining various floating point operations it is convenient to be
able to place certain constants on the fstack directly, by invoicing
their names. Here are some words that have proven useful:
\begin{verbatim}
F=0 (::--O)

F=1 (::--1)

F=Pl (2: --pi = 3.14159...)
F=L2(10) (2: - -Iogz10)
F=L2(E) (:2 "log, a)

F= L10(2) (z: - - Iog,°2)
F=LN(2) (ii-409.2)
\end{verbatim}
 
\subsection{Arithmetic operations}
\TallC{As}
noted in Chapters 1 and 2, FORTH arithmetic operators are
rds \textit{- dumb} words. FORTH uses a distinct set of operators
for each kind (16-bit integer, 32-bit integer, REAL, COMPLEX)
of arithmetic. so the compiler has nothing to decide.

Languages like FORTRAN, BASIC and APL overload arithmetic operators - their
meanings are context-dependent. This
makes it possible to write -say- a FORTRAN expression using
REAL@4, REAL@B, COMPLEX@S or COMFLEX'16 literals
and variables without worrying about how to fetch. store or
convert them. Operator overloading increases the complexity of
compilers and limits the speed and efficiency of compilation.

As we shall see in Chapter 5@1, FORTH enables an alternative
solution in which "smart" data "know" their own types and
"smart" operators "know" what kinds of data are being combined.
The slightly reduced execution speed is offset by improved
ﬂexibility: one canned routine can work with all data types. Even
better, adding this kind of "intelligence" makes no extra demands
on the FORTH compiler.

The standard, dumb FORTH floating-point arithmetic opera-
tions and their actions are

\begin{verbatim}
F+       ( :: yx -- y+x)
F—       ( :: yx -- y—x)
FR—      ( :: yx -- x—y)
F*       ( :: yx -- y*x)
F/       ( :: yx -- y/x)
FRI      ( :: yx -- x/y)
FNEGATE  ( :: x -- -x)
FABS     ( :: x -- |x|)
1/F      ( :: x -- 1/X)
\end{verbatim}

To these it is sometimes useful to add words that do not consume
all their arguments, such as F*NP (floating multiply, no pop)

\begin{verbatim}

F'NP (::xy--xy@x).

\end{verbatim}

that are faster, more convenient. and less demanding of the
87stack than the phrase FOVER F*.

\subsection{Example: evaluating a polynomial}
\TallC{Let} us now write a little program to calculate something using
the floating point lexicon, say a program to evaluate a general
polynomial $P_N (x)$. The formula to evaluate is
\begin{equation}
P_N(x)=a_0x^0+a_1x^1+...+a_Nx^N
\\
=a_0+x(a_1+x(... +xa_N))\nonumber
\end{equation}


\definecolor{gray}{gray}{0.8}

The algorithm can be represented by the (pseudo) FORTH ﬂow
diagrams\sepfootnote{03_05}, where
{\colorbox{gray}{\color{gray}XXX}} indicates the end of the program.

 
\tikzstyle{line} = [draw, -latex']
\tikzset{%
  do path picture/.style={%
    path picture={%
      \pgfpointdiff{\pgfpointanchor{path picture bounding box}{south west}}%
        {\pgfpointanchor{path picture bounding box}{north east}}%
      \pgfgetlastxy\x\y%
      \tikzset{x=\x/2,y=\y/2}%
      #1
    }
  },
  cross/.style={do path picture={    
    \draw [line cap=round] (-1,-1) -- (1,1) (-1,1) -- (1,-1);
  }},
  dot/.style={do path picture={    
    \draw [fill]  circle [radius=1]; 
  }}
}
\begin{figure}
\begin{tikzpicture}[minimum size=0.75cm]
    \node at (1,7) (start) {\,};
    \node [circle, draw, cross]    at (1, 5) (cross) {};
    \node [right, align=left] at (2,5) {0 N 1- DO \textbackslash  \, from I = N-1 to 0} ;
    \node [right, align=left] at (2,4) {sum = sum $*$ x + a[I]} ;
    \node [circle, draw, dot]      at (1, 3) (dot)  {};
    \node [right, align=left] at (2,3) {-1 +LOOP} ;
    \node [right, align=left] at (2,2) {sum = a[N]} ;
    \draw [fill, opacity=0.2] (.5,1.25) rectangle (1.5,.75) {};
    \node at (1,0.90) (ext) {};
    \node [right, align=left] at (2,1) {EXIT} ;
    \path [line]  (cross) -- (dot);
    \path [line]  (dot) -- (ext);
    \path [line]  (start) -- (cross);
\end{tikzpicture}
\caption{ Fig. 3-2 Pseudo FOR TH flow diagram of polynomial evaluation }
\end{figure}

 


Now we translate Fig. 3-2 above into FORTH \sepfootnote{03_06}\sepfootnote{03_07} :

\begin{verbatim}

: }POLY ( [a{] [x] -- ) \ evaluate p(x,N)
    FINIT G@            \ x on fstack
    DUP type@ G=O       (z: -- x sum=0)
    LEN@                ( -- [a{] N )
    SWAP DO             \ begin loop
    DUP I } G@          ( :: -- x sum a[i] )
    G+ GOVER G*         ( :: -- x sum' )
    -1 +LOOP            \ end loop
    GPLUCK              ( :: -- sum )
    0 } G@ G+ ;         ( :: -- p[x,n] )

\ Say: A{ X }POLY

\end{verbatim}

Note that the function \}POLY expects the addresses of its arguments on the
stack, consumes them and leaves its result on the fstack. User-defined FORTH
functions will in general have an interface of this sort. This will be
especially true of the functions built into numerical co-processors.

Actually, such behavior is typical of subroutine linkage in most high
level languages, as anyone knows who has written assembler subroutines that can
be linked to compiled FORTRAN, C or BASIC.
So FORTH really isn't different, only more explicit and efficient.

\subsection{Optimizing: FORTH vs. FORTRAN}
\TallC{A}
 simple-minded compiler will translate an expression such as
\begin{equation}
     \Big( sin(x) \Big)^2\nonumber
\end{equation}

into a form requiring two function calls:

\begin{verbatim}
Y = SIN(X)*SIN(X) .
\end{verbatim}

Obviously this is silly. One of the claims often made for "optimizing" FORTRAN
or C compilers is the ability to recognize an
expression requiring unnecessary function calls, and to re-express
it as. say,
  
\begin{verbatim}
TEMP = SIN(X)
v = TEMP * TEMP .
\end{verbatim}

A globally optimizing compiler has a more extensive repertoire
usually it can recognize static expressions ("invariant code")
within a loop and move them outside; and it can find and eliminate code that is
never evaluated ("dead" code).

FORTH assumes a good programmer \textit{never} overlooks trivia
optimizations like this. Thus nothing in the FORTH incremental
compiler or optimizer is inherently capable of recognizing silly
code and eliminating it.

Optimization in FORTH takes one of several forms, that can be
combined for best results. The simplest is the use of stacks
registers to avoid extra memory shuffling. Referring to the
preceding bad example, we note that a simple floating-point
function f(x) finds its argumentx on the top of the fstack, consume
it, and leaves the result in its place. A simple F**2, defined as

\begin{verbatim}
: F**2 FDUP F* ;
\end{verbatim}

will then evaluate $[f(x)]^2$, with no fetch/store penalty from defining
a temporary variable.

Some FORTHs can optimize by substituting inline code for jumps
and returns to subroutines. In other words, by making the compiled code longer,
some advantage in speed can be gained.
HS/FORTH offers a recursive-descent optimizer of just this sort
that -within its limitations- can optimize as well as good C or
FORTRAN compilers. An optimizer-improved word consists of
all the code bodies of the words in its definition, jammed end to
end and with redundant pushes and pops deleted.

\TallC{Virtually} all FORTH implementations have a built-in assembler

that permits defining a word in machine language. Judiciously
machine-coding selected words can dramatically reduce execution time, since
careful hand coding offers the ultimate performance the machine is capable of.
Some versions of Pascal and C
also have this ability; and of course most compiled and linked
languages can link to functions and subroutines defined in
machine code.

FORTH's advantage over other languages lies in making the 
process of designing, testing and linking hand- coded components
nearly painless.

Another advantage of FORTH over other compiled languages is
that one can specify which parts to optimize and which to leave
as high-level definitions. This is both faster to compile and much
more compact, than optimizing all of the program uniformly. The
rationale of partial (sometimes called "peephole") optimization
is that most programs spend 90\% of their execution time in 10\%
of the code. This 10\% is the only part of the program worth
optimizing  \sepfootnote{03_08}.

\section{Testing floating point numbers}

\TallC{A} nalogous to the test words for integer arithmetic, we require
the words \verb|F0> F0= F0< F> F= F<|.

Test words leave a ﬂag on the parameter stack depending on the
relationship they discover. Moreover, these words consume one
or two arguments on the fstack, following the standard FORTH
practice. As a simple first example of test words, let us define
max and FMIN analogous to MAX and MIN: \sepfootnote{03_09}
\begin{verbatim}
: XDUP FOVER FOVER ;
: FMAX XDUP F< IF FSWAP THEN FDROP ;
: FMIN XDUP F> IF FSWAP THEN FDROP ;
\end{verbatim}

\section{Mathematical functions - the essential function library}

\TallC{Scientific} programming in FORTH requires a suite of exponential,
logarithmic and trigonometric functions (included with all
FORTRAN systems, most BASICs, C's, APL LISP, etc.) The
minimal function library is

\begin{lstlisting}

FSQRT     ( :: x -- (*@$\sqrt x $@*) )
FLN       ( :: x -- (*@$\ln[x]$@*) )
FLOG      ( :: x -- (*@$log_{10}[x]$@*) )
F2**      ( :: x -- (*@$2^x$@*) )
F**       ( :: x y -- (*@$y^x$@*) )
FEXP      ( :: x -- (*@$e^x$@*) )
FSIN      ( :: x -- sin[x] )
FCOS      ( :: x -- cos[x] )
FTAN      ( :: x -- tan[x] )
DEG->RAD  ( :: x -- x(*@$*$@*)pi/180 )
RAD->DEG  ( :: x -- x(*@$*$@*)180/pi )
FATAN     ( :: x -- atan[x] )
FASIN     ( :: x -- asin[x] )
FACOS     ( :: x -- acos[x] )

\end{lstlisting}

Machine code definitions of the above functions for the 80x87
chip will be given in Chapter 4.

\section{Library extensions}

\TallC{The} minimal function library is easily extended. We illustrate
below with the \textbf{FSGN} function and with hyperbolic and inverse hyperbolic
functions. Complex extensions of the function
library is deferred to Chapter 6.

\subsection{The FSGN function}
The most useful form of FSGN finds one argument it (from which
to take an algebraic sign) on the parameter stack, and the floating,
point argument $x$ on the fstack. We may define it using logic, as
\begin{verbatim}

: FSGN ( n -- :: x -- sgn[n]*abs[x])
    FABS O< IF FNEGATE THEN;
\end{verbatim}

\subsection{Cosh, Sinh and their inverses}
We now code the hyperbolic sine and cosine. The formulae are

\begin{eqnarray*}
    sinh(x) & = & \frac{1}{2}\Big(e^x-e^{-x}\Big)\label{hyp_eqn} \\
    cosh(x) & = & \frac{1}{2}\Big(e^x+e^{-x}\Big) 
\end{eqnarray*}


and their definitions are
\begin{verbatim}
: F2/ F=1   ( :: x -- x/2 ) 
    FNEGATE FSWAP FSCALE FPLUCK ;
: HYPER FEXP FDUP 1/F ; ( ::--e**x e**-x)
:SINH HYPER F- F2/ ;
: COSH HYPER F+ F2/ ;
\end{verbatim}

The hyperbolic tangent is then

\begin{verbatim}
FIND XDUP O= ?(: XDUP FOVER FOVER ;)
             \ conditionally compile XDUP
: TANH HYPER ( :: -- ex e-x)
    XDUP F- F-ROT F+ F/ ;
\end{verbatim}

Finally, the inverse hyperbolic sine and cosine can be defined in
terms of logarithms:

\begin{eqnarray*}
    arcsinh(x) & = & \ln{(x + (x^2 + 1)^{^1/_2})} , -\infty < x < \infty \\
    arccosh(x) & = & \ln{(x + (x^2 - 1)^{^1/_2})} , -\infty < x < \infty
\end{eqnarray*}

The corresponding definitions are \sepfootnote{03_10}


\begin{verbatim}
FIND F**2 0= ?(: F**2 FDUP F* ; )
:ARCSINH FDUP F**2 F=1 F+
    FSQRT F+ FLN ;
:ARCCOSH FDUP F**2 F=1 F-
    FDUP F0<
    ABORT" x <1 in ARCCOSH"
    FSQRT F+ FLN ;

 
\end{verbatim}


% PAGE 56 Chapter 3 — Floating Point Arithmetic Sclenfiﬂc FORTH


\section{Pseudo-random number generators (PRNG's)}
\TallC{The} subject of computer-generated (pseudo) random numbers
hfilbeen discussed extensively in the literature of computation \sepfootnote{03_11}
 \sepfootnote{03_12} . We shall
confine ourselves here to translating two useful
algorithms into FORTH, and discussing tests for pseudo-random
number generators (PRNG's).

The first is a method called GGUBS \sepfootnote{03_13} based on the recursion

\begin{equation}
    r_{n+1} = 16807 \times{} r_n \, MOD (2^{31} - 1) .\nonumber
\end{equation}

Since 32-bit modulo arithmetic is inefficient on a 16-bit processor, the
program uses the 80x87 chip, and uses synthetic division
to get N MOD ($2^{31}$ - 1). A version that uses the 32-bit registers
of the 80386/80486 would not be hard to program.

Two specialized words are needed \sepfootnote{03_14}
, that fetch/store 32-bit integers to/from the fstack from/to memory:

\begin{verbatim}
CODE I32@ <% 9B DB 07 5B 9B %> END-CODE
CODE I32! <% 9B DB 1F 5B 9B %> END-CODE
\end{verbatim}

The program data are stored in variables rather than registers so
they can be moved directly to the co-processor \sepfootnote{03_15} .

\begin{verbatim}
DVARIABLE       BIGDIV
21474.83647     BIGDIV D! \2**31-1
DVARIABLE       DIVIS
1277.73         DIVIS D!
DVARIABLE       SEED
VARIABLE        M1 16807 M1 !
VARIABLE        M2 2836  M2 !
\end{verbatim}

The high-level FORTH program itself is\sepfootnote{03_16}\sepfootnote{03_17}

\begin{verbatim}
: RAND                ( :: -- seed)
    FINIT SEED DUP 132@
    DIVIS I32@
    XDUP F/
    FTFIUNC FRNDINT
    FUNDER F* FROT FR-
    M1 |16@ F*
    FSWAP M2 I16@
    F* F- FDUP I32! ;
:RANDOM               ( :: -- random#)
    RAND BIGDIV I32@  ( ::--seed2**31-1)
    FSWAP FDUP F0<    ( -- f :: -- 2**31-1 seed)
    IF FOVER F+ THEN FR/
BEHEAD" BIGDIV RAND
\ make BIGDIV, RAND local

\end{verbatim}
To test the algorithm start with the seed 1, and generate 1000
prn's. The result should then be 522329230.

\begin{verbatim}
: GGUBS.TST 0.1 SEED D!
    1000 0 DO RANDOM LOOP
    SEED D@ D. ;

GGUBS.TST 522329230 0k
\end{verbatim}

% PAGE 58 Chapter 3 — Floating Point Arlthmetlc Scientific FORTH

\subsection{Testing random number generators}
\TallC{When} defining PRNGs it is always important to include a test
for randomness. The simplest is called the $\chi^2$ test: use the
PRNG to generate $N$ integers\sepfootnote{03_18} in the range [0,n-1] and record
the number of occurrences, $f_s$ of each integer $s = 0, 1, \dots n-1$. If
the PRNG is really random, then the probability that an integer
should have any of the $n$ values is $1/n$, hence the expected 
frequencies are <$f_g$> $\equiv \lambda = \frac{N}{n}$ .The $\chi^2$ statistic for $f_s$ is defined to
be

\begin{equation}
\chi^2=\sum_{s=0}^{\infty}\frac{1}{\lambda}\Big(f_s-\lambda\Big)^2, \lambda=\frac{N}{n} %    \nonumber 
\end{equation}


xz should have a value roughly n\sepfootnote{03_19}. GGUBS passes the 12 test: A
program to calculate this statistic (with N = 1000 and n = 100) is

\begin{verbatim}
CREATE FREQS 200 ALLOT OKLW
: IRAND RANDOM 100 S->F
F* FROUND- F->S ;
: INlT-FREOS \initialize freqs array
FREQS 200 0 FILL ;

: GET-FREQS \make frequency table
1000 0 DO IRAND 2*
DUP 199 >ABORT"IRANDTOO LARGE"
FREQS + 1+!
LOOP ;
: STARTCHISQ INIT-FREQS GET—FREQS ;
: CHISO 0 \sum on stack
200 0 DO | FREQS + @
10- DUP * +
2 +LOOP ;

: .FREQS \display distribution
2000DO I FREQS + @ ICE
2 +LOOP ;

\end{verbatim}
 

% crusts-Migranmm 59

The resul are displayed below in Thble 3-1 0 age 59. The mean
of these values is 99.1, and their variance is 210. This agrees
remarkably well with the theoretical formula

02=n(2+%),
whenn = 100 and l = 10.

Table 3-1 Values of 1' for causes

 

STAR@IZG'llSO M80. 1114 olr STARIO-lISO CHSO. 1130 olt
STARICHISO GllSO. 540 olt STARIO'IISO CHSO. 1W olt
STARICHISQ CHlSO. 1294 olt STARICHISO CHISO . % 0k
STARIG'lISO CHISQ . 650 OR STARTGslISO M80. 1052 olt
STABICHISO G-llSO . g) 0k STARICHISO CHISO . 7m olt
STARTCCHISO CHISO . 94 0k STARICHISO CHISO . B42 olt
STARIG'IISQ CHlSO. 1072 0k S'TARIG'IISO CHISO . 976 OR
STARICHISO CHISQ. 1110 olt STARICHISO CHISO. 1W olt
STARIG-IISQ CHISQ. 1183 0k STARICHISO CHISO . $0$ 0k
STARICHISO CHISQ. 360 OR STAR@ECHISO CHISO . 956 OR
STABICHISQ CHISQ. 10m 0k STARIOHISO CHISO . me ok

 

 

 

In one application GGUBS was unsatisfactory because it contained correlations
not revealed by the above tests. This led me
to seek another PRNG with --perhaps-- better properties, a
longer cycle, etc. I offer it as an alternative, since --at the very
least-- it will enable the reader to test his applications with more
than one PRNG\sepfootnote{03_20}. Here is the second PRNG:

% Chapter 3 - Hosting Point Arithmetic
\begin{verbatim}

\PFNGHBAWICWANSLDJLLBYFEW
VARIABLEX VARABLEY VARIABLEZ
:RAND-INrI' 1X|1GIDYI le;
:GEN (ab[n]--n'arnodb) \hl-Ievolvoralon

DUP>R @ MT DUP>R

(--nab:R:--[n]b)

@IMOD DROP DUP O<

IF R> + ELSE FIDFDP THEN

DUP R>I ;
\CODE GEN CXPOP. AXPOP.
\ [BX]WORDP'TR lMUL
\CXIDN.DXOIW 0MP.
\ DX CX ADD.
\ >>>POSITIVE [BX] DX MOV. END-CODE
(Ext; 171 33259 X GEN )

JGEPOSITIVE.

 

Sclendﬂc FORTH

:RAMOM FNT FTRLM
171 ms 011' S->F X GEN I160 FR]
172 30311 UL? S->F Y GEN |18@ PW
F+
170mDIPSr>F ZGEN 116@ FFV
F+FRAC:
% \mmmwmwdaw
% \FTRUNCqsecIf-sromdlngbwado
\end{verbatim}
The corresponding 12 results are given below in Table 3-2.

%Table 3-2 x2 for Wichman-Hill PRNG

\begin{table}
    \caption{$\chi^2$ for Wichman-Hill PRNG}
        \setlength{\tabcolsep}{20pt}
        \begin{tabular}{|ll|}
            \hline & \\
            START.CHISQ CHISQ . 846   &  START.CHISQ CHISQ . 1172  \\ 
            START.CHISQ CHISQ . 1036  &  START.CHISQ CHISQ . 954   \\
            START.CHISQ CHISQ . 852   &  START.CHISQ CHISQ . 908   \\
            START.CHISQ CHISQ . 858   &  START.CHISQ CHISQ . 856   \\
            START.CHISQ CHISQ . 770   &  START.CHISQ CHISQ . 930   \\
            START.CHISQ CHISQ . 882   &  START.CHISQ CHISQ . 868   \\
            START.CHISQ CHISQ . 918   &  START.CHISQ CHISQ . 858   \\
            START.CHISQ CHISQ . 956   &  START.CHISQ CHISQ . 912   \\
            START.CHISQ CHISQ . 1202  &  START.CHISQ CHISQ . 952   \\
            START.CHISQ CHISQ . 1112  &  START.CHISQ CHISQ . 1016  \\
            START.CHISQ CHISQ . 778   & \\
            & \\
            \hline
        \end{tabular} 
\end{table}

\begin{verbatim}
\end{verbatim} 
 
Interestingly, the mean of the 12 statistic for 21 tests is 93.5,
perhaps a bit low, but not outrageously so; however, the variance
in x is suspiciously small — only 135 vs. the expected 210. This
may mean the distribution is excessively even!

One very useful test for randomness involves constructing a
random walk — that is, a sequence of integers generated by the
rule ($r_n$ is the n'th PRN)

\begin{equation}
x_{n+1}=x_n+
\begin{cases}
    \;\; 1 & \quad \text{if } r_{n+1} > 0.5 \\
    -1 & \quad \text{if }  r_{n+1} \leq 0.5
    \end{cases}
\end{equation}

and taking the discrete Fourier transform (DFT) of the sequence\sepfootnote{03_21}.
Any serial correlations will show up as periodicities,
with periods smaller than N, in the DFT of $x_n$.

\subsection{Random data structures}
\TallC{The} prng's we have discussed so far produce prn's uniformly distributed
on the interval [0,1]. What if we want prn's that are
distributed according to the normal distribution on $(-\infty,\infty)$, or
according to some other standard distribution function of mathematical statistics?

There are algorithms for generating prn's whose distribution
function is one of a few standard ones; however, in general one
must resort to brute force. We now engage in a brief mathematical
digression, before showing how prn's with distribution function\sepfootnote{03_22}
$dp(\xi) = \theta(1 - \xi)d\xi$ can be converted to prn's with an arbitrary
distribution function $dp(x) =f(x)dr$.

We suppose there is a function $X(\xi)$ that converts uniform prn's
to prn's distributed according to $f(x)$. But if any of this is to make
sense, the \textbf{inverse function} $\xi(x)$ must also exist, since there is
nothing special about one distribution relative to another\sepfootnote{03_23}. The
condition that both functions exist is $\frac{d\xi}{dx}\neq 0$.

Then\sepfootnote{03_24}

\begin{align}
    f(x) &= \int_{0}^{1}d\xi\delta \Big(x-X(\xi)\Big)\nonumber \\
    &= \int_{0}^{1}d\xi\delta \Big(x-X\Big(\xi(x)+\xi-\xi(x)\Big)\Big)
\end{align}

which, using standard manipulations, we can evaluate as

 

The ordinary differential equation 5 has the formal solution
\begin{verbatim}
'I'henz'4
th) = 1:45 a (x — no)
(4)
=13dto(x-x(5(x) +5 —@(x)))

llaed@r)isthew—caﬂedDuae6-funaim.8eeanystandardtenondistnbutions

ﬂx)= I@deo[(e—t(x))%] = "if (5)
@=r,'f@,@'dxltx) <6) I
I
\end{verbatim}

that defines the new prn's distributed according to f(x)dr, if E is a
pm uniformly distributed on [0,1]. In other words, we have to
solve a (usually) transcendental equation to calculate X(E).

Since most simulation problems demand a lot of pm' 5, it is no use
solving Eq. 6 in real time. The better solution is to define a large
enough table of the X's, and look them up according to E. In this I
case we actually want an integer PRNG uniformly distributed on
[0,N-1], where the table has N entries.

 
\begin{verbatim}

 

\ O HARVARD SOFTWORKS 19%, ALL RIGHTS RESERVED.
HEX
IVARA 2VARB OSFFVARMX

:RANDOM AB+ MX OVER U<

IF MX 1+ - THEN

2' MX OVER U<

IF MX - THEN

BIS A DUP IS 8 ;
:RANDOMIZE (seed1seed2#bits—-)

2 MAX 0F MIN \#bitstruncatedtorange2—16

1 SWAP SAL 1- IS MX

MX MOD IS A

MX MOD IS B

50 DO RANDOM DROP LOOP:

 

h
\end{verbatim}


Fig. 34! PRNG supplied with HS/FORTH

In Fig. 3-3 above we exhibit a PRNG that produces 16-bit intege .
uniformly distributed on [0, 2"— l] . l h ve no idea of its
operandi or its origin, but it passes the 1 tests described above.

We also need a way to invoke user-defined functions: the meth
we have found is shown in Fig. 34 below. I

\begin{verbatim}
ma-MPOHW 83

 

VARIAHE <F>

:USE( [COMPILE] ' (FA <F> i
:F(X) <F> BtECUTE® ;
BEI'EAD' <F> \rrllto<F>iocl

 

 

 
\end{verbatim}

Fig. 34Prorocol for function usage in FORTH

We also require a word analogous to , ("comma") that stores
32-bit floating point numbers in the parameter field of a word:

\begin{verbatim}
: F32, HERE-L 4 ALLOT R32! ;
\ store a 32-bit # from the fstack in the first
\ available place in the dictionary
\end{verbatim}

With these auxiliary definitions we are in a position to define a
word that creates tables of random variates and traverses them in a random order:

\begin{verbatim}
    \ Defining word for tables of prn's distributed according to a given distribution:
    \ P(x<X(xi)) = id defhes X(xi).
    \ Note: the first entry is autormticaliy 0.  
    \ fn.name oonvetts xi to X(xi) 
    \ dist.name is the name of the random N-long table created by )DISTRIBUIION

    :SHAKE.UP ( #bls -- ) TIME@ XOR -ROT XOR ROT RANDOMIZE ;

                             \ randomize seeds
    : )DISTRIBUTION: DUP LG2 ( --N #bits=ln[N])
    SHAKE.UP                 \ Inlhllzepmg
    CREATE F=O F32, \rmkefirstemy
    (N--) 1 DO \Nentrytabie
    IS->F I' S->F F/ \ltiontstack
    F00 F32. \evalmteXOtiiand!
    LOOP
    DOES> RANDOM 4' + R32@ ;
    BEI-EAD" A RANDOM \makethesewordslocd

    \Usage: USE( Inname N )DISTRIBUTION: dbtname
\end{verbatim}
In Fig. 3-5 below we exhibit a frequency plot of 10,000 random
variates drawn from a (rather coarse) table of 64 entries, according to the
distribution $p(x)dx=xe^{-x}dx$, together with $p(x)$.

 


 

 

 

 

 

 

 

 

 

 

 

 

Fig. 3-5 Random variates from table of 64 from distribution $p(x)=xe^{-x}$

With slightly more elaboration we can arrange for each table to 
have a unique prng, thus minimizing correlations between
tables. Because the resulting data structure is nearly an "object",
it is worthwhile to see how this may be done.

To make the prng unique, all that is necessary is to make the seed
VARs unique to a table, and to redefine RANDOMIZE and,
RANDOM to know how to get a given table's seeds. We see that
RANDOM is invoked only in the runtime code for the structure.
This means it has access to the base address, hence the VARs
can be replaced with cells in the table, and the seeds planted there.

